{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b88209d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import ast\n",
    "import json\n",
    "import math\n",
    "import time\n",
    "import pickle\n",
    "import random\n",
    "import urllib\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests as req\n",
    "\n",
    "from copy import copy\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "PLAYER_PATH = 'D://_G_Street/player_model'\n",
    "DROPBOX_PATH = 'C:\\\\Users\\Blake\\G Street Dropbox\\Blake Atkinson\\shared_soccer_data\\data'\n",
    "\n",
    "def save_dict(di_, filename_):\n",
    "    with open(filename_, 'wb') as f:\n",
    "        pickle.dump(di_, f)\n",
    "\n",
    "def load_dict(filename_):\n",
    "    with open(filename_, 'rb') as f:\n",
    "        ret_di = pickle.load(f)\n",
    "    return ret_di\n",
    "\n",
    "\n",
    "teams = load_dict(os.path.join(DROPBOX_PATH, 'IDs/teams'))\n",
    "competitions = load_dict(os.path.join(DROPBOX_PATH, 'IDs/competitions'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e732c4eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['comp2pinny',\n",
       " 'competitions',\n",
       " 'footy',\n",
       " 'gg_mkt_fty_col_ids',\n",
       " 'gg_mkt_sb_col_ids',\n",
       " 'gg_non_mkt_fty_col_ids',\n",
       " 'gg_non_mkt_sb_col_ids',\n",
       " 'id2comp.pkl',\n",
       " 'managers',\n",
       " 'odds_api',\n",
       " 'player_bios.csv',\n",
       " 'player_map',\n",
       " 'referees',\n",
       " 'SBR',\n",
       " 'seasons',\n",
       " 'stadiums',\n",
       " 'teams']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(os.path.join(DROPBOX_PATH, 'IDs'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95acc5ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Blake\\AppData\\Local\\Temp\\ipykernel_24588\\2045045866.py:32: DtypeWarning: Columns (4,5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  normal = pd.read_csv(os.path.join(DROPBOX_PATH, 'schedules/processed_schedule.csv'))\n",
      "C:\\Users\\Blake\\AppData\\Local\\Temp\\ipykernel_24588\\2045045866.py:33: DtypeWarning: Columns (4,5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  stf = pd.read_csv(os.path.join(DROPBOX_PATH, 'schedules/stf_schedule.csv'))\n",
      "C:\\Users\\Blake\\AppData\\Local\\Temp\\ipykernel_24588\\2045045866.py:32: DtypeWarning: Columns (4,5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  normal = pd.read_csv(os.path.join(DROPBOX_PATH, 'schedules/processed_schedule.csv'))\n",
      "C:\\Users\\Blake\\AppData\\Local\\Temp\\ipykernel_24588\\2045045866.py:33: DtypeWarning: Columns (4,5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  stf = pd.read_csv(os.path.join(DROPBOX_PATH, 'schedules/stf_schedule.csv'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating player minutes....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|██████████████████████████████████████████████████████████▊              | 50864/63074 [00:00<00:00, 65197.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error on 3822006\n",
      "Error on 3822007\n",
      "Error on 3822008\n",
      "Error on 3822009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 63074/63074 [00:00<00:00, 65996.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error on 3868691\n",
      "Error on 3868379\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "### need separate model\n",
    "women  = [131,135,1276,37,49,52,182,82,72,53,120]\n",
    "youth = [1284,1331,1333,113,1336]\n",
    "\n",
    "separate = women+youth\n",
    "\n",
    "playoffs = {\n",
    "    130:60,\n",
    "    274:46,\n",
    "    292:7,\n",
    "    280:8,\n",
    "    295:13,\n",
    "    119:9,\n",
    "    121:10,\n",
    "    226:75,\n",
    "    1256:249,\n",
    "    125:80,\n",
    "    1269:104,\n",
    "    218:106,\n",
    "    1259:109,\n",
    "    1426:108,\n",
    "    219:107,\n",
    "    1249:97,\n",
    "    231:88\n",
    "\n",
    "}\n",
    "intl_club = [16,35,90,101,273,353,66,165,1425,102]\n",
    "intl = [254,255,256,257,259,43,55,1226,1249,1278,92,1346]\n",
    "\n",
    "def load_schedules():\n",
    "    \n",
    "    normal = pd.read_csv(os.path.join(DROPBOX_PATH, 'schedules/processed_schedule.csv'))\n",
    "    stf = pd.read_csv(os.path.join(DROPBOX_PATH, 'schedules/stf_schedule.csv'))\n",
    "    \n",
    "    normal = normal.loc[~normal['competition_id'].isin(separate)].reset_index(drop=True)\n",
    "    stf = stf.loc[~stf['competition_id'].isin(separate)].reset_index(drop=True)\n",
    "    normal['is_playoff'] = np.where(normal['competition_id'].isin(list(playoffs.keys())),1,0)\n",
    "    stf['is_playoff'] = np.where(stf['competition_id'].isin(list(playoffs.keys())),1,0)\n",
    "    normal['is_intl'] = np.where(normal['competition_id'].isin(intl),1,0)\n",
    "    stf['is_intl'] = np.where(stf['competition_id'].isin(intl),1,0)\n",
    "    normal['is_intl_c'] = np.where(normal['competition_id'].isin(intl_club),1,0)\n",
    "    stf['is_intl_c'] = np.where(stf['competition_id'].isin(intl_club),1,0)\n",
    "    normal['competition_id'] = normal['competition_id'].apply(lambda x: playoffs[x] if x in playoffs else x)\n",
    "    stf['competition_id'] = stf['competition_id'].apply(lambda x: playoffs[x] if x in playoffs else x)\n",
    "    \n",
    "    normal['home_team_name'] = normal['home_team_id'].apply(lambda x: teams.get(x)['name'])\n",
    "    normal['away_team_name'] = normal['away_team_id'].apply(lambda x: teams.get(x)['name'])\n",
    "\n",
    "    stf['team_name'] = stf['team_id'].apply(lambda x: teams.get(x)['name'])\n",
    "    stf['opp_team_name'] = stf['opp_team_id'].apply(lambda x: teams.get(x)['name'])\n",
    "    \n",
    "    normal['datetime_UTC'] = pd.to_datetime(normal['datetime_UTC'].copy())\n",
    "    stf['datetime_UTC'] = pd.to_datetime(stf['datetime_UTC'].copy())\n",
    "\n",
    "    normal['match_date_UTC'] = normal['datetime_UTC'].copy().dt.date\n",
    "    stf['match_date_UTC'] = stf['datetime_UTC'].copy().dt.date\n",
    "    \n",
    "    normal['last_updated'] = pd.to_datetime(normal['last_updated'].copy())\n",
    "    stf['last_updated'] = pd.to_datetime(stf['last_updated'].copy())\n",
    "    \n",
    "    normal = normal.loc[~normal['match_status'].isin(['deleted','collecting','cancelled','postponed'])].reset_index(drop=True)\n",
    "    stf = stf.loc[~stf['match_status'].isin(['deleted','collecting','cancelled','postponed'])].reset_index(drop=True)\n",
    "    \n",
    "    return normal, stf \n",
    "\n",
    "schedule, stf_schedule = load_schedules()\n",
    "\n",
    "\n",
    "def EST_to_UTC(time):\n",
    "    return time + pd.Timedelta(hours=5)\n",
    "\n",
    "def statsbomb_to_UTC(time):\n",
    "    return time - pd.Timedelta(hours=1)\n",
    "\n",
    "def add_STF_info(cklst):\n",
    "    \n",
    "    cklst['STF'] = False\n",
    "    cklst['stf_path'] = cklst.apply(lambda x: os.path.join(DROPBOX_PATH, f'Statsbomb/STF/{x.competition_id}/{x.season_id}/{x.match_id}-{x.team_id}.csv'), axis=1)\n",
    "    # see if its a new game that doesn't exist yet\n",
    "    cklst['STF'] = cklst.apply(lambda x: os.path.exists(x.stf_path), axis=1)\n",
    "    cklst['as_path'] = cklst.apply(lambda x: os.path.join(DROPBOX_PATH, f'Statsbomb/atomic_sparse/{x.competition_id}/{x.season_id}/{x.match_id}.csv'), axis=1)\n",
    "    return cklst\n",
    "\n",
    "\n",
    "def add_game_clock(game):\n",
    "    \n",
    "    game['time'] = game['minute'].copy() + (game['second'].copy()/60)\n",
    "    to_add = game.groupby(['period'])['time'].max().to_dict()\n",
    "    to_add[0] = 45\n",
    "    game['previous_period'] = game['period'].copy() - 1\n",
    "    game['to_add'] = game['previous_period'].map(to_add)\n",
    "    game['time'] = game['time'].copy() + game['to_add'].copy() - 45\n",
    "    \n",
    "    return game.drop(columns=['previous_period','to_add'])\n",
    "\n",
    "\n",
    "def add_subs(game, lineup_df):\n",
    "    \n",
    "    subs = game.loc[game['type_id']==19].copy().reset_index(drop=True)\n",
    "    subs = subs[['team_id','player_id','time','substitution_replacement_id','substitution_replacement_name','outcome_id','outcome_name']].copy()\n",
    "    \n",
    "    to_append = []\n",
    "    for index,row in subs.iterrows():\n",
    "        player_index = lineup_df[lineup_df['player_id']==row['player_id']].index\n",
    "        if len(player_index)>0:\n",
    "            player_index = player_index[0]\n",
    "        else:\n",
    "            continue\n",
    "        lineup_df.at[player_index, 'end_time'] = row['time']\n",
    "        lineup_df.at[player_index, 'sub_type'] = str(row['outcome_name']) + '_off'\n",
    "        to_append.append(pd.DataFrame([[np.nan, row['substitution_replacement_id'], row['substitution_replacement_name'], \n",
    "        np.nan, np.nan, row['team_id'],  row['time'], game['time'].max(), str(row['outcome_name'])+'_on']], columns=list(lineup_df)))\n",
    "        \n",
    "    to_append = pd.concat(to_append, axis=0).reset_index(drop=True)\n",
    "    lineup_df = pd.concat([lineup_df,to_append], axis=0).reset_index(drop=True)\n",
    "\n",
    "    return lineup_df\n",
    "\n",
    "def add_red_cards(game, lineup_df):\n",
    "    # and 10 man injury, second yellows\n",
    "    \n",
    "    if 'player_off_permanent' in list(game):\n",
    "        injury_loss = (game.player_off_permanent==True)\n",
    "    else:\n",
    "        injury_loss = (game.type_id==99999) # just creating falses\n",
    "    if 'bad_behaviour_card_id' in list(game):\n",
    "        bad_behave_red = (game.bad_behaviour_card_id==5)\n",
    "        bad_behave_sy = (game.bad_behaviour_card_id==6)\n",
    "    else:\n",
    "        bad_behave_red = (game.type_id==99999) # just creating falses\n",
    "        bad_behave_sy = (game.type_id==99999)\n",
    "        \n",
    "    if 'foul_committed_card_id' in list(game):\n",
    "        foul_red = (game.foul_committed_card_id==5)\n",
    "        foul_sy = (game.foul_committed_card_id==6)\n",
    "    else:\n",
    "        foul_red = (game.type_id==99999) # just creating falses\n",
    "        foul_sy = (game.type_id==99999)\n",
    "    \n",
    "    types=['Injury_off','Red','Second_yellow','Red','Second_yellow']\n",
    "    if len(game.loc[injury_loss|bad_behave_red|bad_behave_sy|foul_red|foul_sy]) > 0:\n",
    "        for mask_index, mask in enumerate([injury_loss, bad_behave_red, bad_behave_sy, foul_red, foul_sy]):\n",
    "            if len(game.loc[mask]) > 0:\n",
    "#                 print(mask[mask==True])\n",
    "                \n",
    "                info = game.loc[mask].reset_index(drop=True)\n",
    "                for index, row in info.iterrows():\n",
    "                    try:\n",
    "                        # can't figure out this error\n",
    "                        # usually has a player id that's not in the lineup\n",
    "                        player_index = lineup_df[lineup_df['player_id']==row['player_id']].index[0]\n",
    "                    except:\n",
    "                        continue\n",
    "                    time = row['time']\n",
    "                    type_ = types[mask_index]\n",
    "            \n",
    "                    lineup_df.at[player_index, 'end_time'] = time\n",
    "                    lineup_df.at[player_index, 'sub_type'] = type_\n",
    "    \n",
    "    \n",
    "    return lineup_df\n",
    "\n",
    "def get_starting_lineups(game):\n",
    "    \n",
    "    starting_lineups = game.loc[game['type_id']==35].copy().reset_index(drop=True)\n",
    "    lineup_dict = starting_lineups[['team_id','tactics_lineup']].set_index('team_id').to_dict()\n",
    "    \n",
    "    lineup_df = []\n",
    "    for team in lineup_dict['tactics_lineup'].keys():\n",
    "        team_lineup = pd.json_normalize(ast.literal_eval(lineup_dict['tactics_lineup'][team]))\n",
    "        team_lineup['team_id'] = team\n",
    "        team_lineup.columns = [col.replace('.','_') for col in list(team_lineup)]\n",
    "        team_lineup['start_time'] = 0\n",
    "        team_lineup['end_time'] = game['time'].max()\n",
    "        team_lineup['sub_type'] = 'None'\n",
    "        lineup_df.append(team_lineup)\n",
    "        \n",
    "    lineup_df = pd.concat(lineup_df, axis=0)\n",
    "\n",
    "    return lineup_df.reset_index(drop=True)\n",
    "\n",
    "\n",
    "def extract_playing_time(game, comp_id, season_id):\n",
    "    \n",
    "    match_id = game['match_id'].mode()[0]\n",
    "    \n",
    "    game = add_game_clock(game)\n",
    "    starting_lineups = get_starting_lineups(game)\n",
    "    \n",
    "    lineup_df = add_subs(game, starting_lineups)\n",
    "    lineup_df['match_id'] = match_id\n",
    "    lineup_df['competition_id'] = comp_id\n",
    "    lineup_df['season_id'] = season_id\n",
    "    \n",
    "    # move to front of columns\n",
    "    cols = list(lineup_df)\n",
    "    cols.insert(0, cols.pop(cols.index('team_id')))\n",
    "    cols.insert(0, cols.pop(cols.index('season_id')))\n",
    "    cols.insert(0, cols.pop(cols.index('competition_id')))\n",
    "    cols.insert(0, cols.pop(cols.index('match_id')))\n",
    "    \n",
    "    lineup_df = lineup_df[cols].copy()\n",
    "    lineup_df = add_red_cards(game, lineup_df)\n",
    "    \n",
    "    lineup_df['playing_time'] = lineup_df['end_time'].copy()-lineup_df['start_time'].copy()\n",
    "    \n",
    "    positions = game.copy().groupby(['player_id']).agg({\n",
    "        'position_id':pd.Series.mode,\n",
    "        'position_name':pd.Series.mode\n",
    "    }).reset_index().rename(columns={\n",
    "        'position_id':'backup_position_id',\n",
    "        'position_name':'backup_position_name'\n",
    "    })\n",
    "\n",
    "    lineup_df = lineup_df.merge(positions, how='left', on=['player_id'])\n",
    "    lineup_df['position_id'] = lineup_df['position_id'].fillna(lineup_df['backup_position_id'].copy())\n",
    "    \n",
    "    lineup_df = lineup_df.dropna(subset=['backup_position_id']) # means they didn't record an action, probably barely on the field\n",
    "    lineup_df['pos_type'] = lineup_df['position_id'].apply(lambda x: type(x))\n",
    "    lineup_df['position_id'] = lineup_df['position_id'].apply(lambda x: list(x)[0] if isinstance(x, np.ndarray) else x) # rarely but sometimes is a list, # no real way of determining which list element is better\n",
    "    lineup_df['position_id'] = lineup_df['position_id'].astype(int)\n",
    "    lineup_df['position_name'] = lineup_df['position_name'].fillna(lineup_df['backup_position_name'].copy())\n",
    "    lineup_df = lineup_df.drop(columns=['backup_position_id','backup_position_name','pos_type'])\n",
    "    \n",
    "    team_1_id, team_2_id = list(lineup_df.team_id.unique())[0], list(lineup_df.team_id.unique())[1]\n",
    "    formation = game.loc[(game['team_id']==team_1_id)&(game['type_id']==35)]['tactics_formation'].values[0]\n",
    "    opp_formation = game.loc[(game['team_id']==team_2_id)&(game['type_id']==35)]['tactics_formation'].values[0]\n",
    "    formation_dict = {\n",
    "        team_1_id:formation,\n",
    "        team_2_id:opp_formation\n",
    "    }\n",
    "    lineup_df['team_formation'] = lineup_df['team_id'].map(formation_dict)\n",
    "\n",
    "\n",
    "    return lineup_df\n",
    "\n",
    "## use game vecs to get list of games\n",
    "def create_lineup_checklist(stf_schedule_):\n",
    "    \n",
    "    \"\"\" on external HD \"\"\"\n",
    "    gvecs = stf_schedule_[['datetime_UTC','match_id','team_id','opp_team_id','is_home','competition_id','season_id','match_date_UTC']].copy().merge(pd.read_csv(os.path.join(DROPBOX_PATH,'Statsbomb/game_vecs/game_vecs.csv'),usecols=['match_id','team_id','obv_diff']), how='left', on=['match_id','team_id'])\n",
    "    gvecs['datetime_UTC'] = pd.to_datetime(gvecs['datetime_UTC'])\n",
    "    gvecs = gvecs.dropna(subset=['obv_diff']) ## testing if we have good data\n",
    "    gvecs = gvecs.drop(columns=['obv_diff'])\n",
    "    gvecs['lineup_path'] = gvecs.apply(lambda x: os.path.join(DROPBOX_PATH, f'Statsbomb/raw/lineups/{x.competition_id}/{x.season_id}/{x.match_id}.json'), axis=1)\n",
    "    gvecs['as_path'] = gvecs.apply(lambda x: os.path.join(DROPBOX_PATH, f'Statsbomb/atomic_sparse/{x.competition_id}/{x.season_id}/{x.match_id}.csv'), axis=1)\n",
    "    gvecs['STF_path'] = gvecs.apply(lambda x: os.path.join(DROPBOX_PATH, f'Statsbomb/STF/{x.competition_id}/{x.season_id}/{x.match_id}-{x.team_id}.csv'), axis=1)\n",
    "    \n",
    "    gvecs['minutes_league_folder'] = gvecs.apply(lambda x: os.path.join(PLAYER_PATH,f'playing_time/{x.competition_id}'), axis=1)\n",
    "    gvecs['minutes_season_folder'] = gvecs.apply(lambda x: os.path.join(PLAYER_PATH,f'playing_time/{x.competition_id}/{x.season_id}'), axis=1)\n",
    "    gvecs['minutes_path'] = gvecs['minutes_season_folder'].copy() + '/' + gvecs['match_id'].copy().astype(str) + '.csv'\n",
    "    gvecs['path_exists'] = gvecs['minutes_path'].apply(lambda x: os.path.exists(x))\n",
    "    return gvecs\n",
    "\n",
    "def create_league_season_folders(gl):\n",
    "    \n",
    "    folders_needed = list(gl['minutes_league_folder'].unique())\n",
    "    for folder in folders_needed:\n",
    "        if not os.path.exists(folder):\n",
    "            os.mkdir(folder)\n",
    "            \n",
    "    folders_needed = list(gl['minutes_season_folder'].unique())\n",
    "    for folder in folders_needed:\n",
    "        if not os.path.exists(folder):\n",
    "            os.mkdir(folder)\n",
    "    \n",
    "    return\n",
    "\n",
    "\n",
    "def update_player_minutes():\n",
    "    \n",
    "    schedule_, stf_schedule_ = load_schedules()\n",
    "    game_list = create_lineup_checklist(stf_schedule_)\n",
    "    create_league_season_folders(game_list.copy())\n",
    "    \n",
    "    ## oops, don't need stf \n",
    "    game_list = game_list.drop_duplicates(subset=['match_id']).reset_index(drop=True)\n",
    "    \n",
    "    print(\"Updating player minutes....\")\n",
    "\n",
    "    for index, row in tqdm(game_list.iterrows(), total=len(game_list)):\n",
    "        if row['path_exists'] == False:\n",
    "            try:\n",
    "                sparse = pd.read_csv(row['as_path'])\n",
    "            except:\n",
    "                print(f\"Error on {row['match_id']}\")\n",
    "                continue\n",
    "            comp_id = row['competition_id']\n",
    "            season_id = row['season_id']\n",
    "            minutes_path = row['minutes_path']\n",
    "            lineup = extract_playing_time(sparse, comp_id, season_id)\n",
    "            lineup.to_csv(minutes_path, index=False)\n",
    "    \n",
    "    \n",
    "    return\n",
    "\n",
    "update_player_minutes()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7fdb1c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Blake\\AppData\\Local\\Temp\\ipykernel_24588\\2045045866.py:32: DtypeWarning: Columns (4,5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  normal = pd.read_csv(os.path.join(DROPBOX_PATH, 'schedules/processed_schedule.csv'))\n",
      "C:\\Users\\Blake\\AppData\\Local\\Temp\\ipykernel_24588\\2045045866.py:33: DtypeWarning: Columns (4,5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  stf = pd.read_csv(os.path.join(DROPBOX_PATH, 'schedules/stf_schedule.csv'))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "schedule, stf_schedule = load_schedules()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc6dd5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_lineups():\n",
    "    \n",
    "    return\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b283adb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def assemble_tskill_input(update=True):\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    return\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1090afb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|████████████████████████████████████████████████▏                          | 40526/63074 [02:32<00:51, 437.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minutes not found for D://_G_Street/player_model\\playing_time/104/91/3822006.csv\n",
      "minutes not found for D://_G_Street/player_model\\playing_time/104/91/3822007.csv\n",
      "minutes not found for D://_G_Street/player_model\\playing_time/104/91/3822008.csv\n",
      "minutes not found for D://_G_Street/player_model\\playing_time/104/91/3822009.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|████████████████████████████████████████████████▍                          | 40759/63074 [02:33<01:03, 351.28it/s]"
     ]
    }
   ],
   "source": [
    "\n",
    "player_map = {}\n",
    "stats = ['score_diff','xG_diff','xxG_diff','obv_diff']\n",
    "gvecs = pd.read_csv(os.path.join(DROPBOX_PATH, 'Statsbomb/game_vecs/game_vecs.csv'), usecols=['match_id','team_id','score_diff','xG_diff','xxG_diff','obv_diff'])\n",
    "games = create_lineup_checklist(stf_schedule)\n",
    "games = games.drop_duplicates(subset=['match_id'])\n",
    "games = games.merge(gvecs[['match_id','team_id']+stats], how='left',  on=['match_id','team_id'])\n",
    "games = games.sort_values(by='datetime_UTC')\n",
    "tskill_input = []\n",
    "results = {stat:[] for stat in stats}\n",
    "datetimes = []\n",
    "for index, row in tqdm(games.iterrows(), total=len(games)):\n",
    "    if row['is_home']==1:\n",
    "        home_team_id = row['team_id']\n",
    "        away_team_id = row['opp_team_id']\n",
    "    else:\n",
    "        home_team_id = row['opp_team_id']\n",
    "        away_team_id = row['team_id']\n",
    "    try:\n",
    "        minutes = pd.read_csv(row['minutes_path'])\n",
    "    except:\n",
    "        print(f\"minutes not found for {row['minutes_path']}\")\n",
    "        continue\n",
    "        \n",
    "    pmap_update = minutes.set_index('player_id')['player_name'].to_dict()\n",
    "    player_map.update(pmap_update)\n",
    "        \n",
    "    ## these track team specific home field\n",
    "    home_boost_id = str(home_team_id)+'_hfa' \n",
    "    ## could also do away id if there are a significant amount of neutral site games\n",
    "\n",
    "    ## trying 60 to see if garbage players stop showing up\n",
    "    minutes_threshold = 60 ## I think low is better, there is some implicit skill in being subbed on\n",
    "\n",
    "    contributors = minutes.copy().loc[minutes['playing_time']>minutes_threshold]\n",
    "    home_contributors = contributors.loc[contributors['team_id']==home_team_id].copy().reset_index(drop=True)\n",
    "    away_contributors = contributors.loc[contributors['team_id']==away_team_id].copy().reset_index(drop=True)\n",
    "     \n",
    "    ## I think a couple of games where they are cut short\n",
    "    if len(home_contributors) < 1:\n",
    "        continue\n",
    "    if len(away_contributors) < 1:\n",
    "        continue\n",
    "        \n",
    "    tskill_format = [list(home_contributors.player_id.values)+[home_boost_id], list(away_contributors.player_id.values)]\n",
    "    tskill_input.append(tskill_format)\n",
    "    datetimes.append(row['datetime_UTC'])\n",
    "    for stat in stats:\n",
    "        if row[stat] == 0:\n",
    "            results[stat].append([0,0])\n",
    "        elif row[stat] > 0:\n",
    "            results[stat].append([1,0])\n",
    "        else:\n",
    "            results[stat].append([0,1])\n",
    "            \n",
    "save_dict(player_map, os.path.join(DROPBOX_PATH, 'IDs/player_map'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740b8d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "p_draw = len(games.loc[games['score_diff']==0])/len(games)\n",
    "p_draw = len(games.loc[games['obv_diff']==0])/len(games)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea506358",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import gc\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c709f582",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import trueskillthroughtime as ttt\n",
    "\n",
    "## defaults: sigma = 6.0; beta = 1.0; gamma = 0.03;\n",
    "h = ttt.History(composition = tskill_input, results = results['obv_diff'], times = [d.timestamp()/(60*60*24) for d in datetimes], sigma=3, p_draw=p_draw)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c76153",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "current_rating = []\n",
    "for pid, ratings in h.learning_curves().items():\n",
    "    current_rating.append([pid, ratings[-1][-1].mu, ratings[-1][-1].sigma])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88e0521",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "current_rating = pd.DataFrame(current_rating, columns=['player_id', 'mu','sigma'])\n",
    "current_ratings = current_rating.loc[~current_rating['player_id'].astype(str).str.contains('_hfa')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a9d2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "current_ratings['player_name'] = current_ratings['player_id'].map(player_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8ec6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "current_ratings.sort_values(by='mu', ascending=False).head(60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff742404",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2eadf0e1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'game_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_24424\\730092381.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mgames\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgames\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop_duplicates\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'match_id'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'team_id'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mrow\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgame_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'is_home'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mhome_team_id\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'team_id'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'game_list' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5861c758",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6586e365",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
