{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "082ac5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import torch\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "\n",
    "DROPBOX_PATH = 'C:\\\\Users\\Blake\\G Street Dropbox\\Blake Atkinson\\shared_soccer_data\\data'\n",
    "\n",
    "def save_dict(di_, filename_):\n",
    "    with open(filename_, 'wb') as f:\n",
    "        pickle.dump(di_, f)\n",
    "\n",
    "def load_dict(filename_):\n",
    "    with open(filename_, 'rb') as f:\n",
    "        ret_di = pickle.load(f)\n",
    "    return ret_di\n",
    "\n",
    "teams = load_dict(os.path.join(DROPBOX_PATH, 'IDs/teams'))\n",
    "competitions = load_dict(os.path.join(DROPBOX_PATH, 'IDs/competitions'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6e9a83b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_schedules():\n",
    "    \n",
    "    normal = pd.read_csv(os.path.join(DROPBOX_PATH, 'schedules/processed_schedule.csv'))\n",
    "    stf = pd.read_csv(os.path.join(DROPBOX_PATH, 'schedules/stf_schedule.csv'))\n",
    "\n",
    "    normal['home_team_name'] = normal['home_team_id'].apply(lambda x: teams.get(x)['name'])\n",
    "    normal['away_team_name'] = normal['away_team_id'].apply(lambda x: teams.get(x)['name'])\n",
    "\n",
    "    stf['team_name'] = stf['team_id'].apply(lambda x: teams.get(x)['name'])\n",
    "    stf['opp_team_name'] = stf['opp_team_id'].apply(lambda x: teams.get(x)['name'])\n",
    "    \n",
    "    normal['datetime_UTC'] = pd.to_datetime(normal['datetime_UTC'].copy())\n",
    "    stf['datetime_UTC'] = pd.to_datetime(stf['datetime_UTC'].copy())\n",
    "\n",
    "    normal['match_date_UTC'] = normal['datetime_UTC'].copy().dt.date\n",
    "    stf['match_date_UTC'] = stf['datetime_UTC'].copy().dt.date\n",
    "    \n",
    "    normal['last_updated'] = pd.to_datetime(normal['last_updated'].copy())\n",
    "    stf['last_updated'] = pd.to_datetime(stf['last_updated'].copy())\n",
    "    \n",
    "    normal = normal.loc[~normal['match_status'].isin(['deleted','collecting','cancelled','postponed'])].reset_index(drop=True)\n",
    "    stf = stf.loc[~stf['match_status'].isin(['deleted','collecting','cancelled','postponed'])].reset_index(drop=True)\n",
    "    \n",
    "    return normal, stf \n",
    "\n",
    "\n",
    "cols = [\n",
    "    'id',\n",
    "    'homeID',\n",
    "    'awayID',\n",
    "    'season',\n",
    "    'status',\n",
    "    'roundID',\n",
    "    'date_unix', 'winningTeam', 'no_home_away',\n",
    "    'game_week',\n",
    "    'revised_game_week',\n",
    "    'homeGoalCount',\n",
    "    'awayGoalCount',\n",
    "    'attacks_recorded',\n",
    "    'team_a_yellow_cards',\n",
    "    'team_b_yellow_cards',\n",
    "    'team_a_red_cards',\n",
    "    'team_b_red_cards',\n",
    "    'team_a_shotsOnTarget',\n",
    "    'team_b_shotsOnTarget',\n",
    "    'team_a_shotsOffTarget',\n",
    "    'team_b_shotsOffTarget',\n",
    "    'team_a_shots',\n",
    "    'team_b_shots',\n",
    "    'team_a_fouls',\n",
    "    'team_b_fouls',\n",
    "    'team_a_possession',\n",
    "    'team_b_possession',\n",
    "    'team_a_offsides',\n",
    "    'team_b_offsides',\n",
    "    'team_a_dangerous_attacks',\n",
    "    'team_b_dangerous_attacks',\n",
    "    'team_a_attacks',\n",
    "    'team_b_attacks',\n",
    "    'team_a_xg',\n",
    "    'team_b_xg',\n",
    "    'total_xg',\n",
    "    'team_a_penalties_won',\n",
    "    'team_b_penalties_won',\n",
    "    'team_a_penalty_goals',\n",
    "    'team_b_penalty_goals',\n",
    "    'team_a_penalty_missed',\n",
    "    'team_b_penalty_missed',\n",
    "    'team_a_throwins',\n",
    "    'team_b_throwins',\n",
    "    'team_a_freekicks',\n",
    "    'team_b_freekicks',\n",
    "    'team_a_goalkicks',\n",
    "    'team_b_goalkicks',\n",
    "    'refereeID',\n",
    "    'coach_a_ID',\n",
    "    'coach_b_ID',\n",
    "    'stadium_name',\n",
    "    'stadium_location',\n",
    "    'odds_ft_1',\n",
    "    'odds_ft_x',\n",
    "    'odds_ft_2',\n",
    "    'attendance',\n",
    "    'match_url',\n",
    "    'competition_id',\n",
    "    'competition_name',\n",
    "    'avg_potential', 'home_url', 'home_image', 'home_name', 'away_url', 'away_image', 'away_name'\n",
    "]\n",
    "\n",
    "\n",
    "def load_footy():\n",
    "    \n",
    "    footy = pd.read_csv(os.path.join(DROPBOX_PATH, 'footy/aggregated.csv'))\n",
    "    footy['datetime_UTC'] = pd.to_datetime(footy['datetime_UTC'].copy())\n",
    "    footy['match_date_UTC'] = pd.to_datetime(footy['match_date_UTC'].copy())\n",
    "    footy = footy.drop_duplicates(subset=['id']).reset_index(drop=True)\n",
    "    footy = footy.loc[footy['match_date_UTC']<=datetime.utcnow()+pd.Timedelta(days=14)].reset_index(drop=True)\n",
    "    footy = footy.sort_values(by=['match_date_UTC','id']).reset_index(drop=True)\n",
    "    footy = footy[cols+['datetime_UTC','match_date_UTC']]\n",
    "    footy = footy.loc[footy['match_date_UTC']>=pd.to_datetime('2006-08-01')].reset_index(drop=True) # one outlier date\n",
    "    print(f\"Footy API has {len(footy)} matches\")\n",
    "    return footy\n",
    "\n",
    "\n",
    "def to_STF(tdata):\n",
    "    \n",
    "    tdata = tdata.rename(columns={\n",
    "        'homeGoalCount':'team_a_goals',\n",
    "        'awayGoalCount':'team_b_goals',\n",
    "        'homeID':'team_a_id',\n",
    "        'awayID':'team_b_id'\n",
    "    })\n",
    "    \n",
    "    unchanged_cols = ['match_date_UTC', 'game_week']\n",
    "    homes = tdata.copy()\n",
    "    aways = tdata.copy()\n",
    "    homes.columns=[col.replace('team_a_','team_') for col in list(homes)]\n",
    "    aways.columns=[col.replace('team_b_','team_') for col in list(aways)]\n",
    "    homes.columns=[col.replace('team_b_','opp_') for col in list(homes)]\n",
    "    aways.columns=[col.replace('team_a_','opp_') for col in list(aways)]\n",
    "    \n",
    "    homes['is_home'] = 1\n",
    "    aways['is_home'] = 0\n",
    "    tdata = pd.concat([homes, aways], axis=0).sort_values(by=['match_date_UTC','team_id']).reset_index(drop=True)\n",
    "    \n",
    "    return tdata\n",
    "\n",
    "def prep_footy_data(fdata, teams_footy2id):\n",
    "    \n",
    "    stf = to_STF(fdata)\n",
    "    stf = stf.loc[stf['status']=='complete'].reset_index(drop=True)\n",
    "    \n",
    "    # replace empty fields with nans\n",
    "    for col in ['team_shots','opp_shots','team_shotsOnTarget','opp_shotsOnTarget','team_possession','opp_possession']:\n",
    "        stf[col] = stf[col].copy().replace(-1, np.nan)\n",
    "\n",
    "    for col in ['team_attacks','opp_attacks','team_dangerous_attacks','opp_dangerous_attacks','team_xg','opp_xg',\n",
    "            'odds_ft_1','odds_ft_2','odds_ft_x']:\n",
    "        stf[col] = stf[col].copy().replace(0, np.nan)\n",
    "\n",
    "    stats = ['goals','shots','shotsOnTarget','possession','attacks','dangerous_attacks','xg']\n",
    "    \n",
    "    for stat in stats:\n",
    "        stf[f'{stat}_diff'] = stf[f'team_{stat}'].copy() - stf[f'opp_{stat}'].copy()\n",
    "        \n",
    "    pace_stats = ['goals','shots','shotsOnTarget','attacks','dangerous_attacks','xg']\n",
    "#     pace_thresholds = [2.5, 22, 10, 206, 102,3.05]\n",
    "    \n",
    "    for i,ps in enumerate(pace_stats):\n",
    "#         pace_threshold = pace_thresholds[i]\n",
    "        stf[f'{ps}_total'] = stf[f'team_{ps}'].copy()+stf[f'opp_{ps}'].copy()\n",
    "        \n",
    "    stf =stf.rename(columns={'team_id':'footy_team_id','opp_id':'footy_opp_id'})\n",
    "    stf['team_id'] =stf['footy_team_id'].map(teams_footy2id)\n",
    "    stf['opp_team_id'] =stf['footy_opp_id'].map(teams_footy2id)\n",
    "    \n",
    "    return stf\n",
    "\n",
    "\n",
    "def merge_game_ids(past, fresults, schedule):\n",
    "    \n",
    "    sb_matches = past.copy()[['match_date_UTC','datetime_UTC','match_id','team_id','opp_team_id']]\n",
    "    \n",
    "    sb_matches['match_date_UTC'] = pd.to_datetime(sb_matches['match_date_UTC'])\n",
    "\n",
    "    sb_matches = sb_matches.drop_duplicates(subset=['match_id','team_id'])\n",
    "    num_possible = len(sb_matches)\n",
    "    \n",
    "    combined = fresults.merge(sb_matches, how='left', on=['match_date_UTC','team_id','opp_team_id'])\n",
    "    \n",
    "    ## try fillna with one day higher (helps for 2,000 north american games)\n",
    "    sb_matches2 = sb_matches.copy()\n",
    "    sb_matches2['match_date_UTC'] = sb_matches2['match_date_UTC'] + pd.Timedelta(hours=24)\n",
    "    combined2 = fresults.merge(sb_matches2, how='left', on=['match_date_UTC', 'team_id', 'opp_team_id'])\n",
    "    combined['match_id'] = combined['match_id'].copy().fillna(combined2['match_id'].copy())\n",
    "    combined['team_id'] = combined['team_id'].copy().fillna(combined2['team_id'].copy())\n",
    "    combined['opp_team_id'] = combined['opp_team_id'].copy().fillna(combined2['opp_team_id'].copy())\n",
    "\n",
    "    ## try fillna with one day lower (japanese games maybe?)\n",
    "    sb_matches2 = sb_matches.copy()\n",
    "    sb_matches2['match_date_UTC'] = sb_matches2['match_date_UTC'] - pd.Timedelta(hours=24)\n",
    "    combined2 = fresults.merge(sb_matches2, how='left', on=['match_date_UTC', 'team_id', 'opp_team_id'])\n",
    "    combined['match_id'] = combined['match_id'].copy().fillna(combined2['match_id'].copy())\n",
    "    combined['team_id'] = combined['team_id'].copy().fillna(combined2['team_id'].copy())\n",
    "    combined['opp_team_id'] = combined['opp_team_id'].copy().fillna(combined2['opp_team_id'].copy())\n",
    "        \n",
    "    combined = combined.sort_values(by=['match_date_UTC','match_id','is_home'], ascending=[True, True, False])\n",
    "    total_added = len(combined.loc[combined['match_id'].notnull()])\n",
    "    print(f\"{np.round(total_added/num_possible, 3)*100}% of SB games matched\")\n",
    "    missing = schedule.loc[~schedule['match_id'].isin(list(combined.match_id.unique()))].reset_index(drop=True)\n",
    "    \n",
    "    return combined, missing\n",
    "\n",
    "def add_sb_metrics(data, stf_schedule):\n",
    "    \n",
    "    game_vecs = pd.read_csv(os.path.join(DROPBOX_PATH, 'Statsbomb/game_vecs/game_vecs.csv'))\n",
    "\n",
    "#     data = data.merge(game_vecs[['match_id','team_id',#'score_diff',# for testing merge\n",
    "#                                  'xxG_diff','obv_diff','pace','obv_pace']], how='left', on=['match_id','team_id'])\n",
    "    data = data.rename(columns={\n",
    "        'competition_id':'footy_comp_id',\n",
    "        'competition_name':'footy_comp_name'\n",
    "    })\n",
    "    before = len(data)\n",
    "    data = data.merge(game_vecs, how='outer', on=['match_id','team_id'])\n",
    "    after = len(data)\n",
    "    print(f\"{after-before} games with statsbomb stats but no footy\")\n",
    "    \n",
    "    meta = stf_schedule.copy()[['match_id','team_id','competition_id','season_id','match_date_UTC','datetime_UTC','opp_team_id','is_upcoming']]\n",
    "    meta = meta.rename(columns={'match_date_UTC':'backup_match_date'})\n",
    "    meta = meta.rename(columns={'datetime_UTC':'backup_datetime'})\n",
    "    meta = meta.rename(columns={'opp_team_id':'backup_opp_team_id'})\n",
    "    data = data.merge(meta, how='left', on=['match_id','team_id'])\n",
    "#     data = data.rename(columns={\n",
    "# #         'score_diff':'test', #todo: figure out why a handful of games have different score stats\n",
    "#         'xxG_diff':'xxg_side',\n",
    "#         'pace':'xxg_total',\n",
    "#         'obv_diff':'obv_side',\n",
    "#         'obv_pace':'obv_total'\n",
    "#     })\n",
    "    data['match_date_UTC'] = data['match_date_UTC'].fillna(data['backup_match_date'].copy())\n",
    "    data['datetime_UTC'] = data['datetime_UTC'].fillna(data['backup_datetime'].copy())\n",
    "    data['opp_team_id'] = data['opp_team_id'].fillna(data['backup_opp_team_id'].copy())\n",
    "    data = data.drop(columns=['backup_match_date', 'backup_datetime'])\n",
    "    \n",
    "    \n",
    "    return data\n",
    "\n",
    "def processing_steps(data, schedule):\n",
    "    \n",
    "    ## try to get agreement on attendance data\n",
    "    data.loc[data['attendance']==-1, 'attendance'] = np.nan\n",
    "    data = data.rename(columns={'attendance':'footy_attendance'})\n",
    "    data = data.merge(schedule.copy().rename(columns={'attendance':'sb_attendance'})[[\n",
    "        'match_id','sb_attendance'\n",
    "    ]], how='left', on=['match_id'])\n",
    "    ## make the attendance columns agree\n",
    "    data['attendance'] = data[['footy_attendance','sb_attendance']].mean(axis=1)\n",
    "    # 22 instances of this, and they are just small attendance numbers\n",
    "    data.loc[(data['footy_attendance']==0)&data['sb_attendance'].notnull(), 'attendance'] = data.sb_attendance\n",
    "    data['attendance'] = data['attendance'].fillna(data['sb_attendance'].copy())\n",
    "    data['attendance'] = data['attendance'].fillna(data['footy_attendance'].copy())\n",
    "    data['max_attendance'] = data.groupby(['stadium_name'])['attendance'].transform('max')\n",
    "    data['pct_attendance'] = data['attendance'].copy()/data['max_attendance'].copy()\n",
    "    data = data.drop(columns=['sb_attendance','footy_attendance','max_attendance'])\n",
    "    \n",
    "    ## for forward looking version, we will use market odds\n",
    "    # remove vig and drop one\n",
    "    data['inv_odds_ft_1'] = 1/data['odds_ft_1'].copy()\n",
    "    data['inv_odds_ft_x'] = 1/data['odds_ft_x'].copy()\n",
    "    data['inv_odds_ft_2'] = 1/data['odds_ft_2'].copy()\n",
    "    data['inv_odds_total'] = data[['inv_odds_ft_1','inv_odds_ft_x','inv_odds_ft_2']].copy().sum(axis=1)\n",
    "    data['odds_ft_1'] = 1/(data['inv_odds_ft_1'].copy()/data['inv_odds_total'].copy())\n",
    "    data['odds_ft_x'] = 1/(data['inv_odds_ft_x'].copy()/data['inv_odds_total'].copy())\n",
    "    data['odds_ft_2'] = 1/(data['inv_odds_ft_2'].copy()/data['inv_odds_total'].copy())\n",
    "    data['market_home_pct'] = data['inv_odds_ft_1'].copy()/data['inv_odds_total'].copy()\n",
    "    data['market_draw_pct'] = data['inv_odds_ft_x'].copy()/data['inv_odds_total'].copy()\n",
    "    data['market_away_pct'] = data['inv_odds_ft_2'].copy()/data['inv_odds_total'].copy()\n",
    "\n",
    "    data = data.drop(columns=['odds_ft_2']) # only need 2 of 3 way ML, last is implied\n",
    "    \n",
    "    \n",
    "    ## sb not in team/opp format, instead in diff/total format for key stats. recovering team/opp here.\n",
    "    target_pred_cols = ['score_diff', 'xG_diff', 'shot_diff', 'sot_diff','xxG_diff','obv_diff'] # man_adv_v2\n",
    "    tm_col_name_map = {\n",
    "        'score_diff':'sb_team_goals',\n",
    "        'xG_diff':'sb_team_xG',\n",
    "        'shot_diff':'sb_team_shots',\n",
    "        'sot_diff':'sb_team_SOTs',\n",
    "        'xxG_diff':'sb_team_xxG',\n",
    "        'obv_diff':'sb_team_obv'\n",
    "    }\n",
    "\n",
    "    tm_total_col_name = {\n",
    "        'score_diff':'score_total',\n",
    "        'xG_diff':'xG_total',\n",
    "        'shot_diff':'shot_total',\n",
    "        'sot_diff':'sot_total',\n",
    "        'xxG_diff':'pace',\n",
    "        'obv_diff':'obv_pace'\n",
    "    }\n",
    "\n",
    "    predict_cols = []\n",
    "    for tpc in tqdm(target_pred_cols):\n",
    "        tm_col_name = tm_col_name_map[tpc]\n",
    "        opp_col_name = tm_col_name.replace('team','opp')\n",
    "        predict_cols.extend([tm_col_name, opp_col_name])\n",
    "        total_col_name = tm_total_col_name[tpc]\n",
    "\n",
    "        data['tpc_temp'] = (data[total_col_name].copy()+1)*(data[tpc].copy())\n",
    "        data[tm_col_name] = (data['tpc_temp'].copy()+data[total_col_name].copy())/2\n",
    "        data[opp_col_name] = data[total_col_name].copy()-data[tm_col_name].copy()\n",
    "        data = data.drop(columns=['tpc_temp'])\n",
    "        \n",
    "    ## handful of instances where we're missing data from one source or the other\n",
    "    col_fillnas = {\n",
    "        'datetime_UTC':'footy_datetime_UTC',\n",
    "        'sb_team_goals':'team_goals',\n",
    "        'sb_opp_goals':'opp_goals',\n",
    "        'sb_team_xG':'team_xg',\n",
    "        'sb_opp_xG':'opp_xg',\n",
    "        'sb_team_shots':'team_shots',\n",
    "        'sb_opp_shots':'opp_shots',\n",
    "        'sb_team_SOTs':'team_shotsOnTarget',\n",
    "        'sb_opp_SOTs':'opp_shotsOnTarget'\n",
    "    }\n",
    "\n",
    "    for k,v in col_fillnas.items():\n",
    "        data[k] = data[k].copy().fillna(data[v].copy())\n",
    "        data[v] = data[v].copy().fillna(data[k].copy())\n",
    "\n",
    "\n",
    "    ## standardize season ids\n",
    "    data = data.drop(columns=['footy_datetime_UTC'])\n",
    "    season_map = data.groupby(['season'])['season_id'].apply(lambda x: pd.Series.mode(x)).reset_index().drop(columns='level_1').set_index('season_id').to_dict()['season']\n",
    "    data['backup_season'] = data['season_id'].map(season_map)\n",
    "    data['season'] = data['season'].fillna(data['backup_season'].copy())\n",
    "    data = data.drop(columns=['backup_season','season_id'])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bcd4433b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating non market game grades...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Blake\\AppData\\Local\\Temp\\ipykernel_14304\\1326865530.py:3: DtypeWarning: Columns (4,5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  normal = pd.read_csv(os.path.join(DROPBOX_PATH, 'schedules/processed_schedule.csv'))\n",
      "C:\\Users\\Blake\\AppData\\Local\\Temp\\ipykernel_14304\\1326865530.py:4: DtypeWarning: Columns (4,5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  stf = pd.read_csv(os.path.join(DROPBOX_PATH, 'schedules/stf_schedule.csv'))\n",
      "C:\\Users\\Blake\\AppData\\Local\\Temp\\ipykernel_14304\\1326865530.py:93: DtypeWarning: Columns (68,69,207,208,209,210,211,212,213) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  footy = pd.read_csv(os.path.join(DROPBOX_PATH, 'footy/aggregated.csv'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Footy API has 332597 matches\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "## gg 1: non market\n",
    "print(\"Creating non market game grades...\")\n",
    "### data prep ###\n",
    "teams = load_dict(os.path.join(DROPBOX_PATH, 'IDs/teams'))\n",
    "competitions = load_dict(os.path.join(DROPBOX_PATH, 'IDs/competitions'))\n",
    "\n",
    "schedule, stf_schedule = load_schedules()\n",
    "teams_id2footy = load_dict(os.path.join(DROPBOX_PATH, 'IDs/footy/teams_id2footy'))\n",
    "teams_footy2id = load_dict(os.path.join(DROPBOX_PATH, 'IDs/footy/teams_footy2id'))\n",
    "footy = load_footy()\n",
    "footy = prep_footy_data(footy.copy(), teams_footy2id)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "75fc192c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89.2% of SB games matched\n",
      "10993 games with statsbomb stats but no footy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:03<00:00,  1.64it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df, missing = merge_game_ids(stf_schedule.copy(), footy.copy().rename(columns={'datetime_UTC':'footy_datetime_UTC'}), schedule.copy())\n",
    "\n",
    "df = add_sb_metrics(df, stf_schedule)\n",
    "df = df.rename(columns={'id':'footy_match_id'})\n",
    "\n",
    "df = processing_steps(df, schedule.copy())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7d101ed8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>footy_match_id</th>\n",
       "      <th>footy_team_id</th>\n",
       "      <th>footy_opp_id</th>\n",
       "      <th>season</th>\n",
       "      <th>status</th>\n",
       "      <th>roundID</th>\n",
       "      <th>date_unix</th>\n",
       "      <th>winningTeam</th>\n",
       "      <th>no_home_away</th>\n",
       "      <th>game_week</th>\n",
       "      <th>...</th>\n",
       "      <th>sb_team_xG</th>\n",
       "      <th>sb_opp_xG</th>\n",
       "      <th>sb_team_shots</th>\n",
       "      <th>sb_opp_shots</th>\n",
       "      <th>sb_team_SOTs</th>\n",
       "      <th>sb_opp_SOTs</th>\n",
       "      <th>sb_team_xxG</th>\n",
       "      <th>sb_opp_xxG</th>\n",
       "      <th>sb_team_obv</th>\n",
       "      <th>sb_opp_obv</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1036757.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>2006/2007</td>\n",
       "      <td>complete</td>\n",
       "      <td>58573.0</td>\n",
       "      <td>1.155322e+09</td>\n",
       "      <td>38.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1036773.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>540.0</td>\n",
       "      <td>2006/2007</td>\n",
       "      <td>complete</td>\n",
       "      <td>58573.0</td>\n",
       "      <td>1.156086e+09</td>\n",
       "      <td>38.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1036781.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>127.0</td>\n",
       "      <td>2006/2007</td>\n",
       "      <td>complete</td>\n",
       "      <td>58573.0</td>\n",
       "      <td>1.156599e+09</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1036787.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>543.0</td>\n",
       "      <td>2006/2007</td>\n",
       "      <td>complete</td>\n",
       "      <td>58573.0</td>\n",
       "      <td>1.158413e+09</td>\n",
       "      <td>543.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1036799.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>6772.0</td>\n",
       "      <td>2006/2007</td>\n",
       "      <td>complete</td>\n",
       "      <td>58573.0</td>\n",
       "      <td>1.159018e+09</td>\n",
       "      <td>38.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>665034</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1.106945</td>\n",
       "      <td>1.563542</td>\n",
       "      <td>9.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-76.873749</td>\n",
       "      <td>101.495372</td>\n",
       "      <td>13.519763</td>\n",
       "      <td>3.112539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>665035</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.699907</td>\n",
       "      <td>1.307060</td>\n",
       "      <td>12.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>57.446543</td>\n",
       "      <td>-31.001758</td>\n",
       "      <td>6.076690</td>\n",
       "      <td>11.204780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>665036</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2022/2023</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.752409</td>\n",
       "      <td>1.283314</td>\n",
       "      <td>10.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>-58.286345</td>\n",
       "      <td>85.655994</td>\n",
       "      <td>-14.416333</td>\n",
       "      <td>34.932667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>665037</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2022/2023</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1.221010</td>\n",
       "      <td>1.194463</td>\n",
       "      <td>10.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-42.778625</td>\n",
       "      <td>71.672925</td>\n",
       "      <td>12.174533</td>\n",
       "      <td>6.891559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>665038</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2022/2023</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1.133681</td>\n",
       "      <td>0.932404</td>\n",
       "      <td>15.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-14.946094</td>\n",
       "      <td>39.464816</td>\n",
       "      <td>44.621532</td>\n",
       "      <td>-23.547579</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>665039 rows × 377 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        footy_match_id  footy_team_id  footy_opp_id     season    status  \\\n",
       "0            1036757.0           38.0          33.0  2006/2007  complete   \n",
       "1            1036773.0           38.0         540.0  2006/2007  complete   \n",
       "2            1036781.0           38.0         127.0  2006/2007  complete   \n",
       "3            1036787.0           38.0         543.0  2006/2007  complete   \n",
       "4            1036799.0           38.0        6772.0  2006/2007  complete   \n",
       "...                ...            ...           ...        ...       ...   \n",
       "665034             NaN            NaN           NaN       2023       NaN   \n",
       "665035             NaN            NaN           NaN       2023       NaN   \n",
       "665036             NaN            NaN           NaN  2022/2023       NaN   \n",
       "665037             NaN            NaN           NaN  2022/2023       NaN   \n",
       "665038             NaN            NaN           NaN  2022/2023       NaN   \n",
       "\n",
       "        roundID     date_unix  winningTeam  no_home_away  game_week  ...  \\\n",
       "0       58573.0  1.155322e+09         38.0           0.0        1.0  ...   \n",
       "1       58573.0  1.156086e+09         38.0           0.0        2.0  ...   \n",
       "2       58573.0  1.156599e+09         -1.0           0.0        3.0  ...   \n",
       "3       58573.0  1.158413e+09        543.0           0.0        4.0  ...   \n",
       "4       58573.0  1.159018e+09         38.0           0.0        5.0  ...   \n",
       "...         ...           ...          ...           ...        ...  ...   \n",
       "665034      NaN           NaN          NaN           NaN        NaN  ...   \n",
       "665035      NaN           NaN          NaN           NaN        NaN  ...   \n",
       "665036      NaN           NaN          NaN           NaN        NaN  ...   \n",
       "665037      NaN           NaN          NaN           NaN        NaN  ...   \n",
       "665038      NaN           NaN          NaN           NaN        NaN  ...   \n",
       "\n",
       "        sb_team_xG  sb_opp_xG  sb_team_shots  sb_opp_shots  sb_team_SOTs  \\\n",
       "0              NaN        NaN            NaN           NaN           NaN   \n",
       "1              NaN        NaN            NaN           NaN           NaN   \n",
       "2              NaN        NaN            NaN           NaN           NaN   \n",
       "3              NaN        NaN            NaN           NaN           NaN   \n",
       "4              NaN        NaN            NaN           NaN           NaN   \n",
       "...            ...        ...            ...           ...           ...   \n",
       "665034    1.106945   1.563542            9.0          20.0           4.0   \n",
       "665035    0.699907   1.307060           12.0           8.0           6.0   \n",
       "665036    0.752409   1.283314           10.0          16.0           6.0   \n",
       "665037    1.221010   1.194463           10.0          12.0           3.0   \n",
       "665038    1.133681   0.932404           15.0          13.0           8.0   \n",
       "\n",
       "        sb_opp_SOTs  sb_team_xxG  sb_opp_xxG  sb_team_obv  sb_opp_obv  \n",
       "0               NaN          NaN         NaN          NaN         NaN  \n",
       "1               NaN          NaN         NaN          NaN         NaN  \n",
       "2               NaN          NaN         NaN          NaN         NaN  \n",
       "3               NaN          NaN         NaN          NaN         NaN  \n",
       "4               NaN          NaN         NaN          NaN         NaN  \n",
       "...             ...          ...         ...          ...         ...  \n",
       "665034          5.0   -76.873749  101.495372    13.519763    3.112539  \n",
       "665035          1.0    57.446543  -31.001758     6.076690   11.204780  \n",
       "665036          7.0   -58.286345   85.655994   -14.416333   34.932667  \n",
       "665037          3.0   -42.778625   71.672925    12.174533    6.891559  \n",
       "665038          5.0   -14.946094   39.464816    44.621532  -23.547579  \n",
       "\n",
       "[665039 rows x 377 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d3897f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# sb_data = df.copy().loc[df['match_id'].notnull()].reset_index(drop=True)\n",
    "# sb_data = sb_data.merge(schedule[['match_id','season_id']], how='left')\n",
    "# non_sb_data = df.copy().loc[df['match_id'].isnull()].reset_index(drop=True)\n",
    "\n",
    "\n",
    "# stats = [\n",
    "#     'score_diff', 'xG_diff', 'shot_diff', 'sq_diff', 'sot_diff', 'score_total', 'xG_total', 'shot_total', 'sq_total', 'sot_total', 'sit_boost', 'pace', 'obv_pace', 'xxG_diff', 'pct_xxG_diff', 'obv_diff', 'pct_obv_diff', 'cgoal_skew', 'cgoal_kurt', 'cgoal_sum', 'cgoal_std', 'cconcede_skew', 'cconcede_kurt', 'cconcede_sum', 'cconcede_std', 'xxG_conversion', 'xG_conversion', 'opp_xG_conversion', 'opp_xxG_conversion', 'win_prob', 'draw_prob', 'opp_dt_eff', 'opp_mt_eff', 'opp_at_eff', 'opp_d_qual', 'opp_mid_qual', 'opp_atck_qual', 'team_dt_eff', 'team_mt_eff', 'team_at_eff', 'team_d_qual', 'team_mid_qual', 'team_atck_qual', 'opp_gk_pass_loc', 'opp_def_pass_loc', 'opp_mid_pass_loc', 'opp_atck_pass_loc', 'opp_gk_pass_angle', 'opp_def_pass_angle', 'opp_mid_pass_angle', 'opp_atck_pass_angle', 'opp_gk_pass_count', 'opp_def_pass_count', 'opp_mid_pass_count', 'opp_atck_pass_count', 'opp_gk_pass_dist', 'opp_def_pass_dist', 'opp_mid_pass_dist', 'opp_atck_pass_dist', 'opp_gk_pass_obv_vol', 'opp_def_pass_obv_vol', 'opp_mid_pass_obv_vol', 'opp_atck_pass_obv_vol', 'opp_gk_pass_obv_eff', 'opp_def_pass_obv_eff', 'opp_mid_pass_obv_eff', 'opp_atck_pass_obv_eff', 'team_gk_pass_loc', 'team_def_pass_loc', 'team_mid_pass_loc', 'team_atck_pass_loc', 'team_gk_pass_angle', 'team_def_pass_angle', 'team_mid_pass_angle', 'team_atck_pass_angle', 'team_gk_pass_count', 'team_def_pass_count', 'team_mid_pass_count', 'team_atck_pass_count', 'team_gk_pass_dist', 'team_def_pass_dist', 'team_mid_pass_dist', 'team_atck_pass_dist', 'team_gk_pass_obv_vol', 'team_def_pass_obv_vol', 'team_mid_pass_obv_vol', 'team_atck_pass_obv_vol', 'team_gk_pass_obv_eff', 'team_def_pass_obv_eff', 'team_mid_pass_obv_eff', 'team_atck_pass_obv_eff', 'opp_gk_carry_angle', 'opp_def_carry_angle', 'opp_mid_carry_angle', 'opp_atck_carry_angle', 'opp_gk_carry_count', 'opp_def_carry_count', 'opp_mid_carry_count', 'opp_atck_carry_count', 'opp_gk_carry_dist', 'opp_def_carry_dist', 'opp_mid_carry_dist', 'opp_atck_carry_dist', 'opp_gk_carry_vol', 'opp_def_carry_vol', 'opp_mid_carry_vol', 'opp_atck_carry_vol', 'opp_gk_carry_eff', 'opp_def_carry_eff', 'opp_mid_carry_eff', 'opp_atck_carry_eff', 'team_gk_carry_angle', 'team_def_carry_angle', 'team_mid_carry_angle', 'team_atck_carry_angle', 'team_gk_carry_dist', 'team_def_carry_dist', 'team_mid_carry_dist', 'team_atck_carry_dist', 'team_gk_carry_count', 'team_def_carry_count', 'team_mid_carry_count', 'team_atck_carry_count', 'team_gk_carry_vol', 'team_def_carry_vol', 'team_mid_carry_vol', 'team_atck_carry_vol', 'team_gk_carry_eff', 'team_def_carry_eff', 'team_mid_carry_eff', 'team_atck_carry_eff', 'opp_gk_dfn_pos', 'opp_def_dfn_pos', 'opp_mid_dfn_pos', 'opp_atck_dfn_pos', 'opp_gk_dfn_obv_vol', 'opp_def_dfn_obv_vol', 'opp_mid_dfn_obv_vol', 'opp_atck_dfn_obv_vol', 'opp_gk_dfn_obv_eff', 'opp_def_dfn_obv_eff', 'opp_mid_dfn_obv_eff', 'opp_atck_dfn_obv_eff', 'opp_gk_dfn_xxG_vol', 'opp_def_dfn_xxG_vol', 'opp_mid_dfn_xxG_vol', 'opp_atck_dfn_xxG_vol', 'team_gk_dfn_pos', 'team_def_dfn_pos', 'team_mid_dfn_pos', 'team_atck_dfn_pos', 'team_gk_dfn_obv_vol', 'team_def_dfn_obv_vol', 'team_mid_dfn_obv_vol', 'team_atck_dfn_obv_vol', 'team_gk_dfn_obv_eff', 'team_def_dfn_obv_eff', 'team_mid_dfn_obv_eff', 'team_atck_dfn_obv_eff', 'team_gk_dfn_xxG_vol', 'team_def_dfn_xxG_vol', 'team_mid_dfn_xxG_vol', 'team_atck_dfn_xxG_vol', 'team_weighted_fouls', 'opp_weighted_fouls', 'opp_corners', 'team_corners', 'team_crosses', 'opp_crosses', 'team_cross_pct', 'opp_cross_pct', 'team_pens', 'opp_pens', 'carry_lengths', 'gk_dist', 'fields_gained', 'fields_gained_comp', 'med_def_action', 'threat_pp', 'dthreat_pp', 'off_embed_0', 'off_embed_1', 'off_embed_2', 'off_embed_3', 'off_embed_4', 'off_embed_5', 'def_embed_0', 'def_embed_1', 'def_embed_2', 'def_embed_3', 'def_embed_4', 'def_embed_5', 'opp_poss', 'opp_ppp', 'opp_spp', 'team_poss', 'team_ppp', 'team_spp', 'team_poss_start', 'opp_poss_start', 'team_poss_len', 'opp_poss_len', 'team_poss_width', 'opp_poss_width', 'team_poss_time_sum', 'opp_poss_time_sum', 'team_poss_time_median', 'opp_poss_time_median', 'oxG_f3', 'txG_f3', 'pct_lead', 'pct_tied', 'pct_trail', 'man_adv', 'team_dx_sec', 'team_xxG_sec', 'opp_dx_sec', 'opp_xxG_sec', 'opp_d3_passes', 'opp_d3_comp%', 'opp_d3_fcomp%', 'opp_m3_passes', 'opp_m3_comp%', 'opp_m3_fcomp%', 'opp_a3_passes', 'opp_a3_comp%', 'opp_a3_fcomp%', 'team_d3_passes', 'team_d3_comp%', 'team_d3_fcomp%', 'team_m3_passes', 'team_m3_comp%', 'team_m3_fcomp%', 'team_a3_passes', 'team_a3_comp%', 'team_a3_fcomp%', 'opp_SOT%', 'team_save%', 'team_SOT%', 'opp_save%', 'team_XGOT/SOT', 'opp_XGOT/SOT', 'opp_end_d3', 'opp_end_m3', 'opp_end_a3', 'team_end_d3', 'team_end_m3', 'team_end_a3', 'team_switches', 'opp_switches', 'opp_d3_press', 'opp_m3_press', 'opp_a3_press', 'team_d3_press', 'team_m3_press', 'team_a3_press', 'opp_wide_poss', 'team_wide_poss',\n",
    "# ]\n",
    "# stats+=[\n",
    "# 'team_goals', 'opp_goals', 'attacks_recorded', 'team_yellow_cards', 'opp_yellow_cards', 'team_red_cards', 'opp_red_cards', 'team_shotsOnTarget', 'opp_shotsOnTarget', 'team_shotsOffTarget', 'opp_shotsOffTarget', 'team_shots', 'opp_shots', 'team_fouls', 'opp_fouls', 'team_possession', 'opp_possession', 'team_offsides', 'opp_offsides', 'team_dangerous_attacks', 'opp_dangerous_attacks', 'team_attacks', 'opp_attacks', 'team_xg', 'opp_xg', 'total_xg', 'team_penalties_won', 'opp_penalties_won', 'team_penalty_goals', 'opp_penalty_goals', 'team_penalty_missed', 'opp_penalty_missed', 'team_throwins', 'opp_throwins', 'team_freekicks', 'opp_freekicks', 'team_goalkicks', 'opp_goalkicks'\n",
    "# ]\n",
    "# stats+=[\n",
    "#     'goals_total', 'shots_total', 'shotsOnTarget_total', 'attacks_total', 'dangerous_attacks_total', 'xg_total'\n",
    "# ]\n",
    "\n",
    "# stats+=[\n",
    "#     'avg_potential','goals_diff', 'shots_diff', 'shotsOnTarget_diff', 'possession_diff', 'attacks_diff', 'dangerous_attacks_diff', 'xg_diff','no_home_away','is_home'\n",
    "# ]\n",
    "\n",
    "# # leaky for what happened in past\n",
    "# # stats+=[\n",
    "# #     'odds_ft_1', 'odds_ft_x', 'odds_ft_2'\n",
    "# # ]\n",
    "\n",
    "# # found to not be useful # these were useful for pace\n",
    "# # 171           team_pens    0.000000\n",
    "# # 172            opp_pens    0.000000\n",
    "# # 283  team_penalties_won    0.000000\n",
    "# for stat in ['opp_penalty_goals','team_penalty_goals','team_penalties_won','opp_penalties_won','opp_penalty_missed','attacks_recorded','goals_total']:\n",
    "#     stats.remove(stat)\n",
    "\n",
    "# total_non_sb = len(non_sb_data)\n",
    "# # drop cols where 93% nulls\n",
    "# threshold = int(0.93*total_non_sb)\n",
    "# footy_stats = stats.copy()\n",
    "# to_drop = list(non_sb_data[stats].isnull().sum()[non_sb_data[stats].isnull().sum()>threshold].index)\n",
    "# for td in to_drop:\n",
    "#     footy_stats.remove(td)\n",
    "# print(non_sb_data.shape)\n",
    "# non_sb_data = non_sb_data.drop(columns=to_drop)\n",
    "# print(non_sb_data.shape)\n",
    "# non_sb_data = non_sb_data.drop(columns=['team_id','opp_team_id','match_id','competition_id','is_upcoming'])\n",
    "# print(list(non_sb_data))\n",
    "# def prep_non_sb_data(data):\n",
    "\n",
    "#     data['target_temp'] = data['team_goals'].copy()-data['opp_goals'].copy()\n",
    "#     data['target_2_temp'] = data['team_goals'].copy()+data['opp_goals'].copy()\n",
    "#     data['target_num_games'] = data.groupby(['footy_team_id','season'])['target_temp'].transform('count')\n",
    "#     data['target_szn_sum'] = data.groupby(['footy_team_id','season'])['target_temp'].transform('sum')\n",
    "#     data['target_2_szn_sum'] = data.groupby(['footy_team_id','season'])['target_2_temp'].transform('sum')\n",
    "#     data['target'] = (data['target_szn_sum'].copy()-data['target_temp'])/(data['target_num_games'].copy()-1)\n",
    "#     data['target_2'] = (data['target_2_szn_sum'].copy()-data['target_2_temp'])/(data['target_num_games'].copy()-1)\n",
    "\n",
    "#     data = data.drop(columns=['target_temp','target_2_temp','target_num_games','target_szn_sum','target_2_szn_sum'])\n",
    "#     data = data.dropna(subset=['target','target_2'])\n",
    "\n",
    "#     return data\n",
    "\n",
    "\n",
    "# non_sb_data = prep_non_sb_data(non_sb_data)\n",
    "\n",
    "# def prep_sb_data(data):\n",
    "\n",
    "#     data['target_temp'] = data['team_goals'].copy()-data['opp_goals'].copy()\n",
    "#     data['target_2_temp'] = data['team_goals'].copy()+data['opp_goals'].copy()\n",
    "#     data['target_num_games'] = data.groupby(['team_id','season_id'])['target_temp'].transform('count')\n",
    "#     data['target_szn_sum'] = data.groupby(['team_id','season_id'])['target_temp'].transform('sum')\n",
    "#     data['target_2_szn_sum'] = data.groupby(['team_id','season_id'])['target_2_temp'].transform('sum')\n",
    "#     data['target'] = (data['target_szn_sum'].copy()-data['target_temp'])/(data['target_num_games'].copy()-1)\n",
    "#     data['target_2'] = (data['target_2_szn_sum'].copy()-data['target_2_temp'])/(data['target_num_games'].copy()-1)\n",
    "#     data = data.drop(columns=['target_temp','target_2_temp','target_num_games','target_szn_sum','target_2_szn_sum'])\n",
    "#     data = data.dropna(subset=['target'])\n",
    "\n",
    "#     return data\n",
    "\n",
    "# sb_data = prep_sb_data(sb_data)\n",
    "\n",
    "# sb_data = sb_data.drop(columns=['season_id'])\n",
    "# non_sb_data = non_sb_data.drop(columns=['season'])\n",
    "\n",
    "# def add_game_grades_non_sb(data):\n",
    "\n",
    "#     # # ### for getting feature importance\n",
    "#     X = data[footy_stats+['target','target_2']].copy()\n",
    "#     # X = X.sample(frac=1)\n",
    "#     y1 = X['target'].copy()\n",
    "#     y2 = X['target_2'].copy()\n",
    "#     X = X.drop(columns=['target','target_2'])\n",
    "\n",
    "#     eval_cv = KFold(3, shuffle=True, random_state=17)\n",
    "#     prod_cv = KFold(10, shuffle=True, random_state=17)\n",
    "#     reg = catboost.CatBoostRegressor(verbose=False)\n",
    "#     # 08/23/22 Score diff cross val 0.17366522630199602\n",
    "#     print(\"Score diff cross val\", np.mean(cross_val_score(reg, X, y1, cv=eval_cv)))\n",
    "#     # # option 1\n",
    "#     reg = catboost.CatBoostRegressor(verbose=False)\n",
    "#     reg.fit(X, y1)\n",
    "#     feat_importances = pd.DataFrame({\n",
    "#         'stats':footy_stats,\n",
    "#         'importance':reg.feature_importances_\n",
    "#     }).sort_values(by='importance',ascending=False)\n",
    "#     print(feat_importances)\n",
    "\n",
    "#     data['game_score'] = cross_val_predict(reg, X, y1, cv=prod_cv)\n",
    "#     reg = catboost.CatBoostRegressor(verbose=False)\n",
    "\n",
    "#     # 08/23/22 Pace cross val 0.23575666835862108\n",
    "#     print(\"Pace cross val\", np.mean(cross_val_score(reg, X, y2, cv=eval_cv)))\n",
    "#     data['pace_score'] = cross_val_predict(reg, X, y2, cv=prod_cv)\n",
    "\n",
    "#     reg = catboost.CatBoostRegressor(verbose=False)\n",
    "#     reg.fit(X, y2)\n",
    "#     feat_importances = pd.DataFrame({\n",
    "#         'stats':footy_stats,\n",
    "#         'importance':reg.feature_importances_\n",
    "#     }).sort_values(by='importance',ascending=False)\n",
    "#     print(feat_importances)\n",
    "\n",
    "#     return data\n",
    "\n",
    "\n",
    "# non_sb_data = add_game_grades_non_sb(non_sb_data)\n",
    "\n",
    "# def add_game_grades_sb(data, show_score=False):\n",
    "\n",
    "#     # # ### for getting feature importance\n",
    "#     X = data[stats+['target','target_2']].copy()\n",
    "#     # X = X.sample(frac=1)\n",
    "#     y1 = X['target'].copy()\n",
    "#     y2 = X['target_2'].copy()\n",
    "#     X = X.drop(columns=['target','target_2'])\n",
    "\n",
    "#     eval_cv = KFold(3, shuffle=True, random_state=17)\n",
    "# #     np.mean(cross_val_score(reg, X, y1, cv=eval_cv))\n",
    "#     prod_cv = KFold(6, shuffle=True, random_state=17)\n",
    "\n",
    "# #     sd_params = {'iterations': 19524,\n",
    "# #      'od_wait': 1270,\n",
    "# #      'learning_rate': 0.011194201649297929,\n",
    "# #      'reg_lambda': 44.63399671384952,\n",
    "# #      'subsample': 0.7496120387317952,\n",
    "# #      'random_strength': 33.75970628327746,\n",
    "# #      'depth': 6,\n",
    "# #      'min_data_in_leaf': 18,\n",
    "# #      'leaf_estimation_iterations': 5}\n",
    "#     reg = catboost.CatBoostRegressor(verbose=False)#, **sd_params)\n",
    "#     # # option 2\n",
    "#     # Score diff cross val 0.2784195893187421 # did better without opt?\n",
    "#     if show_score:\n",
    "#         print(\"Score diff cross val\", np.mean(cross_val_score(reg, X, y1, cv=eval_cv)))\n",
    "#     # # option 1\n",
    "#     reg = catboost.CatBoostRegressor(verbose=False)#,**sd_params)\n",
    "#     reg.fit(X, y1)\n",
    "#     feat_importances = pd.DataFrame({\n",
    "#         'stats':stats,\n",
    "#         'importance':reg.feature_importances_\n",
    "#     }).sort_values(by='importance',ascending=False)\n",
    "#     print(feat_importances)\n",
    "\n",
    "#     data['game_score'] = cross_val_predict(reg, X, y1, cv=prod_cv)\n",
    "#     reg = catboost.CatBoostRegressor(verbose=False)\n",
    "# #     Pace cross val 0.33348896960501095\n",
    "#     if show_score:\n",
    "#         print(\"Pace cross val\", np.mean(cross_val_score(reg, X, y2, cv=eval_cv)))\n",
    "#     data['pace_score'] = cross_val_predict(reg, X, y2, cv=prod_cv)\n",
    "\n",
    "#     reg = catboost.CatBoostRegressor(verbose=False)\n",
    "#     reg.fit(X, y2)\n",
    "#     feat_importances = pd.DataFrame({\n",
    "#         'stats':stats,\n",
    "#         'importance':reg.feature_importances_\n",
    "#     }).sort_values(by='importance',ascending=False)\n",
    "#     print(feat_importances)\n",
    "\n",
    "#     return data\n",
    "\n",
    "\n",
    "# sb_data = add_game_grades_sb(sb_data)\n",
    "\n",
    "# non_sb_scores = non_sb_data.copy()[['footy_match_id','footy_team_id','footy_opp_id','match_date_UTC',\n",
    "#                             'game_score','pace_score']]\n",
    "# sb_game_scores = sb_data.copy()[['footy_match_id','footy_team_id','footy_opp_id','match_date_UTC','match_id','team_id','opp_team_id',\n",
    "#                             'game_score','pace_score']]\n",
    "\n",
    "# game_scores = pd.concat([sb_game_scores, non_sb_scores], axis=0).sort_values(by='match_date_UTC').reset_index(drop=True)\n",
    "\n",
    "# def prep_for_network(df):\n",
    "\n",
    "#     opp_data = df.copy().rename(columns={'id':'opp_id','opp_id':'id'})\n",
    "#     opp_data = opp_data.rename(columns={'game_score':'opp_game_score','pace_score':'opp_pace_score'})\n",
    "\n",
    "#     df = df.merge(opp_data[['match_date','footy_match_id','id','opp_id']+['opp_game_score','opp_pace_score']], how='left', on=['match_date','footy_match_id','id','opp_id'])\n",
    "#     # # just do two networks for now (later more)\n",
    "#     df['rtg_avg'] = (df['game_score'].copy() + (-1*df['opp_game_score'].copy()))/2\n",
    "#     df['pace_avg'] = (df['pace_score'].copy() + (df['opp_pace_score'].copy()))/2\n",
    "\n",
    "\n",
    "#     min_date = df.match_date.min()\n",
    "#     # # helper col\n",
    "#     df['rating_period'] = df['match_date'].copy().rank(method='dense').astype(int)\n",
    "#     df['date_since_inception'] = (df['match_date'].copy()-min_date).dt.days\n",
    "#     df['days_since_last'] = df['date_since_inception'].diff()\n",
    "#     df['game_no'] = df.groupby(['id'])['footy_match_id'].transform('cumcount')\n",
    "#     df['opp_game_no'] = df.groupby(['opp_id'])['footy_match_id'].transform('cumcount')\n",
    "#     df = df.drop_duplicates(subset=['footy_match_id']).reset_index(drop=True)\n",
    "\n",
    "#     ## only using statsbomb teams\n",
    "#     df['backup_team_id'] = df['id'].map(teams_footy2id)\n",
    "#     df['backup_opp_id'] = df['opp_id'].map(teams_footy2id)\n",
    "#     df['team_id'] = df['team_id'].fillna(df['backup_team_id'].copy())\n",
    "#     df['opp_team_id'] = df['opp_team_id'].fillna(df['backup_opp_id'].copy())\n",
    "#     df = df.drop(columns=['backup_team_id','backup_opp_id'])\n",
    "\n",
    "\n",
    "#     return df\n",
    "\n",
    "# game_scores = game_scores.rename(columns={'footy_team_id':'id', 'footy_opp_id':'opp_id','match_date_UTC':'match_date'})\n",
    "# game_scores = prep_for_network(game_scores)\n",
    "\n",
    "# # create network\n",
    "# df = game_scores.copy().dropna(subset=['team_id','opp_team_id','rtg_avg','pace_avg'])\n",
    "# df['days_since_last'] = df['days_since_last'].fillna(1)# one case\n",
    "# df = df.drop(columns=['id','opp_id'])\n",
    "# teams = list(set(list(df.team_id.unique())+list(df.opp_team_id.unique())))\n",
    "# num_teams = len(teams)\n",
    "# teams.sort()\n",
    "# team_map = {}\n",
    "# for idx, team in enumerate(teams):\n",
    "#     team_map[team] = idx\n",
    "\n",
    "# prank_mat = np.zeros((num_teams,num_teams))\n",
    "# neg_prank_mat = np.zeros((num_teams,num_teams))\n",
    "# pace_mat = np.ones((num_teams, num_teams))*2.5\n",
    "# pace_std = np.ones((num_teams, num_teams))\n",
    "# rating_periods = list(df.rating_period.unique())\n",
    "# df.match_date = pd.to_datetime(df.match_date.copy())\n",
    "# df = df.sort_values(by='match_date')\n",
    "\n",
    "\n",
    "# def calc_ratings(protag_matrix):\n",
    "\n",
    "#     N = biggest_index = protag_matrix.shape[0]    \n",
    "#     d = 5e-3\n",
    "#     A = (d * protag_matrix + (1 - d) / N)    \n",
    "#     v = np.repeat(1/biggest_index, biggest_index)\n",
    "#     for i in range(150):\n",
    "#         v = A@v\n",
    "#         norm = np.linalg.norm(v)\n",
    "#         v = v/norm\n",
    "\n",
    "#     return v\n",
    "\n",
    "# history = []\n",
    "# for rp in tqdm(rating_periods):\n",
    "#     rating_update = np.zeros((num_teams, num_teams))\n",
    "#     neg_rating_update = np.zeros((num_teams, num_teams))\n",
    "#     pace_update = np.zeros((num_teams, num_teams))\n",
    "#     pace_std_update = np.zeros((num_teams, num_teams))\n",
    "#     rp_data = df.copy().loc[df.rating_period==rp].reset_index(drop=True)\n",
    "#     days_since = rp_data.days_since_last.copy().max()\n",
    "#     if days_since < 1:\n",
    "#         days_since = 1\n",
    "\n",
    "#     # time decay the matrix\n",
    "#     prank_mat *= np.exp(-(1/150)*days_since)\n",
    "#     neg_prank_mat *= np.exp(-(1/150)*days_since)\n",
    "#     pace_mat *= np.exp(-(1/125)*days_since)\n",
    "#     pace_std *= np.exp(-(1/125)*days_since)\n",
    "\n",
    "#     ratings_vec = calc_ratings(prank_mat)\n",
    "#     neg_ratings_vec = calc_ratings(neg_prank_mat)\n",
    "\n",
    "#     pace_vec = calc_ratings(pace_mat.copy()/pace_std.copy())\n",
    "# #     ratings_entr = entropy(prank_mat)\n",
    "# #     pace_entr = entropy(pace_mat.copy()/pace_std.copy())\n",
    "#     for index, row in rp_data.iterrows():\n",
    "#         match_id = row['match_id']\n",
    "#         protag_id = row['team_id']\n",
    "#         antag_id = row['opp_team_id']\n",
    "#         protag_index = team_map[protag_id]\n",
    "#         antag_index = team_map[antag_id]\n",
    "#         ## grab ratings going into games\n",
    "#         protag_rating = ratings_vec[protag_index] \n",
    "#         antag_rating = ratings_vec[antag_index]\n",
    "#         protag_neg_rating = neg_ratings_vec[protag_index]\n",
    "#         antag_neg_rating = neg_ratings_vec[antag_index]\n",
    "# #         protag_entr = ratings_entr[protag_index]\n",
    "# #         antag_entr = ratings_entr[antag_index]\n",
    "#         protag_pace = pace_vec[protag_index]\n",
    "#         antag_pace = pace_vec[antag_index]\n",
    "# #         protag_pentr = pace_entr[protag_index]\n",
    "# #         antag_pentr = pace_entr[antag_index]\n",
    "#         history.append([match_id, protag_id, antag_id, protag_rating, antag_rating, protag_neg_rating,antag_neg_rating,#protag_entr, antag_entr,\n",
    "#                         protag_pace, antag_pace]) #, protag_pentr, antag_pentr])\n",
    "\n",
    "#         # update\n",
    "#         game_rating = row['rtg_avg']\n",
    "#         pace_rating = row['pace_avg']\n",
    "#         if np.isnan(game_rating):\n",
    "#             continue\n",
    "#         if np.isnan(pace_rating):\n",
    "#             continue\n",
    "\n",
    "#         if game_rating > 0:\n",
    "#             rating_update[protag_index][antag_index]+=np.abs(game_rating)\n",
    "#             neg_rating_update[antag_index][protag_index]+=np.abs(game_rating)\n",
    "#         else:\n",
    "#             rating_update[antag_index][protag_index]+=np.abs(game_rating)\n",
    "#             neg_rating_update[protag_index][antag_index]+=np.abs(game_rating)\n",
    "#         pace_update[protag_index][antag_index] += pace_rating\n",
    "#         pace_update[antag_index][protag_index] += pace_rating\n",
    "#         pace_std_update[protag_index][antag_index] += 1\n",
    "#         pace_std_update[antag_index][protag_index] += 1\n",
    "\n",
    "#     prank_mat+=rating_update\n",
    "#     neg_prank_mat+=neg_rating_update\n",
    "#     pace_mat += pace_update\n",
    "#     pace_std+= pace_std_update\n",
    "\n",
    "# history = pd.DataFrame(history, columns=['match_id','id','opp_id','rating','opp_rating','neg_rating','neg_opp_rating',#'entropy','opp_entropy',\n",
    "#                                         'pace','opp_pace'])#,'pace_entropy','opp_pace_entropy'])\n",
    "# history = history.merge(stf_schedule[['match_id','team_id','score','opp_score']].rename(columns={'team_id':'id'}),\n",
    "#                     how='left', on=['match_id','id'])\n",
    "\n",
    "# history['score_diff'] = history['score'].copy()-history['opp_score'].copy()\n",
    "# history['ratings_diff'] = history['rating'].copy()-history['opp_rating'].copy()\n",
    "# history['ratings_v2'] = history['rating'].copy()-history['neg_rating'].copy()\n",
    "# history['opp_ratings_v2'] = history['opp_rating'].copy()-history['neg_opp_rating'].copy()\n",
    "# history['rtg_diff_v2'] = history['ratings_v2'].copy()-history['opp_ratings_v2'].copy()\n",
    "# history['score_total'] = history['score'].copy()+history['opp_score'].copy()\n",
    "# history['pace_comb'] = history['pace'].copy()+history['opp_pace'].copy()\n",
    "\n",
    "# # \n",
    "# to_save = history.copy().dropna(subset=['match_id']).reset_index(drop=True)\n",
    "# to_save = to_save.drop(columns=['score','opp_score','score_diff','score_total'])\n",
    "# to_save = to_save.rename(columns={'id':'team_id','opp_id':'opp_team_id'})\n",
    "# to_save_opp = to_save.copy().reset_index(drop=True)\n",
    "# to_save_opp.columns=['match_id','opp_team_id','team_id','opp_rating','rating','neg_opp_rating','neg_rating',\n",
    "#                     #'opp_entropy','entropy',\n",
    "#                     'opp_pace','pace',\n",
    "#                     #'opp_pace_entropy','pace_entropy',\n",
    "#                     'ratings_diff','opp_ratings_v2','ratings_v2','rtg_diff_v2','pace_comb']\n",
    "# to_save_opp = to_save_opp[list(to_save)]\n",
    "# to_save_opp['ratings_diff'] = to_save_opp['ratings_diff'].copy()*-1\n",
    "# to_save_opp['rtg_diff_v2'] = to_save_opp['rtg_diff_v2'].copy()*-1\n",
    "# to_save = pd.concat([to_save, to_save_opp], axis=0).reset_index(drop=True)\n",
    "# to_save.sort_values(by=['match_id'])\n",
    "# to_save = to_save.drop(columns=['opp_rating','rating','neg_opp_rating','neg_rating','ratings_diff'])\n",
    "\n",
    "# def get_current_ratings():\n",
    "\n",
    "#     current_rtgs = []\n",
    "#     for protag_id,protag_index, in team_map.items():\n",
    "#         protag_index = team_map[protag_id]\n",
    "#         ## grab ratings going into games\n",
    "#         protag_rating = ratings_vec[protag_index] \n",
    "#         protag_neg_rating = neg_ratings_vec[protag_index]\n",
    "#         #protag_entr = ratings_entr[protag_index]\n",
    "#         protag_pace = pace_vec[protag_index]\n",
    "#         #protag_pentr = pace_entr[protag_index]\n",
    "\n",
    "#         current_rtgs.append([protag_id, protag_rating, protag_neg_rating, protag_pace]) #, protag_entr, protag_pace, protag_pentr])\n",
    "\n",
    "#     return pd.DataFrame(current_rtgs, columns=['team_id','rating','neg_rating','pace'])#,'entropy','pace','pace_entropy'])\n",
    "# #     protag_id = row['team_id']\n",
    "\n",
    "# current_ratings = get_current_ratings()\n",
    "# opp_current_ratings = current_ratings.copy()\n",
    "# opp_current_ratings.columns=['opp_team_id','opp_rating','neg_opp_rating','opp_pace']#'opp_entropy','opp_pace','opp_pace_entropy']\n",
    "# upc = stf_schedule.copy().loc[stf_schedule['is_upcoming']==1].reset_index(drop=True)[['match_id','team_id','opp_team_id']]\n",
    "\n",
    "\n",
    "\n",
    "# upc = upc.merge(current_ratings.copy(), how='left', on=['team_id'])\n",
    "# upc = upc.merge(opp_current_ratings.copy(), how='left', on='opp_team_id')\n",
    "# # upc['ratings_diff'] = upc['rating'].copy()-upc['opp_rating'].copy()\n",
    "# upc['ratings_v2'] = upc['rating'].copy()-upc['neg_rating'].copy()\n",
    "# upc['opp_ratings_v2'] = upc['opp_rating'].copy()-upc['neg_opp_rating'].copy()\n",
    "# upc['rtg_diff_v2'] = upc['ratings_v2'].copy()-upc['opp_ratings_v2'].copy()\n",
    "# upc = upc.drop(columns=['opp_rating','rating','neg_opp_rating','neg_rating'])\n",
    "# upc['pace_comb'] = upc['pace'].copy()+upc['opp_pace'].copy()\n",
    "# to_save = pd.concat([to_save, upc], axis=0).reset_index(drop=True)\n",
    "\n",
    "# to_save.to_csv(os.path.join(DROPBOX_PATH, 'team_ratings/grade_network_no_mkt.csv'), index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31ad554",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f467c8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b65e3b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c38cd6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a50832",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e49ed8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7252456",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e0b9ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111360a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca57b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "\n",
    "easiest to handle as own file, although could be dissected into data_process and data_model_prep with some work\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "import math\n",
    "import time\n",
    "import pickle\n",
    "import random\n",
    "import urllib\n",
    "import catboost\n",
    "import simdkalman\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests as req\n",
    "\n",
    "from copy import copy\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "from scipy.stats import entropy\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import log_loss,mean_squared_error\n",
    "from fuzzywuzzy import fuzz, process\n",
    "from sklearn.metrics import make_scorer\n",
    "from datetime import datetime, timedelta\n",
    "from bayes_opt import BayesianOptimization\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.linear_model import LogisticRegression,LinearRegression\n",
    "from sklearn.model_selection import KFold, cross_val_score, cross_val_predict\n",
    "\n",
    "from multiprocessing import Process\n",
    "\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "\n",
    "DROPBOX_PATH = 'C:\\\\Users\\Blake\\G Street Dropbox\\Blake Atkinson\\shared_soccer_data\\data'\n",
    "\n",
    "def save_dict(di_, filename_):\n",
    "    with open(filename_, 'wb') as f:\n",
    "        pickle.dump(di_, f)\n",
    "\n",
    "def load_dict(filename_):\n",
    "    with open(filename_, 'rb') as f:\n",
    "        ret_di = pickle.load(f)\n",
    "    return ret_di\n",
    "\n",
    "\n",
    "teams = load_dict(os.path.join(DROPBOX_PATH, 'IDs/teams'))\n",
    "competitions = load_dict(os.path.join(DROPBOX_PATH, 'IDs/competitions'))\n",
    "def load_schedules():\n",
    "    \n",
    "    normal = pd.read_csv(os.path.join(DROPBOX_PATH, 'schedules/processed_schedule.csv'))\n",
    "    stf = pd.read_csv(os.path.join(DROPBOX_PATH, 'schedules/stf_schedule.csv'))\n",
    "\n",
    "    normal['home_team_name'] = normal['home_team_id'].apply(lambda x: teams.get(x)['name'])\n",
    "    normal['away_team_name'] = normal['away_team_id'].apply(lambda x: teams.get(x)['name'])\n",
    "\n",
    "    stf['team_name'] = stf['team_id'].apply(lambda x: teams.get(x)['name'])\n",
    "    stf['opp_team_name'] = stf['opp_team_id'].apply(lambda x: teams.get(x)['name'])\n",
    "    \n",
    "    normal['datetime_UTC'] = pd.to_datetime(normal['datetime_UTC'].copy())\n",
    "    stf['datetime_UTC'] = pd.to_datetime(stf['datetime_UTC'].copy())\n",
    "\n",
    "    normal['match_date_UTC'] = normal['datetime_UTC'].copy().dt.date\n",
    "    stf['match_date_UTC'] = stf['datetime_UTC'].copy().dt.date\n",
    "    \n",
    "    normal['last_updated'] = pd.to_datetime(normal['last_updated'].copy())\n",
    "    stf['last_updated'] = pd.to_datetime(stf['last_updated'].copy())\n",
    "    \n",
    "    normal = normal.loc[~normal['match_status'].isin(['deleted','collecting','cancelled','postponed'])].reset_index(drop=True)\n",
    "    stf = stf.loc[~stf['match_status'].isin(['deleted','collecting','cancelled','postponed'])].reset_index(drop=True)\n",
    "    \n",
    "    return normal, stf \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### there's a lot more in the raw data, not much I found useful though ###\n",
    "\n",
    "cols = [\n",
    "    'id',\n",
    "    'homeID',\n",
    "    'awayID',\n",
    "    'season',\n",
    "    'status',\n",
    "    'roundID',\n",
    "    'date_unix', 'winningTeam', 'no_home_away',\n",
    "    'game_week',\n",
    "    'revised_game_week',\n",
    "    'homeGoalCount',\n",
    "    'awayGoalCount',\n",
    "    'attacks_recorded',\n",
    "    'team_a_yellow_cards',\n",
    "    'team_b_yellow_cards',\n",
    "    'team_a_red_cards',\n",
    "    'team_b_red_cards',\n",
    "    'team_a_shotsOnTarget',\n",
    "    'team_b_shotsOnTarget',\n",
    "    'team_a_shotsOffTarget',\n",
    "    'team_b_shotsOffTarget',\n",
    "    'team_a_shots',\n",
    "    'team_b_shots',\n",
    "    'team_a_fouls',\n",
    "    'team_b_fouls',\n",
    "    'team_a_possession',\n",
    "    'team_b_possession',\n",
    "    'team_a_offsides',\n",
    "    'team_b_offsides',\n",
    "    'team_a_dangerous_attacks',\n",
    "    'team_b_dangerous_attacks',\n",
    "    'team_a_attacks',\n",
    "    'team_b_attacks',\n",
    "    'team_a_xg',\n",
    "    'team_b_xg',\n",
    "    'total_xg',\n",
    "    'team_a_penalties_won',\n",
    "    'team_b_penalties_won',\n",
    "    'team_a_penalty_goals',\n",
    "    'team_b_penalty_goals',\n",
    "    'team_a_penalty_missed',\n",
    "    'team_b_penalty_missed',\n",
    "    'team_a_throwins',\n",
    "    'team_b_throwins',\n",
    "    'team_a_freekicks',\n",
    "    'team_b_freekicks',\n",
    "    'team_a_goalkicks',\n",
    "    'team_b_goalkicks',\n",
    "    'refereeID',\n",
    "    'coach_a_ID',\n",
    "    'coach_b_ID',\n",
    "    'stadium_name',\n",
    "    'stadium_location',\n",
    "    'odds_ft_1',\n",
    "    'odds_ft_x',\n",
    "    'odds_ft_2',\n",
    "    'attendance',\n",
    "    'match_url',\n",
    "    'competition_id',\n",
    "    'competition_name',\n",
    "    'avg_potential', 'home_url', 'home_image', 'home_name', 'away_url', 'away_image', 'away_name'\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def load_footy():\n",
    "    \n",
    "    footy = pd.read_csv(os.path.join(DROPBOX_PATH, 'footy/aggregated.csv'))\n",
    "    footy['datetime_UTC'] = pd.to_datetime(footy['datetime_UTC'].copy())\n",
    "    footy['match_date_UTC'] = pd.to_datetime(footy['match_date_UTC'].copy())\n",
    "    footy = footy.drop_duplicates(subset=['id']).reset_index(drop=True)\n",
    "    footy = footy.loc[footy['match_date_UTC']<=datetime.utcnow()+pd.Timedelta(days=14)].reset_index(drop=True)\n",
    "    footy = footy.sort_values(by=['match_date_UTC','id']).reset_index(drop=True)\n",
    "    footy = footy[cols+['datetime_UTC','match_date_UTC']]\n",
    "    footy = footy.loc[footy['match_date_UTC']>=pd.to_datetime('2006-08-01')].reset_index(drop=True) # one outlier date\n",
    "    print(f\"Footy API has {len(footy)} matches\")\n",
    "    return footy\n",
    "\n",
    "\n",
    "def to_STF(tdata):\n",
    "    \n",
    "    tdata = tdata.rename(columns={\n",
    "        'homeGoalCount':'team_a_goals',\n",
    "        'awayGoalCount':'team_b_goals',\n",
    "        'homeID':'team_a_id',\n",
    "        'awayID':'team_b_id'\n",
    "    })\n",
    "    \n",
    "    unchanged_cols = ['match_date_UTC', 'game_week']\n",
    "    homes = tdata.copy()\n",
    "    aways = tdata.copy()\n",
    "    homes.columns=[col.replace('team_a_','team_') for col in list(homes)]\n",
    "    aways.columns=[col.replace('team_b_','team_') for col in list(aways)]\n",
    "    homes.columns=[col.replace('team_b_','opp_') for col in list(homes)]\n",
    "    aways.columns=[col.replace('team_a_','opp_') for col in list(aways)]\n",
    "    \n",
    "    homes['is_home'] = 1\n",
    "    aways['is_home'] = 0\n",
    "    tdata = pd.concat([homes, aways], axis=0).sort_values(by=['match_date_UTC','team_id']).reset_index(drop=True)\n",
    "    \n",
    "    return tdata\n",
    "\n",
    "def prep_footy_data(fdata, teams_footy2id):\n",
    "    \n",
    "    stf = to_STF(fdata)\n",
    "    stf = stf.loc[stf['status']=='complete'].reset_index(drop=True)\n",
    "    \n",
    "    # replace empty fields with nans\n",
    "    for col in ['team_shots','opp_shots','team_shotsOnTarget','opp_shotsOnTarget','team_possession','opp_possession']:\n",
    "        stf[col] = stf[col].copy().replace(-1, np.nan)\n",
    "\n",
    "    for col in ['team_attacks','opp_attacks','team_dangerous_attacks','opp_dangerous_attacks','team_xg','opp_xg',\n",
    "            'odds_ft_1','odds_ft_2','odds_ft_x']:\n",
    "        stf[col] = stf[col].copy().replace(0, np.nan)\n",
    "\n",
    "    stats = ['goals','shots','shotsOnTarget','possession','attacks','dangerous_attacks','xg']\n",
    "    \n",
    "    for stat in stats:\n",
    "        stf[f'{stat}_diff'] = stf[f'team_{stat}'].copy() - stf[f'opp_{stat}'].copy()\n",
    "        \n",
    "    pace_stats = ['goals','shots','shotsOnTarget','attacks','dangerous_attacks','xg']\n",
    "#     pace_thresholds = [2.5, 22, 10, 206, 102,3.05]\n",
    "    \n",
    "    for i,ps in enumerate(pace_stats):\n",
    "#         pace_threshold = pace_thresholds[i]\n",
    "        stf[f'{ps}_total'] = stf[f'team_{ps}'].copy()+stf[f'opp_{ps}'].copy()\n",
    "        \n",
    "    stf =stf.rename(columns={'team_id':'footy_team_id','opp_id':'footy_opp_id'})\n",
    "    stf['team_id'] =stf['footy_team_id'].map(teams_footy2id)\n",
    "    stf['opp_team_id'] =stf['footy_opp_id'].map(teams_footy2id)\n",
    "    \n",
    "    return stf\n",
    "\n",
    "\n",
    "def merge_game_ids(past, fresults, schedule):\n",
    "    \n",
    "    sb_matches = past.copy()[['match_date_UTC','datetime_UTC','match_id','team_id','opp_team_id']]\n",
    "    \n",
    "    sb_matches['match_date_UTC'] = pd.to_datetime(sb_matches['match_date_UTC'])\n",
    "\n",
    "    sb_matches = sb_matches.drop_duplicates(subset=['match_id','team_id'])\n",
    "    num_possible = len(sb_matches)\n",
    "    \n",
    "    combined = fresults.merge(sb_matches, how='left', on=['match_date_UTC','team_id','opp_team_id'])\n",
    "    \n",
    "    ## try fillna with one day higher (helps for 2,000 north american games)\n",
    "    sb_matches2 = sb_matches.copy()\n",
    "    sb_matches2['match_date_UTC'] = sb_matches2['match_date_UTC'] + pd.Timedelta(hours=24)\n",
    "    combined2 = fresults.merge(sb_matches2, how='left', on=['match_date_UTC', 'team_id', 'opp_team_id'])\n",
    "    combined['match_id'] = combined['match_id'].copy().fillna(combined2['match_id'].copy())\n",
    "    combined['team_id'] = combined['team_id'].copy().fillna(combined2['team_id'].copy())\n",
    "    combined['opp_team_id'] = combined['opp_team_id'].copy().fillna(combined2['opp_team_id'].copy())\n",
    "\n",
    "    ## try fillna with one day lower (japanese games maybe?)\n",
    "    sb_matches2 = sb_matches.copy()\n",
    "    sb_matches2['match_date_UTC'] = sb_matches2['match_date_UTC'] - pd.Timedelta(hours=24)\n",
    "    combined2 = fresults.merge(sb_matches2, how='left', on=['match_date_UTC', 'team_id', 'opp_team_id'])\n",
    "    combined['match_id'] = combined['match_id'].copy().fillna(combined2['match_id'].copy())\n",
    "    combined['team_id'] = combined['team_id'].copy().fillna(combined2['team_id'].copy())\n",
    "    combined['opp_team_id'] = combined['opp_team_id'].copy().fillna(combined2['opp_team_id'].copy())\n",
    "        \n",
    "    combined = combined.sort_values(by=['match_date_UTC','match_id','is_home'], ascending=[True, True, False])\n",
    "    total_added = len(combined.loc[combined['match_id'].notnull()])\n",
    "    print(f\"{np.round(total_added/num_possible, 3)*100}% of SB games matched\")\n",
    "    missing = schedule.loc[~schedule['match_id'].isin(list(combined.match_id.unique()))].reset_index(drop=True)\n",
    "    \n",
    "    return combined, missing\n",
    "\n",
    "def add_sb_metrics(data, stf_schedule):\n",
    "    \n",
    "    game_vecs = pd.read_csv(os.path.join(DROPBOX_PATH, 'Statsbomb/game_vecs/game_vecs.csv'))\n",
    "\n",
    "#     data = data.merge(game_vecs[['match_id','team_id',#'score_diff',# for testing merge\n",
    "#                                  'xxG_diff','obv_diff','pace','obv_pace']], how='left', on=['match_id','team_id'])\n",
    "    data = data.rename(columns={\n",
    "        'competition_id':'footy_comp_id',\n",
    "        'competition_name':'footy_comp_name'\n",
    "    })\n",
    "    before = len(data)\n",
    "    data = data.merge(game_vecs, how='outer', on=['match_id','team_id'])\n",
    "    after = len(data)\n",
    "    print(f\"{after-before} games with statsbomb stats but no footy\")\n",
    "    \n",
    "    meta = stf_schedule.copy()[['match_id','team_id','competition_id','season_id','match_date_UTC','datetime_UTC','opp_team_id','is_upcoming']]\n",
    "    meta = meta.rename(columns={'match_date_UTC':'backup_match_date'})\n",
    "    meta = meta.rename(columns={'datetime_UTC':'backup_datetime'})\n",
    "    meta = meta.rename(columns={'opp_team_id':'backup_opp_team_id'})\n",
    "    data = data.merge(meta, how='left', on=['match_id','team_id'])\n",
    "#     data = data.rename(columns={\n",
    "# #         'score_diff':'test', #todo: figure out why a handful of games have different score stats\n",
    "#         'xxG_diff':'xxg_side',\n",
    "#         'pace':'xxg_total',\n",
    "#         'obv_diff':'obv_side',\n",
    "#         'obv_pace':'obv_total'\n",
    "#     })\n",
    "    data['match_date_UTC'] = data['match_date_UTC'].fillna(data['backup_match_date'].copy())\n",
    "    data['datetime_UTC'] = data['datetime_UTC'].fillna(data['backup_datetime'].copy())\n",
    "    data['opp_team_id'] = data['opp_team_id'].fillna(data['backup_opp_team_id'].copy())\n",
    "    data = data.drop(columns=['backup_match_date', 'backup_datetime'])\n",
    "    \n",
    "    \n",
    "    return data\n",
    "\n",
    "def processing_steps(data, schedule):\n",
    "    \n",
    "    ## try to get agreement on attendance data\n",
    "    data.loc[data['attendance']==-1, 'attendance'] = np.nan\n",
    "    data = data.rename(columns={'attendance':'footy_attendance'})\n",
    "    data = data.merge(schedule.copy().rename(columns={'attendance':'sb_attendance'})[[\n",
    "        'match_id','sb_attendance'\n",
    "    ]], how='left', on=['match_id'])\n",
    "    ## make the attendance columns agree\n",
    "    data['attendance'] = data[['footy_attendance','sb_attendance']].mean(axis=1)\n",
    "    # 22 instances of this, and they are just small attendance numbers\n",
    "    data.loc[(data['footy_attendance']==0)&data['sb_attendance'].notnull(), 'attendance'] = data.sb_attendance\n",
    "    data['attendance'] = data['attendance'].fillna(data['sb_attendance'].copy())\n",
    "    data['attendance'] = data['attendance'].fillna(data['footy_attendance'].copy())\n",
    "    data['max_attendance'] = data.groupby(['stadium_name'])['attendance'].transform('max')\n",
    "    data['pct_attendance'] = data['attendance'].copy()/data['max_attendance'].copy()\n",
    "    data = data.drop(columns=['sb_attendance','footy_attendance','max_attendance'])\n",
    "    \n",
    "    ## for forward looking version, we will use market odds\n",
    "    # remove vig and drop one\n",
    "    data['inv_odds_ft_1'] = 1/data['odds_ft_1'].copy()\n",
    "    data['inv_odds_ft_x'] = 1/data['odds_ft_x'].copy()\n",
    "    data['inv_odds_ft_2'] = 1/data['odds_ft_2'].copy()\n",
    "    data['inv_odds_total'] = data[['inv_odds_ft_1','inv_odds_ft_x','inv_odds_ft_2']].copy().sum(axis=1)\n",
    "    data['odds_ft_1'] = 1/(data['inv_odds_ft_1'].copy()/data['inv_odds_total'].copy())\n",
    "    data['odds_ft_x'] = 1/(data['inv_odds_ft_x'].copy()/data['inv_odds_total'].copy())\n",
    "    data['odds_ft_2'] = 1/(data['inv_odds_ft_2'].copy()/data['inv_odds_total'].copy())\n",
    "    data['market_home_pct'] = data['inv_odds_ft_1'].copy()/data['inv_odds_total'].copy()\n",
    "    data['market_draw_pct'] = data['inv_odds_ft_x'].copy()/data['inv_odds_total'].copy()\n",
    "    data['market_away_pct'] = data['inv_odds_ft_2'].copy()/data['inv_odds_total'].copy()\n",
    "\n",
    "    data = data.drop(columns=['odds_ft_2']) # only need 2 of 3 way ML, last is implied\n",
    "    \n",
    "    \n",
    "    ## sb not in team/opp format, instead in diff/total format for key stats. recovering team/opp here.\n",
    "    target_pred_cols = ['score_diff', 'xG_diff', 'shot_diff', 'sot_diff','xxG_diff','obv_diff'] # man_adv_v2\n",
    "    tm_col_name_map = {\n",
    "        'score_diff':'sb_team_goals',\n",
    "        'xG_diff':'sb_team_xG',\n",
    "        'shot_diff':'sb_team_shots',\n",
    "        'sot_diff':'sb_team_SOTs',\n",
    "        'xxG_diff':'sb_team_xxG',\n",
    "        'obv_diff':'sb_team_obv'\n",
    "    }\n",
    "\n",
    "    tm_total_col_name = {\n",
    "        'score_diff':'score_total',\n",
    "        'xG_diff':'xG_total',\n",
    "        'shot_diff':'shot_total',\n",
    "        'sot_diff':'sot_total',\n",
    "        'xxG_diff':'pace',\n",
    "        'obv_diff':'obv_pace'\n",
    "    }\n",
    "\n",
    "    predict_cols = []\n",
    "    for tpc in tqdm(target_pred_cols):\n",
    "        tm_col_name = tm_col_name_map[tpc]\n",
    "        opp_col_name = tm_col_name.replace('team','opp')\n",
    "        predict_cols.extend([tm_col_name, opp_col_name])\n",
    "        total_col_name = tm_total_col_name[tpc]\n",
    "\n",
    "        data['tpc_temp'] = (data[total_col_name].copy()+1)*(data[tpc].copy())\n",
    "        data[tm_col_name] = (data['tpc_temp'].copy()+data[total_col_name].copy())/2\n",
    "        data[opp_col_name] = data[total_col_name].copy()-data[tm_col_name].copy()\n",
    "        data = data.drop(columns=['tpc_temp'])\n",
    "        \n",
    "    ## handful of instances where we're missing data from one source or the other\n",
    "    col_fillnas = {\n",
    "        'datetime_UTC':'footy_datetime_UTC',\n",
    "        'sb_team_goals':'team_goals',\n",
    "        'sb_opp_goals':'opp_goals',\n",
    "        'sb_team_xG':'team_xg',\n",
    "        'sb_opp_xG':'opp_xg',\n",
    "        'sb_team_shots':'team_shots',\n",
    "        'sb_opp_shots':'opp_shots',\n",
    "        'sb_team_SOTs':'team_shotsOnTarget',\n",
    "        'sb_opp_SOTs':'opp_shotsOnTarget'\n",
    "    }\n",
    "\n",
    "    for k,v in col_fillnas.items():\n",
    "        data[k] = data[k].copy().fillna(data[v].copy())\n",
    "        data[v] = data[v].copy().fillna(data[k].copy())\n",
    "\n",
    "\n",
    "    ## standardize season ids\n",
    "    data = data.drop(columns=['footy_datetime_UTC'])\n",
    "    season_map = data.groupby(['season'])['season_id'].apply(lambda x: pd.Series.mode(x)).reset_index().drop(columns='level_1').set_index('season_id').to_dict()['season']\n",
    "    data['backup_season'] = data['season_id'].map(season_map)\n",
    "    data['season'] = data['season'].fillna(data['backup_season'].copy())\n",
    "    data = data.drop(columns=['backup_season','season_id'])\n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def gg1():\n",
    "    ## gg 1: non market\n",
    "    print(\"Creating non market game grades...\")\n",
    "    ### data prep ###\n",
    "    teams = load_dict(os.path.join(DROPBOX_PATH, 'IDs/teams'))\n",
    "    competitions = load_dict(os.path.join(DROPBOX_PATH, 'IDs/competitions'))\n",
    "    \n",
    "    schedule, stf_schedule = load_schedules()\n",
    "    teams_id2footy = load_dict(os.path.join(DROPBOX_PATH, 'IDs/footy/teams_id2footy'))\n",
    "    teams_footy2id = load_dict(os.path.join(DROPBOX_PATH, 'IDs/footy/teams_footy2id'))\n",
    "    footy = load_footy()\n",
    "    footy = prep_footy_data(footy.copy(), teams_footy2id)\n",
    "\n",
    "    df, missing = merge_game_ids(stf_schedule.copy(), footy.copy().rename(columns={'datetime_UTC':'footy_datetime_UTC'}), schedule.copy())\n",
    "\n",
    "    df = add_sb_metrics(df, stf_schedule)\n",
    "    df = df.rename(columns={'id':'footy_match_id'})\n",
    "\n",
    "    df = processing_steps(df, schedule.copy())\n",
    "\n",
    "    sb_data = df.copy().loc[df['match_id'].notnull()].reset_index(drop=True)\n",
    "    sb_data = sb_data.merge(schedule[['match_id','season_id']], how='left')\n",
    "    non_sb_data = df.copy().loc[df['match_id'].isnull()].reset_index(drop=True)\n",
    "\n",
    "\n",
    "    stats = [\n",
    "        'score_diff', 'xG_diff', 'shot_diff', 'sq_diff', 'sot_diff', 'score_total', 'xG_total', 'shot_total', 'sq_total', 'sot_total', 'sit_boost', 'pace', 'obv_pace', 'xxG_diff', 'pct_xxG_diff', 'obv_diff', 'pct_obv_diff', 'cgoal_skew', 'cgoal_kurt', 'cgoal_sum', 'cgoal_std', 'cconcede_skew', 'cconcede_kurt', 'cconcede_sum', 'cconcede_std', 'xxG_conversion', 'xG_conversion', 'opp_xG_conversion', 'opp_xxG_conversion', 'win_prob', 'draw_prob', 'opp_dt_eff', 'opp_mt_eff', 'opp_at_eff', 'opp_d_qual', 'opp_mid_qual', 'opp_atck_qual', 'team_dt_eff', 'team_mt_eff', 'team_at_eff', 'team_d_qual', 'team_mid_qual', 'team_atck_qual', 'opp_gk_pass_loc', 'opp_def_pass_loc', 'opp_mid_pass_loc', 'opp_atck_pass_loc', 'opp_gk_pass_angle', 'opp_def_pass_angle', 'opp_mid_pass_angle', 'opp_atck_pass_angle', 'opp_gk_pass_count', 'opp_def_pass_count', 'opp_mid_pass_count', 'opp_atck_pass_count', 'opp_gk_pass_dist', 'opp_def_pass_dist', 'opp_mid_pass_dist', 'opp_atck_pass_dist', 'opp_gk_pass_obv_vol', 'opp_def_pass_obv_vol', 'opp_mid_pass_obv_vol', 'opp_atck_pass_obv_vol', 'opp_gk_pass_obv_eff', 'opp_def_pass_obv_eff', 'opp_mid_pass_obv_eff', 'opp_atck_pass_obv_eff', 'team_gk_pass_loc', 'team_def_pass_loc', 'team_mid_pass_loc', 'team_atck_pass_loc', 'team_gk_pass_angle', 'team_def_pass_angle', 'team_mid_pass_angle', 'team_atck_pass_angle', 'team_gk_pass_count', 'team_def_pass_count', 'team_mid_pass_count', 'team_atck_pass_count', 'team_gk_pass_dist', 'team_def_pass_dist', 'team_mid_pass_dist', 'team_atck_pass_dist', 'team_gk_pass_obv_vol', 'team_def_pass_obv_vol', 'team_mid_pass_obv_vol', 'team_atck_pass_obv_vol', 'team_gk_pass_obv_eff', 'team_def_pass_obv_eff', 'team_mid_pass_obv_eff', 'team_atck_pass_obv_eff', 'opp_gk_carry_angle', 'opp_def_carry_angle', 'opp_mid_carry_angle', 'opp_atck_carry_angle', 'opp_gk_carry_count', 'opp_def_carry_count', 'opp_mid_carry_count', 'opp_atck_carry_count', 'opp_gk_carry_dist', 'opp_def_carry_dist', 'opp_mid_carry_dist', 'opp_atck_carry_dist', 'opp_gk_carry_vol', 'opp_def_carry_vol', 'opp_mid_carry_vol', 'opp_atck_carry_vol', 'opp_gk_carry_eff', 'opp_def_carry_eff', 'opp_mid_carry_eff', 'opp_atck_carry_eff', 'team_gk_carry_angle', 'team_def_carry_angle', 'team_mid_carry_angle', 'team_atck_carry_angle', 'team_gk_carry_dist', 'team_def_carry_dist', 'team_mid_carry_dist', 'team_atck_carry_dist', 'team_gk_carry_count', 'team_def_carry_count', 'team_mid_carry_count', 'team_atck_carry_count', 'team_gk_carry_vol', 'team_def_carry_vol', 'team_mid_carry_vol', 'team_atck_carry_vol', 'team_gk_carry_eff', 'team_def_carry_eff', 'team_mid_carry_eff', 'team_atck_carry_eff', 'opp_gk_dfn_pos', 'opp_def_dfn_pos', 'opp_mid_dfn_pos', 'opp_atck_dfn_pos', 'opp_gk_dfn_obv_vol', 'opp_def_dfn_obv_vol', 'opp_mid_dfn_obv_vol', 'opp_atck_dfn_obv_vol', 'opp_gk_dfn_obv_eff', 'opp_def_dfn_obv_eff', 'opp_mid_dfn_obv_eff', 'opp_atck_dfn_obv_eff', 'opp_gk_dfn_xxG_vol', 'opp_def_dfn_xxG_vol', 'opp_mid_dfn_xxG_vol', 'opp_atck_dfn_xxG_vol', 'team_gk_dfn_pos', 'team_def_dfn_pos', 'team_mid_dfn_pos', 'team_atck_dfn_pos', 'team_gk_dfn_obv_vol', 'team_def_dfn_obv_vol', 'team_mid_dfn_obv_vol', 'team_atck_dfn_obv_vol', 'team_gk_dfn_obv_eff', 'team_def_dfn_obv_eff', 'team_mid_dfn_obv_eff', 'team_atck_dfn_obv_eff', 'team_gk_dfn_xxG_vol', 'team_def_dfn_xxG_vol', 'team_mid_dfn_xxG_vol', 'team_atck_dfn_xxG_vol', 'team_weighted_fouls', 'opp_weighted_fouls', 'opp_corners', 'team_corners', 'team_crosses', 'opp_crosses', 'team_cross_pct', 'opp_cross_pct', 'team_pens', 'opp_pens', 'carry_lengths', 'gk_dist', 'fields_gained', 'fields_gained_comp', 'med_def_action', 'threat_pp', 'dthreat_pp', 'off_embed_0', 'off_embed_1', 'off_embed_2', 'off_embed_3', 'off_embed_4', 'off_embed_5', 'def_embed_0', 'def_embed_1', 'def_embed_2', 'def_embed_3', 'def_embed_4', 'def_embed_5', 'opp_poss', 'opp_ppp', 'opp_spp', 'team_poss', 'team_ppp', 'team_spp', 'team_poss_start', 'opp_poss_start', 'team_poss_len', 'opp_poss_len', 'team_poss_width', 'opp_poss_width', 'team_poss_time_sum', 'opp_poss_time_sum', 'team_poss_time_median', 'opp_poss_time_median', 'oxG_f3', 'txG_f3', 'pct_lead', 'pct_tied', 'pct_trail', 'man_adv', 'team_dx_sec', 'team_xxG_sec', 'opp_dx_sec', 'opp_xxG_sec', 'opp_d3_passes', 'opp_d3_comp%', 'opp_d3_fcomp%', 'opp_m3_passes', 'opp_m3_comp%', 'opp_m3_fcomp%', 'opp_a3_passes', 'opp_a3_comp%', 'opp_a3_fcomp%', 'team_d3_passes', 'team_d3_comp%', 'team_d3_fcomp%', 'team_m3_passes', 'team_m3_comp%', 'team_m3_fcomp%', 'team_a3_passes', 'team_a3_comp%', 'team_a3_fcomp%', 'opp_SOT%', 'team_save%', 'team_SOT%', 'opp_save%', 'team_XGOT/SOT', 'opp_XGOT/SOT', 'opp_end_d3', 'opp_end_m3', 'opp_end_a3', 'team_end_d3', 'team_end_m3', 'team_end_a3', 'team_switches', 'opp_switches', 'opp_d3_press', 'opp_m3_press', 'opp_a3_press', 'team_d3_press', 'team_m3_press', 'team_a3_press', 'opp_wide_poss', 'team_wide_poss',\n",
    "    ]\n",
    "    stats+=[\n",
    "    'team_goals', 'opp_goals', 'attacks_recorded', 'team_yellow_cards', 'opp_yellow_cards', 'team_red_cards', 'opp_red_cards', 'team_shotsOnTarget', 'opp_shotsOnTarget', 'team_shotsOffTarget', 'opp_shotsOffTarget', 'team_shots', 'opp_shots', 'team_fouls', 'opp_fouls', 'team_possession', 'opp_possession', 'team_offsides', 'opp_offsides', 'team_dangerous_attacks', 'opp_dangerous_attacks', 'team_attacks', 'opp_attacks', 'team_xg', 'opp_xg', 'total_xg', 'team_penalties_won', 'opp_penalties_won', 'team_penalty_goals', 'opp_penalty_goals', 'team_penalty_missed', 'opp_penalty_missed', 'team_throwins', 'opp_throwins', 'team_freekicks', 'opp_freekicks', 'team_goalkicks', 'opp_goalkicks'\n",
    "    ]\n",
    "    stats+=[\n",
    "        'goals_total', 'shots_total', 'shotsOnTarget_total', 'attacks_total', 'dangerous_attacks_total', 'xg_total'\n",
    "    ]\n",
    "\n",
    "    stats+=[\n",
    "        'avg_potential','goals_diff', 'shots_diff', 'shotsOnTarget_diff', 'possession_diff', 'attacks_diff', 'dangerous_attacks_diff', 'xg_diff','no_home_away','is_home'\n",
    "    ]\n",
    "\n",
    "    # leaky for what happened in past\n",
    "    # stats+=[\n",
    "    #     'odds_ft_1', 'odds_ft_x', 'odds_ft_2'\n",
    "    # ]\n",
    "\n",
    "    # found to not be useful # these were useful for pace\n",
    "    # 171           team_pens    0.000000\n",
    "    # 172            opp_pens    0.000000\n",
    "    # 283  team_penalties_won    0.000000\n",
    "    for stat in ['opp_penalty_goals','team_penalty_goals','team_penalties_won','opp_penalties_won','opp_penalty_missed','attacks_recorded','goals_total']:\n",
    "        stats.remove(stat)\n",
    "\n",
    "    total_non_sb = len(non_sb_data)\n",
    "    # drop cols where 93% nulls\n",
    "    threshold = int(0.93*total_non_sb)\n",
    "    footy_stats = stats.copy()\n",
    "    to_drop = list(non_sb_data[stats].isnull().sum()[non_sb_data[stats].isnull().sum()>threshold].index)\n",
    "    for td in to_drop:\n",
    "        footy_stats.remove(td)\n",
    "    print(non_sb_data.shape)\n",
    "    non_sb_data = non_sb_data.drop(columns=to_drop)\n",
    "    print(non_sb_data.shape)\n",
    "    non_sb_data = non_sb_data.drop(columns=['team_id','opp_team_id','match_id','competition_id','is_upcoming'])\n",
    "    print(list(non_sb_data))\n",
    "    def prep_non_sb_data(data):\n",
    "    \n",
    "        data['target_temp'] = data['team_goals'].copy()-data['opp_goals'].copy()\n",
    "        data['target_2_temp'] = data['team_goals'].copy()+data['opp_goals'].copy()\n",
    "        data['target_num_games'] = data.groupby(['footy_team_id','season'])['target_temp'].transform('count')\n",
    "        data['target_szn_sum'] = data.groupby(['footy_team_id','season'])['target_temp'].transform('sum')\n",
    "        data['target_2_szn_sum'] = data.groupby(['footy_team_id','season'])['target_2_temp'].transform('sum')\n",
    "        data['target'] = (data['target_szn_sum'].copy()-data['target_temp'])/(data['target_num_games'].copy()-1)\n",
    "        data['target_2'] = (data['target_2_szn_sum'].copy()-data['target_2_temp'])/(data['target_num_games'].copy()-1)\n",
    "        \n",
    "        data = data.drop(columns=['target_temp','target_2_temp','target_num_games','target_szn_sum','target_2_szn_sum'])\n",
    "        data = data.dropna(subset=['target','target_2'])\n",
    "        \n",
    "        return data\n",
    "\n",
    "\n",
    "    non_sb_data = prep_non_sb_data(non_sb_data)\n",
    "\n",
    "    def prep_sb_data(data):\n",
    "        \n",
    "        data['target_temp'] = data['team_goals'].copy()-data['opp_goals'].copy()\n",
    "        data['target_2_temp'] = data['team_goals'].copy()+data['opp_goals'].copy()\n",
    "        data['target_num_games'] = data.groupby(['team_id','season_id'])['target_temp'].transform('count')\n",
    "        data['target_szn_sum'] = data.groupby(['team_id','season_id'])['target_temp'].transform('sum')\n",
    "        data['target_2_szn_sum'] = data.groupby(['team_id','season_id'])['target_2_temp'].transform('sum')\n",
    "        data['target'] = (data['target_szn_sum'].copy()-data['target_temp'])/(data['target_num_games'].copy()-1)\n",
    "        data['target_2'] = (data['target_2_szn_sum'].copy()-data['target_2_temp'])/(data['target_num_games'].copy()-1)\n",
    "        data = data.drop(columns=['target_temp','target_2_temp','target_num_games','target_szn_sum','target_2_szn_sum'])\n",
    "        data = data.dropna(subset=['target'])\n",
    "        \n",
    "        return data\n",
    "\n",
    "    sb_data = prep_sb_data(sb_data)\n",
    "\n",
    "    sb_data = sb_data.drop(columns=['season_id'])\n",
    "    non_sb_data = non_sb_data.drop(columns=['season'])\n",
    "\n",
    "    def add_game_grades_non_sb(data):\n",
    "        \n",
    "        # # ### for getting feature importance\n",
    "        X = data[footy_stats+['target','target_2']].copy()\n",
    "        # X = X.sample(frac=1)\n",
    "        y1 = X['target'].copy()\n",
    "        y2 = X['target_2'].copy()\n",
    "        X = X.drop(columns=['target','target_2'])\n",
    "\n",
    "        eval_cv = KFold(3, shuffle=True, random_state=17)\n",
    "        prod_cv = KFold(10, shuffle=True, random_state=17)\n",
    "        reg = catboost.CatBoostRegressor(verbose=False)\n",
    "        # 08/23/22 Score diff cross val 0.17366522630199602\n",
    "        print(\"Score diff cross val\", np.mean(cross_val_score(reg, X, y1, cv=eval_cv)))\n",
    "        # # option 1\n",
    "        reg = catboost.CatBoostRegressor(verbose=False)\n",
    "        reg.fit(X, y1)\n",
    "        feat_importances = pd.DataFrame({\n",
    "            'stats':footy_stats,\n",
    "            'importance':reg.feature_importances_\n",
    "        }).sort_values(by='importance',ascending=False)\n",
    "        print(feat_importances)\n",
    "\n",
    "        data['game_score'] = cross_val_predict(reg, X, y1, cv=prod_cv)\n",
    "        reg = catboost.CatBoostRegressor(verbose=False)\n",
    "        \n",
    "        # 08/23/22 Pace cross val 0.23575666835862108\n",
    "        print(\"Pace cross val\", np.mean(cross_val_score(reg, X, y2, cv=eval_cv)))\n",
    "        data['pace_score'] = cross_val_predict(reg, X, y2, cv=prod_cv)\n",
    "        \n",
    "        reg = catboost.CatBoostRegressor(verbose=False)\n",
    "        reg.fit(X, y2)\n",
    "        feat_importances = pd.DataFrame({\n",
    "            'stats':footy_stats,\n",
    "            'importance':reg.feature_importances_\n",
    "        }).sort_values(by='importance',ascending=False)\n",
    "        print(feat_importances)\n",
    "        \n",
    "        return data\n",
    "\n",
    "\n",
    "    non_sb_data = add_game_grades_non_sb(non_sb_data)\n",
    "\n",
    "    def add_game_grades_sb(data, show_score=False):\n",
    "        \n",
    "        # # ### for getting feature importance\n",
    "        X = data[stats+['target','target_2']].copy()\n",
    "        # X = X.sample(frac=1)\n",
    "        y1 = X['target'].copy()\n",
    "        y2 = X['target_2'].copy()\n",
    "        X = X.drop(columns=['target','target_2'])\n",
    "\n",
    "        eval_cv = KFold(3, shuffle=True, random_state=17)\n",
    "    #     np.mean(cross_val_score(reg, X, y1, cv=eval_cv))\n",
    "        prod_cv = KFold(6, shuffle=True, random_state=17)\n",
    "        \n",
    "    #     sd_params = {'iterations': 19524,\n",
    "    #      'od_wait': 1270,\n",
    "    #      'learning_rate': 0.011194201649297929,\n",
    "    #      'reg_lambda': 44.63399671384952,\n",
    "    #      'subsample': 0.7496120387317952,\n",
    "    #      'random_strength': 33.75970628327746,\n",
    "    #      'depth': 6,\n",
    "    #      'min_data_in_leaf': 18,\n",
    "    #      'leaf_estimation_iterations': 5}\n",
    "        reg = catboost.CatBoostRegressor(verbose=False)#, **sd_params)\n",
    "        # # option 2\n",
    "        # Score diff cross val 0.2784195893187421 # did better without opt?\n",
    "        if show_score:\n",
    "            print(\"Score diff cross val\", np.mean(cross_val_score(reg, X, y1, cv=eval_cv)))\n",
    "        # # option 1\n",
    "        reg = catboost.CatBoostRegressor(verbose=False)#,**sd_params)\n",
    "        reg.fit(X, y1)\n",
    "        feat_importances = pd.DataFrame({\n",
    "            'stats':stats,\n",
    "            'importance':reg.feature_importances_\n",
    "        }).sort_values(by='importance',ascending=False)\n",
    "        print(feat_importances)\n",
    "\n",
    "        data['game_score'] = cross_val_predict(reg, X, y1, cv=prod_cv)\n",
    "        reg = catboost.CatBoostRegressor(verbose=False)\n",
    "    #     Pace cross val 0.33348896960501095\n",
    "        if show_score:\n",
    "            print(\"Pace cross val\", np.mean(cross_val_score(reg, X, y2, cv=eval_cv)))\n",
    "        data['pace_score'] = cross_val_predict(reg, X, y2, cv=prod_cv)\n",
    "        \n",
    "        reg = catboost.CatBoostRegressor(verbose=False)\n",
    "        reg.fit(X, y2)\n",
    "        feat_importances = pd.DataFrame({\n",
    "            'stats':stats,\n",
    "            'importance':reg.feature_importances_\n",
    "        }).sort_values(by='importance',ascending=False)\n",
    "        print(feat_importances)\n",
    "        \n",
    "        return data\n",
    "\n",
    "\n",
    "    sb_data = add_game_grades_sb(sb_data)\n",
    "\n",
    "    non_sb_scores = non_sb_data.copy()[['footy_match_id','footy_team_id','footy_opp_id','match_date_UTC',\n",
    "                                'game_score','pace_score']]\n",
    "    sb_game_scores = sb_data.copy()[['footy_match_id','footy_team_id','footy_opp_id','match_date_UTC','match_id','team_id','opp_team_id',\n",
    "                                'game_score','pace_score']]\n",
    "\n",
    "    game_scores = pd.concat([sb_game_scores, non_sb_scores], axis=0).sort_values(by='match_date_UTC').reset_index(drop=True)\n",
    "\n",
    "    def prep_for_network(df):\n",
    "        \n",
    "        opp_data = df.copy().rename(columns={'id':'opp_id','opp_id':'id'})\n",
    "        opp_data = opp_data.rename(columns={'game_score':'opp_game_score','pace_score':'opp_pace_score'})\n",
    "\n",
    "        df = df.merge(opp_data[['match_date','footy_match_id','id','opp_id']+['opp_game_score','opp_pace_score']], how='left', on=['match_date','footy_match_id','id','opp_id'])\n",
    "        # # just do two networks for now (later more)\n",
    "        df['rtg_avg'] = (df['game_score'].copy() + (-1*df['opp_game_score'].copy()))/2\n",
    "        df['pace_avg'] = (df['pace_score'].copy() + (df['opp_pace_score'].copy()))/2\n",
    "\n",
    "\n",
    "        min_date = df.match_date.min()\n",
    "        # # helper col\n",
    "        df['rating_period'] = df['match_date'].copy().rank(method='dense').astype(int)\n",
    "        df['date_since_inception'] = (df['match_date'].copy()-min_date).dt.days\n",
    "        df['days_since_last'] = df['date_since_inception'].diff()\n",
    "        df['game_no'] = df.groupby(['id'])['footy_match_id'].transform('cumcount')\n",
    "        df['opp_game_no'] = df.groupby(['opp_id'])['footy_match_id'].transform('cumcount')\n",
    "        df = df.drop_duplicates(subset=['footy_match_id']).reset_index(drop=True)\n",
    "        \n",
    "        ## only using statsbomb teams\n",
    "        df['backup_team_id'] = df['id'].map(teams_footy2id)\n",
    "        df['backup_opp_id'] = df['opp_id'].map(teams_footy2id)\n",
    "        df['team_id'] = df['team_id'].fillna(df['backup_team_id'].copy())\n",
    "        df['opp_team_id'] = df['opp_team_id'].fillna(df['backup_opp_id'].copy())\n",
    "        df = df.drop(columns=['backup_team_id','backup_opp_id'])\n",
    "\n",
    "        \n",
    "        return df\n",
    "\n",
    "    game_scores = game_scores.rename(columns={'footy_team_id':'id', 'footy_opp_id':'opp_id','match_date_UTC':'match_date'})\n",
    "    game_scores = prep_for_network(game_scores)\n",
    "\n",
    "    # create network\n",
    "    df = game_scores.copy().dropna(subset=['team_id','opp_team_id','rtg_avg','pace_avg'])\n",
    "    df['days_since_last'] = df['days_since_last'].fillna(1)# one case\n",
    "    df = df.drop(columns=['id','opp_id'])\n",
    "    teams = list(set(list(df.team_id.unique())+list(df.opp_team_id.unique())))\n",
    "    num_teams = len(teams)\n",
    "    teams.sort()\n",
    "    team_map = {}\n",
    "    for idx, team in enumerate(teams):\n",
    "        team_map[team] = idx\n",
    "        \n",
    "    prank_mat = np.zeros((num_teams,num_teams))\n",
    "    neg_prank_mat = np.zeros((num_teams,num_teams))\n",
    "    pace_mat = np.ones((num_teams, num_teams))*2.5\n",
    "    pace_std = np.ones((num_teams, num_teams))\n",
    "    rating_periods = list(df.rating_period.unique())\n",
    "    df.match_date = pd.to_datetime(df.match_date.copy())\n",
    "    df = df.sort_values(by='match_date')\n",
    "\n",
    "\n",
    "    def calc_ratings(protag_matrix):\n",
    "        \n",
    "        N = biggest_index = protag_matrix.shape[0]    \n",
    "        d = 5e-3\n",
    "        A = (d * protag_matrix + (1 - d) / N)    \n",
    "        v = np.repeat(1/biggest_index, biggest_index)\n",
    "        for i in range(150):\n",
    "            v = A@v\n",
    "            norm = np.linalg.norm(v)\n",
    "            v = v/norm\n",
    "        \n",
    "        return v\n",
    "\n",
    "    history = []\n",
    "    for rp in tqdm(rating_periods):\n",
    "        rating_update = np.zeros((num_teams, num_teams))\n",
    "        neg_rating_update = np.zeros((num_teams, num_teams))\n",
    "        pace_update = np.zeros((num_teams, num_teams))\n",
    "        pace_std_update = np.zeros((num_teams, num_teams))\n",
    "        rp_data = df.copy().loc[df.rating_period==rp].reset_index(drop=True)\n",
    "        days_since = rp_data.days_since_last.copy().max()\n",
    "        if days_since < 1:\n",
    "            days_since = 1\n",
    "        \n",
    "        # time decay the matrix\n",
    "        prank_mat *= np.exp(-(1/150)*days_since)\n",
    "        neg_prank_mat *= np.exp(-(1/150)*days_since)\n",
    "        pace_mat *= np.exp(-(1/125)*days_since)\n",
    "        pace_std *= np.exp(-(1/125)*days_since)\n",
    "        \n",
    "        ratings_vec = calc_ratings(prank_mat)\n",
    "        neg_ratings_vec = calc_ratings(neg_prank_mat)\n",
    "        \n",
    "        pace_vec = calc_ratings(pace_mat.copy()/pace_std.copy())\n",
    "    #     ratings_entr = entropy(prank_mat)\n",
    "    #     pace_entr = entropy(pace_mat.copy()/pace_std.copy())\n",
    "        for index, row in rp_data.iterrows():\n",
    "            match_id = row['match_id']\n",
    "            protag_id = row['team_id']\n",
    "            antag_id = row['opp_team_id']\n",
    "            protag_index = team_map[protag_id]\n",
    "            antag_index = team_map[antag_id]\n",
    "            ## grab ratings going into games\n",
    "            protag_rating = ratings_vec[protag_index] \n",
    "            antag_rating = ratings_vec[antag_index]\n",
    "            protag_neg_rating = neg_ratings_vec[protag_index]\n",
    "            antag_neg_rating = neg_ratings_vec[antag_index]\n",
    "    #         protag_entr = ratings_entr[protag_index]\n",
    "    #         antag_entr = ratings_entr[antag_index]\n",
    "            protag_pace = pace_vec[protag_index]\n",
    "            antag_pace = pace_vec[antag_index]\n",
    "    #         protag_pentr = pace_entr[protag_index]\n",
    "    #         antag_pentr = pace_entr[antag_index]\n",
    "            history.append([match_id, protag_id, antag_id, protag_rating, antag_rating, protag_neg_rating,antag_neg_rating,#protag_entr, antag_entr,\n",
    "                            protag_pace, antag_pace]) #, protag_pentr, antag_pentr])\n",
    "            \n",
    "            # update\n",
    "            game_rating = row['rtg_avg']\n",
    "            pace_rating = row['pace_avg']\n",
    "            if np.isnan(game_rating):\n",
    "                continue\n",
    "            if np.isnan(pace_rating):\n",
    "                continue\n",
    "                \n",
    "            if game_rating > 0:\n",
    "                rating_update[protag_index][antag_index]+=np.abs(game_rating)\n",
    "                neg_rating_update[antag_index][protag_index]+=np.abs(game_rating)\n",
    "            else:\n",
    "                rating_update[antag_index][protag_index]+=np.abs(game_rating)\n",
    "                neg_rating_update[protag_index][antag_index]+=np.abs(game_rating)\n",
    "            pace_update[protag_index][antag_index] += pace_rating\n",
    "            pace_update[antag_index][protag_index] += pace_rating\n",
    "            pace_std_update[protag_index][antag_index] += 1\n",
    "            pace_std_update[antag_index][protag_index] += 1\n",
    "            \n",
    "        prank_mat+=rating_update\n",
    "        neg_prank_mat+=neg_rating_update\n",
    "        pace_mat += pace_update\n",
    "        pace_std+= pace_std_update\n",
    "\n",
    "    history = pd.DataFrame(history, columns=['match_id','id','opp_id','rating','opp_rating','neg_rating','neg_opp_rating',#'entropy','opp_entropy',\n",
    "                                            'pace','opp_pace'])#,'pace_entropy','opp_pace_entropy'])\n",
    "    history = history.merge(stf_schedule[['match_id','team_id','score','opp_score']].rename(columns={'team_id':'id'}),\n",
    "                        how='left', on=['match_id','id'])\n",
    "\n",
    "    history['score_diff'] = history['score'].copy()-history['opp_score'].copy()\n",
    "    history['ratings_diff'] = history['rating'].copy()-history['opp_rating'].copy()\n",
    "    history['ratings_v2'] = history['rating'].copy()-history['neg_rating'].copy()\n",
    "    history['opp_ratings_v2'] = history['opp_rating'].copy()-history['neg_opp_rating'].copy()\n",
    "    history['rtg_diff_v2'] = history['ratings_v2'].copy()-history['opp_ratings_v2'].copy()\n",
    "    history['score_total'] = history['score'].copy()+history['opp_score'].copy()\n",
    "    history['pace_comb'] = history['pace'].copy()+history['opp_pace'].copy()\n",
    "\n",
    "    # \n",
    "    to_save = history.copy().dropna(subset=['match_id']).reset_index(drop=True)\n",
    "    to_save = to_save.drop(columns=['score','opp_score','score_diff','score_total'])\n",
    "    to_save = to_save.rename(columns={'id':'team_id','opp_id':'opp_team_id'})\n",
    "    to_save_opp = to_save.copy().reset_index(drop=True)\n",
    "    to_save_opp.columns=['match_id','opp_team_id','team_id','opp_rating','rating','neg_opp_rating','neg_rating',\n",
    "                        #'opp_entropy','entropy',\n",
    "                        'opp_pace','pace',\n",
    "                        #'opp_pace_entropy','pace_entropy',\n",
    "                        'ratings_diff','opp_ratings_v2','ratings_v2','rtg_diff_v2','pace_comb']\n",
    "    to_save_opp = to_save_opp[list(to_save)]\n",
    "    to_save_opp['ratings_diff'] = to_save_opp['ratings_diff'].copy()*-1\n",
    "    to_save_opp['rtg_diff_v2'] = to_save_opp['rtg_diff_v2'].copy()*-1\n",
    "    to_save = pd.concat([to_save, to_save_opp], axis=0).reset_index(drop=True)\n",
    "    to_save.sort_values(by=['match_id'])\n",
    "    to_save = to_save.drop(columns=['opp_rating','rating','neg_opp_rating','neg_rating','ratings_diff'])\n",
    "\n",
    "    def get_current_ratings():\n",
    "        \n",
    "        current_rtgs = []\n",
    "        for protag_id,protag_index, in team_map.items():\n",
    "            protag_index = team_map[protag_id]\n",
    "            ## grab ratings going into games\n",
    "            protag_rating = ratings_vec[protag_index] \n",
    "            protag_neg_rating = neg_ratings_vec[protag_index]\n",
    "            #protag_entr = ratings_entr[protag_index]\n",
    "            protag_pace = pace_vec[protag_index]\n",
    "            #protag_pentr = pace_entr[protag_index]\n",
    "            \n",
    "            current_rtgs.append([protag_id, protag_rating, protag_neg_rating, protag_pace]) #, protag_entr, protag_pace, protag_pentr])\n",
    "        \n",
    "        return pd.DataFrame(current_rtgs, columns=['team_id','rating','neg_rating','pace'])#,'entropy','pace','pace_entropy'])\n",
    "    #     protag_id = row['team_id']\n",
    "\n",
    "    current_ratings = get_current_ratings()\n",
    "    opp_current_ratings = current_ratings.copy()\n",
    "    opp_current_ratings.columns=['opp_team_id','opp_rating','neg_opp_rating','opp_pace']#'opp_entropy','opp_pace','opp_pace_entropy']\n",
    "    upc = stf_schedule.copy().loc[stf_schedule['is_upcoming']==1].reset_index(drop=True)[['match_id','team_id','opp_team_id']]\n",
    "\n",
    "\n",
    "    \n",
    "    upc = upc.merge(current_ratings.copy(), how='left', on=['team_id'])\n",
    "    upc = upc.merge(opp_current_ratings.copy(), how='left', on='opp_team_id')\n",
    "    # upc['ratings_diff'] = upc['rating'].copy()-upc['opp_rating'].copy()\n",
    "    upc['ratings_v2'] = upc['rating'].copy()-upc['neg_rating'].copy()\n",
    "    upc['opp_ratings_v2'] = upc['opp_rating'].copy()-upc['neg_opp_rating'].copy()\n",
    "    upc['rtg_diff_v2'] = upc['ratings_v2'].copy()-upc['opp_ratings_v2'].copy()\n",
    "    upc = upc.drop(columns=['opp_rating','rating','neg_opp_rating','neg_rating'])\n",
    "    upc['pace_comb'] = upc['pace'].copy()+upc['opp_pace'].copy()\n",
    "    to_save = pd.concat([to_save, upc], axis=0).reset_index(drop=True)\n",
    "\n",
    "    to_save.to_csv(os.path.join(DROPBOX_PATH, 'team_ratings/grade_network_no_mkt.csv'), index=False)\n",
    "\n",
    "    return\n",
    "\n",
    "\n",
    "def gg2():\n",
    "\n",
    "    ## gg 2: market gg\n",
    "    print(\"Creating market game grades...\")\n",
    "    ### data prep ###\n",
    "    teams = load_dict(os.path.join(DROPBOX_PATH, 'IDs/teams'))\n",
    "    competitions = load_dict(os.path.join(DROPBOX_PATH, 'IDs/competitions'))\n",
    "    \n",
    "    schedule, stf_schedule = load_schedules()\n",
    "    teams_id2footy = load_dict(os.path.join(DROPBOX_PATH, 'IDs/footy/teams_id2footy'))\n",
    "    teams_footy2id = load_dict(os.path.join(DROPBOX_PATH, 'IDs/footy/teams_footy2id'))\n",
    "    footy = load_footy()\n",
    "    footy = prep_footy_data(footy.copy(), teams_footy2id)\n",
    "\n",
    "    df, missing = merge_game_ids(stf_schedule.copy(), footy.copy().rename(columns={'datetime_UTC':'footy_datetime_UTC'}), schedule.copy())\n",
    "\n",
    "    df = add_sb_metrics(df, stf_schedule)\n",
    "    df = df.rename(columns={'id':'footy_match_id'})\n",
    "\n",
    "    df = processing_steps(df, schedule.copy())\n",
    "\n",
    "    # remove vig and drop one\n",
    "    # remove vig\n",
    "    df['inv_odds_ft_1'] = 1/df['odds_ft_1'].copy()\n",
    "    df['inv_odds_ft_x'] = 1/df['odds_ft_x'].copy()\n",
    "    # df['inv_odds_ft_2'] = 1/df['odds_ft_2'].copy()\n",
    "    df['inv_odds_total'] = df[['inv_odds_ft_1','inv_odds_ft_x','inv_odds_ft_2']].copy().sum(axis=1)\n",
    "    df['odds_ft_1'] = 1/(df['inv_odds_ft_1'].copy()/df['inv_odds_total'].copy())\n",
    "    df['odds_ft_x'] = 1/(df['inv_odds_ft_x'].copy()/df['inv_odds_total'].copy())\n",
    "    # df['odds_ft_2'] = 1/(df['inv_odds_ft_2'].copy()/df['inv_odds_total'].copy())\n",
    "\n",
    "    # df = df.drop(columns=['odds_ft_2']) # only need 2 of 3 way ML, last is implied\n",
    "\n",
    "    sb_data = df.copy().loc[df['match_id'].notnull()].reset_index(drop=True)\n",
    "    sb_data = sb_data.merge(schedule[['match_id','season_id']], how='left')\n",
    "    non_sb_data = df.copy().loc[df['match_id'].isnull()].reset_index(drop=True)\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    very weird bug when i try to train with dropping games remaining <= 5\n",
    "    so this is a workaround\n",
    "\n",
    "    \"\"\"\n",
    "    def prep_non_sb_data(data):\n",
    "        \n",
    "        data['target_1_temp'] = data['team_goals'].copy()-data['opp_goals'].copy()\n",
    "        data['target_2_temp'] = data['team_goals'].copy()+data['opp_goals'].copy()\n",
    "        data['target_num_games'] = data.groupby(['footy_team_id','season'])['target_1_temp'].transform('count')\n",
    "        data['target_1_szn_sum'] = data.groupby(['footy_team_id','season'])['target_1_temp'].transform('sum')\n",
    "        data['target_2_szn_sum'] = data.groupby(['footy_team_id','season'])['target_2_temp'].transform('sum')\n",
    "    #     print(data[['target_1_temp','target_2_temp','target_num_games','target_1_szn_sum','target_2_szn_sum']].isnull().sum())\n",
    "        # now using these as backup for end of season\n",
    "        data['target_1'] = (data['target_1_szn_sum'].copy()-data['target_1_temp'])/(data['target_num_games'].copy()-1)\n",
    "        data['target_2'] = (data['target_2_szn_sum'].copy()-data['target_2_temp'])/(data['target_num_games'].copy()-1)\n",
    "        data = data.drop(columns=['target_1_temp','target_2_temp','target_num_games','target_1_szn_sum','target_2_szn_sum'])\n",
    "    #     data = data.dropna(subset=['target'])\n",
    "        \n",
    "        \n",
    "        data['datetime_UTC'] = pd.to_datetime(data['datetime_UTC'] )\n",
    "        data = data.sort_values(by=['datetime_UTC'])\n",
    "        data['games_in_season'] = data.groupby(['footy_team_id','season'])['footy_match_id'].transform('count')\n",
    "        data['szn_already_played'] = data.groupby(['footy_team_id','season'])['footy_match_id'].transform('cumcount')\n",
    "        data['games_remaining'] = data['games_in_season'].copy()-data['szn_already_played'].copy()\n",
    "        # print(list(non_sb_data))\n",
    "        # sns.displot(data.sample(frac=0.05)['games_remaining'])\n",
    "        data['score_diff_temp'] = data['team_goals'].copy()-data['opp_goals'].copy()\n",
    "        data['sd_season'] = data.groupby(['footy_team_id','season'])['score_diff_temp'].transform('sum')\n",
    "        data['sd_already'] = data.groupby(['footy_team_id','season'])['score_diff_temp'].transform('cumsum')\n",
    "        data['sd_ROS'] = data['sd_season'].copy()-data['sd_already'].copy()\n",
    "\n",
    "        data['score_total_temp'] = data['team_goals'].copy()+data['opp_goals'].copy()\n",
    "        data['st_season'] = data.groupby(['footy_team_id','season'])['score_total_temp'].transform('sum')\n",
    "        data['st_already'] = data.groupby(['footy_team_id','season'])['score_total_temp'].transform('cumsum')\n",
    "        data['st_ROS'] = data['st_season'].copy()-data['st_already'].copy()\n",
    "\n",
    "        data['side_target'] = data['sd_ROS'].copy()/data['games_remaining'].copy()\n",
    "        data['total_target'] = data['st_ROS'].copy()/data['games_remaining'].copy()\n",
    "\n",
    "        data = data.drop(columns=['games_in_season','szn_already_played','sd_season','st_season','sd_already','st_already',\n",
    "                                'st_ROS','sd_ROS','score_diff_temp','score_total_temp'])\n",
    "        \n",
    "        # if less than 2 games remaining, use backup target\n",
    "        data['side_target'] = np.where(data['games_remaining'] <= 2, data['target_1'].copy(), data['side_target'].copy())\n",
    "        data['total_target'] = np.where(data['games_remaining'] <= 2, data['target_2'].copy(), data['total_target'].copy())\n",
    "        \n",
    "    #     print(data[['side_target','total_target','target_1','target_2']].corr())\n",
    "        data = data.drop(columns=['target_1','target_2'])\n",
    "        data = data.dropna(subset=['side_target','total_target'])\n",
    "        \n",
    "        return data.reset_index(drop=True)\n",
    "\n",
    "    def prep_sb_data(data):\n",
    "        \n",
    "        ## first get backup target\n",
    "        data['target_1_temp'] = data['team_goals'].copy()-data['opp_goals'].copy()\n",
    "        data['target_2_temp'] = data['team_goals'].copy()+data['opp_goals'].copy()\n",
    "        data['target_num_games'] = data.groupby(['team_id','season_id'])['target_1_temp'].transform('count')\n",
    "        data['target_1_szn_sum'] = data.groupby(['team_id','season_id'])['target_1_temp'].transform('sum')\n",
    "        data['target_2_szn_sum'] = data.groupby(['team_id','season_id'])['target_2_temp'].transform('sum')\n",
    "    #     print(data[['target_1_temp','target_2_temp','target_num_games','target_1_szn_sum','target_2_szn_sum']].isnull().sum())\n",
    "        # now using these as backup for end of season\n",
    "        data['target_1'] = (data['target_1_szn_sum'].copy()-data['target_1_temp'])/(data['target_num_games'].copy()-1)\n",
    "        data['target_2'] = (data['target_2_szn_sum'].copy()-data['target_2_temp'])/(data['target_num_games'].copy()-1)\n",
    "        data = data.drop(columns=['target_1_temp','target_2_temp','target_num_games','target_1_szn_sum','target_2_szn_sum'])\n",
    "        \n",
    "        data['datetime_UTC'] = pd.to_datetime(data['datetime_UTC'] )\n",
    "        data = data.sort_values(by=['datetime_UTC'])\n",
    "        data['games_in_season'] = data.groupby(['team_id','season_id'])['match_id'].transform('count')\n",
    "        data['szn_already_played'] = data.groupby(['team_id','season_id'])['match_id'].transform('cumcount')\n",
    "        data['games_remaining'] = data['games_in_season'].copy()-data['szn_already_played'].copy()\n",
    "        # print(list(non_sb_data))\n",
    "        # sns.displot(data.sample(frac=0.05)['games_remaining'])\n",
    "        data['score_diff_temp'] = data['team_goals'].copy()-data['opp_goals'].copy()\n",
    "        data['sd_season'] = data.groupby(['team_id','season_id'])['score_diff_temp'].transform('sum')\n",
    "        data['sd_already'] = data.groupby(['team_id','season_id'])['score_diff_temp'].transform('cumsum')\n",
    "        data['sd_ROS'] = data['sd_season'].copy()-data['sd_already'].copy()\n",
    "\n",
    "        data['score_total_temp'] = data['team_goals'].copy()+data['opp_goals'].copy()\n",
    "        data['st_season'] = data.groupby(['footy_team_id','season'])['score_total_temp'].transform('sum')\n",
    "        data['st_already'] = data.groupby(['footy_team_id','season'])['score_total_temp'].transform('cumsum')\n",
    "        data['st_ROS'] = data['st_season'].copy()-data['st_already'].copy()\n",
    "\n",
    "        data['side_target'] = data['sd_ROS'].copy()/data['games_remaining'].copy()\n",
    "        data['total_target'] = data['st_ROS'].copy()/data['games_remaining'].copy()\n",
    "        \n",
    "    #     print(data.loc[((data['competition_id']==2)&(data['season_id']==90))][['games_in_season','szn_already_played','games_remaining']])\n",
    "    #     print(data.loc[((data['competition_id']==2)&(data['season_id']==90))][['sd_season','sd_already','sd_ROS']])\n",
    "    #     print(data.loc[((data['competition_id']==2)&(data['season_id']==90))][['st_season','st_already','st_ROS']])\n",
    "        \n",
    "        data = data.drop(columns=['games_in_season','szn_already_played','sd_season','st_season','sd_already','st_already',\n",
    "                                'st_ROS','sd_ROS','score_diff_temp','score_total_temp'])\n",
    "\n",
    "        data = data.dropna(subset=['side_target','total_target'])\n",
    "        \n",
    "        # if less than 2 games remaining, use backup target\n",
    "        data['side_target'] = np.where(data['games_remaining'] <= 2, data['target_1'].copy(), data['side_target'].copy())\n",
    "        data['total_target'] = np.where(data['games_remaining'] <= 2, data['target_2'].copy(), data['total_target'].copy())\n",
    "        \n",
    "    #     print(data[['side_target','total_target','target_1','target_2']].corr())\n",
    "        data = data.drop(columns=['target_1','target_2'])\n",
    "        data = data.dropna(subset=['side_target','total_target'])\n",
    "        \n",
    "        return data\n",
    "\n",
    "        \n",
    "    non_sb_data = prep_non_sb_data(non_sb_data)\n",
    "    sb_data = prep_sb_data(sb_data)\n",
    "\n",
    "\n",
    "    # sb_data['man_adv'].sum()\n",
    "    # sns.displ[ot(sb_data['man_adv'])\n",
    "    stats = [\n",
    "        'score_diff', 'xG_diff', 'shot_diff', 'sq_diff', 'sot_diff', 'score_total', 'xG_total', 'shot_total', 'sq_total', 'sot_total', 'sit_boost', 'pace', 'obv_pace', 'xxG_diff', 'pct_xxG_diff', 'obv_diff', 'pct_obv_diff', 'cgoal_skew', 'cgoal_kurt', 'cgoal_sum', 'cgoal_std', 'cconcede_skew', 'cconcede_kurt', 'cconcede_sum', 'cconcede_std', 'xxG_conversion', 'xG_conversion', 'opp_xG_conversion', 'opp_xxG_conversion', 'win_prob', 'draw_prob', 'opp_dt_eff', 'opp_mt_eff', 'opp_at_eff', 'opp_d_qual', 'opp_mid_qual', 'opp_atck_qual', 'team_dt_eff', 'team_mt_eff', 'team_at_eff', 'team_d_qual', 'team_mid_qual', 'team_atck_qual', 'opp_gk_pass_loc', 'opp_def_pass_loc', 'opp_mid_pass_loc', 'opp_atck_pass_loc', 'opp_gk_pass_angle', 'opp_def_pass_angle', 'opp_mid_pass_angle', 'opp_atck_pass_angle', 'opp_gk_pass_count', 'opp_def_pass_count', 'opp_mid_pass_count', 'opp_atck_pass_count', 'opp_gk_pass_dist', 'opp_def_pass_dist', 'opp_mid_pass_dist', 'opp_atck_pass_dist', 'opp_gk_pass_obv_vol', 'opp_def_pass_obv_vol', 'opp_mid_pass_obv_vol', 'opp_atck_pass_obv_vol', 'opp_gk_pass_obv_eff', 'opp_def_pass_obv_eff', 'opp_mid_pass_obv_eff', 'opp_atck_pass_obv_eff', 'team_gk_pass_loc', 'team_def_pass_loc', 'team_mid_pass_loc', 'team_atck_pass_loc', 'team_gk_pass_angle', 'team_def_pass_angle', 'team_mid_pass_angle', 'team_atck_pass_angle', 'team_gk_pass_count', 'team_def_pass_count', 'team_mid_pass_count', 'team_atck_pass_count', 'team_gk_pass_dist', 'team_def_pass_dist', 'team_mid_pass_dist', 'team_atck_pass_dist', 'team_gk_pass_obv_vol', 'team_def_pass_obv_vol', 'team_mid_pass_obv_vol', 'team_atck_pass_obv_vol', 'team_gk_pass_obv_eff', 'team_def_pass_obv_eff', 'team_mid_pass_obv_eff', 'team_atck_pass_obv_eff', 'opp_gk_carry_angle', 'opp_def_carry_angle', 'opp_mid_carry_angle', 'opp_atck_carry_angle', 'opp_gk_carry_count', 'opp_def_carry_count', 'opp_mid_carry_count', 'opp_atck_carry_count', 'opp_gk_carry_dist', 'opp_def_carry_dist', 'opp_mid_carry_dist', 'opp_atck_carry_dist', 'opp_gk_carry_vol', 'opp_def_carry_vol', 'opp_mid_carry_vol', 'opp_atck_carry_vol', 'opp_gk_carry_eff', 'opp_def_carry_eff', 'opp_mid_carry_eff', 'opp_atck_carry_eff', 'team_gk_carry_angle', 'team_def_carry_angle', 'team_mid_carry_angle', 'team_atck_carry_angle', 'team_gk_carry_dist', 'team_def_carry_dist', 'team_mid_carry_dist', 'team_atck_carry_dist', 'team_gk_carry_count', 'team_def_carry_count', 'team_mid_carry_count', 'team_atck_carry_count', 'team_gk_carry_vol', 'team_def_carry_vol', 'team_mid_carry_vol', 'team_atck_carry_vol', 'team_gk_carry_eff', 'team_def_carry_eff', 'team_mid_carry_eff', 'team_atck_carry_eff', 'opp_gk_dfn_pos', 'opp_def_dfn_pos', 'opp_mid_dfn_pos', 'opp_atck_dfn_pos', 'opp_gk_dfn_obv_vol', 'opp_def_dfn_obv_vol', 'opp_mid_dfn_obv_vol', 'opp_atck_dfn_obv_vol', 'opp_gk_dfn_obv_eff', 'opp_def_dfn_obv_eff', 'opp_mid_dfn_obv_eff', 'opp_atck_dfn_obv_eff', 'opp_gk_dfn_xxG_vol', 'opp_def_dfn_xxG_vol', 'opp_mid_dfn_xxG_vol', 'opp_atck_dfn_xxG_vol', 'team_gk_dfn_pos', 'team_def_dfn_pos', 'team_mid_dfn_pos', 'team_atck_dfn_pos', 'team_gk_dfn_obv_vol', 'team_def_dfn_obv_vol', 'team_mid_dfn_obv_vol', 'team_atck_dfn_obv_vol', 'team_gk_dfn_obv_eff', 'team_def_dfn_obv_eff', 'team_mid_dfn_obv_eff', 'team_atck_dfn_obv_eff', 'team_gk_dfn_xxG_vol', 'team_def_dfn_xxG_vol', 'team_mid_dfn_xxG_vol', 'team_atck_dfn_xxG_vol', 'team_weighted_fouls', 'opp_weighted_fouls', 'opp_corners', 'team_corners', 'team_crosses', 'opp_crosses', 'team_cross_pct', 'opp_cross_pct', 'team_pens', 'opp_pens', 'carry_lengths', 'gk_dist', 'fields_gained', 'fields_gained_comp', 'med_def_action', 'threat_pp', 'dthreat_pp', 'off_embed_0', 'off_embed_1', 'off_embed_2', 'off_embed_3', 'off_embed_4', 'off_embed_5', 'def_embed_0', 'def_embed_1', 'def_embed_2', 'def_embed_3', 'def_embed_4', 'def_embed_5', 'opp_poss', 'opp_ppp', 'opp_spp', 'team_poss', 'team_ppp', 'team_spp', 'team_poss_start', 'opp_poss_start', 'team_poss_len', 'opp_poss_len', 'team_poss_width', 'opp_poss_width', 'team_poss_time_sum', 'opp_poss_time_sum', 'team_poss_time_median', 'opp_poss_time_median', 'oxG_f3', 'txG_f3', 'pct_lead', 'pct_tied', 'pct_trail', 'man_adv', 'team_dx_sec', 'team_xxG_sec', 'opp_dx_sec', 'opp_xxG_sec', 'opp_d3_passes', 'opp_d3_comp%', 'opp_d3_fcomp%', 'opp_m3_passes', 'opp_m3_comp%', 'opp_m3_fcomp%', 'opp_a3_passes', 'opp_a3_comp%', 'opp_a3_fcomp%', 'team_d3_passes', 'team_d3_comp%', 'team_d3_fcomp%', 'team_m3_passes', 'team_m3_comp%', 'team_m3_fcomp%', 'team_a3_passes', 'team_a3_comp%', 'team_a3_fcomp%', 'opp_SOT%', 'team_save%', 'team_SOT%', 'opp_save%', 'team_XGOT/SOT', 'opp_XGOT/SOT', 'opp_end_d3', 'opp_end_m3', 'opp_end_a3', 'team_end_d3', 'team_end_m3', 'team_end_a3', 'team_switches', 'opp_switches', 'opp_d3_press', 'opp_m3_press', 'opp_a3_press', 'team_d3_press', 'team_m3_press', 'team_a3_press', 'opp_wide_poss', 'team_wide_poss',\n",
    "    ]\n",
    "    stats+=[\n",
    "    'team_goals', 'opp_goals', 'attacks_recorded', 'team_yellow_cards', 'opp_yellow_cards', 'team_red_cards', 'opp_red_cards', 'team_shotsOnTarget', 'opp_shotsOnTarget', 'team_shotsOffTarget', 'opp_shotsOffTarget', 'team_shots', 'opp_shots', 'team_fouls', 'opp_fouls', 'team_possession', 'opp_possession', 'team_offsides', 'opp_offsides', 'team_dangerous_attacks', 'opp_dangerous_attacks', 'team_attacks', 'opp_attacks', 'team_xg', 'opp_xg', 'total_xg', 'team_penalties_won', 'opp_penalties_won', 'team_penalty_goals', 'opp_penalty_goals', 'team_penalty_missed', 'opp_penalty_missed', 'team_throwins', 'opp_throwins', 'team_freekicks', 'opp_freekicks', 'team_goalkicks', 'opp_goalkicks'\n",
    "    ]\n",
    "    stats+=[\n",
    "        'goals_total', 'shots_total', 'shotsOnTarget_total', 'attacks_total', 'dangerous_attacks_total', 'xg_total'\n",
    "    ]\n",
    "\n",
    "    stats+=[\n",
    "        'avg_potential','goals_diff', 'shots_diff', 'shotsOnTarget_diff', 'possession_diff', 'attacks_diff', 'dangerous_attacks_diff', 'xg_diff','no_home_away','is_home'\n",
    "    ]\n",
    "\n",
    "    # leaky for what happened in past\n",
    "    ## but fixed this notebook\n",
    "    stats+=[\n",
    "        'odds_ft_1', 'odds_ft_x','games_remaining'\n",
    "    ]\n",
    "\n",
    "    # found to not be useful # these were useful for pace\n",
    "    # 171           team_pens    0.000000\n",
    "    # 172            opp_pens    0.000000\n",
    "    # 283  team_penalties_won    0.000000\n",
    "    for stat in ['opp_penalty_goals','team_penalty_goals','team_penalties_won','opp_penalties_won','opp_penalty_missed','attacks_recorded','goals_total']:\n",
    "        stats.remove(stat)\n",
    "        \n",
    "    total_non_sb = len(non_sb_data)\n",
    "    # drop cols where 93% nulls\n",
    "    threshold = int(0.93*total_non_sb)\n",
    "    footy_stats = stats.copy()\n",
    "    to_drop = list(non_sb_data[stats].isnull().sum()[non_sb_data[stats].isnull().sum()>threshold].index)\n",
    "    for td in to_drop:\n",
    "        footy_stats.remove(td)\n",
    "    non_sb_data = non_sb_data.drop(columns=to_drop)\n",
    "    non_sb_data = non_sb_data.drop(columns=['team_id','opp_team_id','match_id','competition_id','is_upcoming'])\n",
    "\n",
    "    non_sb_data = non_sb_data.drop_duplicates(subset=['footy_match_id','footy_team_id'])\n",
    "\n",
    "    def add_game_grades_non_sb(data, show_score=False, show_fi=False):\n",
    "    \n",
    "        # # ### for getting feature importance\n",
    "        X = data[footy_stats+['side_target','total_target']].copy()\n",
    "        # X = X.sample(frac=1)\n",
    "        y1 = X['side_target'].copy()\n",
    "        y2 = X['total_target'].copy()\n",
    "        X = X.drop(columns=['side_target','total_target'])\n",
    "        y1_tr = y1[X['games_remaining']>=5]\n",
    "        y2_tr = y2[X['games_remaining']>=5]\n",
    "        X_tr = X.copy().loc[X['games_remaining']>=5].reset_index(drop=True)\n",
    "        \n",
    "        X_tr=X_tr.drop(columns=['games_remaining'])\n",
    "        \n",
    "        eval_cv = KFold(3, shuffle=True, random_state=17)\n",
    "        prod_cv = KFold(8, shuffle=True, random_state=17)\n",
    "        \n",
    "        if show_score:\n",
    "            reg = catboost.CatBoostRegressor(verbose=False)\n",
    "            # 08/23/22 Score diff cross val 0.17366522630199602\n",
    "            print(\"Score diff cross val\", np.mean(cross_val_score(reg, X_tr, y1_tr, cv=eval_cv)))\n",
    "        # # option 1\n",
    "        stat_display = footy_stats.copy()\n",
    "        stat_display.remove('games_remaining')\n",
    "        \n",
    "        params = {\n",
    "            \n",
    "        }\n",
    "\n",
    "        if show_fi:\n",
    "            reg = catboost.CatBoostRegressor(verbose=False)\n",
    "            reg.fit(X_tr, y1_tr)\n",
    "            feat_importances = pd.DataFrame({\n",
    "                'stats':stat_display,\n",
    "                'importance':reg.feature_importances_\n",
    "            }).sort_values(by='importance',ascending=False)\n",
    "            print(feat_importances)\n",
    "            \n",
    "        X = X.drop(columns=['games_remaining'])\n",
    "        reg = catboost.CatBoostRegressor(verbose=False, **params)\n",
    "    #     reg = xgb.XGBRegressor()\n",
    "        data['game_score'] = cross_val_predict(reg, X, y1, cv=prod_cv)\n",
    "        \n",
    "        reg = catboost.CatBoostRegressor(verbose=False, **params)\n",
    "        data['pace_score'] = cross_val_predict(reg, X, y2, cv=prod_cv)\n",
    "    \n",
    "        \n",
    "        return data\n",
    "\n",
    "\n",
    "    # Score diff cross val 0.1750731599982609 without odds\n",
    "    # Score diff cross val 0.174051975593478513 with odds\n",
    "    # 08/23/22 Pace cross val 0.23575666835862108 without odds\n",
    "    # Pace cross val 0.12337012842172541 with odds\n",
    "    non_sb_data = add_game_grades_non_sb(non_sb_data)\n",
    "\n",
    "    def add_game_grades_sb(data, show_score=False, show_fi=False):\n",
    "    \n",
    "        # # ### for getting feature importance\n",
    "        X = data[stats+['side_target','total_target']].copy()\n",
    "        # X = X.sample(frac=1)\n",
    "        y1 = X['side_target'].copy()\n",
    "        y2 = X['total_target'].copy()\n",
    "        X = X.drop(columns=['side_target','total_target'])\n",
    "        y1_tr = y1[X['games_remaining']>=5]\n",
    "        y2_tr = y2[X['games_remaining']>=5]\n",
    "        X_tr = X.copy().loc[X['games_remaining']>=5].reset_index(drop=True)\n",
    "        \n",
    "        eval_cv = KFold(3, shuffle=True, random_state=17)\n",
    "        prod_cv = KFold(6, shuffle=True, random_state=17)\n",
    "        X_tr = X_tr.drop(columns=['games_remaining'])\n",
    "        if show_score:\n",
    "            reg = catboost.CatBoostRegressor(verbose=False)\n",
    "            # 08/23/22 Score diff cross val 0.17366522630199602\n",
    "            print(\"Score diff cross val\", np.mean(cross_val_score(reg, X_tr, y1_tr, cv=eval_cv)))\n",
    "        # # option 1\n",
    "        stat_display = stats.copy()\n",
    "        stat_display.remove('games_remaining')\n",
    "        if show_fi:\n",
    "            reg = catboost.CatBoostRegressor(verbose=False)\n",
    "            reg.fit(X_tr, y1_tr)\n",
    "            feat_importances = pd.DataFrame({\n",
    "                'stats':stat_display,\n",
    "                'importance':reg.feature_importances_\n",
    "            }).sort_values(by='importance',ascending=False)\n",
    "            print(feat_importances)\n",
    "            \n",
    "        X = X.drop(columns=['games_remaining'])\n",
    "        reg = catboost.CatBoostRegressor(verbose=False)\n",
    "        data['game_score'] = cross_val_predict(reg, X, y1, cv=prod_cv)\n",
    "        \n",
    "        reg = catboost.CatBoostRegressor(verbose=False)\n",
    "        data['pace_score'] = cross_val_predict(reg, X, y2, cv=prod_cv)\n",
    "        \n",
    "        return data\n",
    "\n",
    "    sb_data = add_game_grades_sb(sb_data)\n",
    "\n",
    "    \n",
    "    non_sb_scores = non_sb_data.copy()[['footy_match_id','footy_team_id','footy_opp_id','match_date_UTC',\n",
    "                                'game_score','pace_score']]\n",
    "    sb_game_scores = sb_data.copy()[['footy_match_id','footy_team_id','footy_opp_id','match_date_UTC','match_id','team_id','opp_team_id',\n",
    "                                'game_score','pace_score']]\n",
    "\n",
    "    game_scores = pd.concat([sb_game_scores, non_sb_scores], axis=0).sort_values(by='match_date_UTC').reset_index(drop=True)\n",
    "\n",
    "\n",
    "    def prep_for_network(df):\n",
    "        \n",
    "        opp_data = df.copy().rename(columns={'id':'opp_id','opp_id':'id'})\n",
    "        opp_data = opp_data.rename(columns={'game_score':'opp_game_score','pace_score':'opp_pace_score'})\n",
    "\n",
    "        df = df.merge(opp_data[['match_date','footy_match_id','id','opp_id']+['opp_game_score','opp_pace_score']], how='left', on=['match_date','footy_match_id','id','opp_id'])\n",
    "        # # just do two networks for now (later more)\n",
    "        df['rtg_avg'] = (df['game_score'].copy() + (-1*df['opp_game_score'].copy()))/2\n",
    "        df['pace_avg'] = (df['pace_score'].copy() + (df['opp_pace_score'].copy()))/2\n",
    "\n",
    "\n",
    "        min_date = df.match_date.min()\n",
    "        # # helper col\n",
    "        df['rating_period'] = df['match_date'].copy().rank(method='dense').astype(int)\n",
    "        df['date_since_inception'] = (df['match_date'].copy()-min_date).dt.days\n",
    "        df['days_since_last'] = df['date_since_inception'].diff()\n",
    "        df['game_no'] = df.groupby(['id'])['footy_match_id'].transform('cumcount')\n",
    "        df['opp_game_no'] = df.groupby(['opp_id'])['footy_match_id'].transform('cumcount')\n",
    "        df = df.drop_duplicates(subset=['footy_match_id']).reset_index(drop=True)\n",
    "        \n",
    "        ## only using statsbomb teams\n",
    "        df['backup_team_id'] = df['id'].map(teams_footy2id)\n",
    "        df['backup_opp_id'] = df['opp_id'].map(teams_footy2id)\n",
    "        df['team_id'] = df['team_id'].fillna(df['backup_team_id'].copy())\n",
    "        df['opp_team_id'] = df['opp_team_id'].fillna(df['backup_opp_id'].copy())\n",
    "        df = df.drop(columns=['backup_team_id','backup_opp_id'])\n",
    "\n",
    "        \n",
    "        return df\n",
    "\n",
    "    game_scores = game_scores.rename(columns={'footy_team_id':'id', 'footy_opp_id':'opp_id','match_date_UTC':'match_date'})\n",
    "    game_scores = prep_for_network(game_scores)\n",
    "\n",
    "    # create network\n",
    "    df = game_scores.copy().dropna(subset=['team_id','opp_team_id','rtg_avg','pace_avg'])\n",
    "    df['days_since_last'] = df['days_since_last'].fillna(1)# one case\n",
    "    df = df.drop(columns=['id','opp_id'])\n",
    "    teams = list(set(list(df.team_id.unique())+list(df.opp_team_id.unique())))\n",
    "    num_teams = len(teams)\n",
    "    teams.sort()\n",
    "    team_map = {}\n",
    "    for idx, team in enumerate(teams):\n",
    "        team_map[team] = idx\n",
    "        \n",
    "    prank_mat = np.zeros((num_teams,num_teams))\n",
    "    neg_prank_mat = np.zeros((num_teams,num_teams))\n",
    "    pace_mat = np.ones((num_teams, num_teams))*2.5\n",
    "    pace_std = np.ones((num_teams, num_teams))\n",
    "    rating_periods = list(df.rating_period.unique())\n",
    "    df.match_date = pd.to_datetime(df.match_date.copy())\n",
    "    df = df.sort_values(by='match_date')\n",
    "\n",
    "\n",
    "    def calc_ratings(protag_matrix):\n",
    "        \n",
    "        N = biggest_index = protag_matrix.shape[0]    \n",
    "        d = 5e-4\n",
    "        A = (d * protag_matrix + (1 - d) / N)\n",
    "        v = np.repeat(1/biggest_index, biggest_index)\n",
    "        for i in range(150):\n",
    "            v = A@v\n",
    "            norm = np.linalg.norm(v)\n",
    "            v = v/norm\n",
    "        \n",
    "        return v\n",
    "\n",
    "    history = []\n",
    "    for rp in tqdm(rating_periods):\n",
    "        rating_update = np.zeros((num_teams, num_teams))\n",
    "        neg_rating_update = np.zeros((num_teams, num_teams))\n",
    "        pace_update = np.zeros((num_teams, num_teams))\n",
    "        pace_std_update = np.zeros((num_teams, num_teams))\n",
    "        rp_data = df.copy().loc[df.rating_period==rp].reset_index(drop=True)\n",
    "        days_since = rp_data.days_since_last.copy().max()\n",
    "        if days_since < 1:\n",
    "            days_since = 1\n",
    "        \n",
    "        # time decay the matrix\n",
    "        prank_mat *= np.exp(-(1/150)*days_since)\n",
    "        neg_prank_mat *= np.exp(-(1/150)*days_since)\n",
    "        pace_mat *= np.exp(-(1/125)*days_since)\n",
    "        pace_std *= np.exp(-(1/125)*days_since)\n",
    "        \n",
    "        ratings_vec = calc_ratings(prank_mat)\n",
    "        neg_ratings_vec = calc_ratings(neg_prank_mat)\n",
    "        \n",
    "        pace_vec = calc_ratings(pace_mat.copy()/pace_std.copy())\n",
    "        ratings_entr = entropy(prank_mat)\n",
    "        pace_entr = entropy(pace_mat.copy()/pace_std.copy())\n",
    "        for index, row in rp_data.iterrows():\n",
    "            match_id = row['match_id']\n",
    "            protag_id = row['team_id']\n",
    "            antag_id = row['opp_team_id']\n",
    "            protag_index = team_map[protag_id]\n",
    "            antag_index = team_map[antag_id]\n",
    "            ## grab ratings going into games\n",
    "            protag_rating = ratings_vec[protag_index] \n",
    "            antag_rating = ratings_vec[antag_index]\n",
    "            protag_neg_rating = neg_ratings_vec[protag_index] \n",
    "            antag_neg_rating = neg_ratings_vec[antag_index]\n",
    "            protag_entr = ratings_entr[protag_index]\n",
    "            antag_entr = ratings_entr[antag_index]\n",
    "            protag_pace = pace_vec[protag_index]\n",
    "            antag_pace = pace_vec[antag_index]\n",
    "            protag_pentr = pace_entr[protag_index]\n",
    "            antag_pentr = pace_entr[antag_index]\n",
    "            history.append([match_id, protag_id, antag_id, protag_rating, antag_rating, protag_neg_rating,antag_neg_rating,protag_entr, antag_entr,\n",
    "                            protag_pace, antag_pace, protag_pentr, antag_pentr])\n",
    "            \n",
    "            # update\n",
    "            game_rating = row['rtg_avg']\n",
    "            pace_rating = row['pace_avg']\n",
    "            if np.isnan(game_rating):\n",
    "                continue\n",
    "            if np.isnan(pace_rating):\n",
    "                continue\n",
    "                \n",
    "            if game_rating > 0:\n",
    "                rating_update[protag_index][antag_index]+=np.abs(game_rating)\n",
    "                neg_rating_update[antag_index][protag_index]+=np.abs(game_rating)\n",
    "            else:\n",
    "                rating_update[antag_index][protag_index]+=np.abs(game_rating)\n",
    "                neg_rating_update[protag_index][antag_index]+=np.abs(game_rating)\n",
    "            pace_update[protag_index][antag_index] += pace_rating\n",
    "            pace_update[antag_index][protag_index] += pace_rating\n",
    "            pace_std_update[protag_index][antag_index] += 1\n",
    "            pace_std_update[antag_index][protag_index] += 1\n",
    "            \n",
    "        prank_mat+=rating_update\n",
    "        neg_prank_mat+=neg_rating_update\n",
    "        pace_mat += pace_update\n",
    "        pace_std+= pace_std_update\n",
    "\n",
    "\n",
    "\n",
    "    history = pd.DataFrame(history, columns=['match_id','id','opp_id','rating','opp_rating','neg_rating','neg_opp_rating','entropy','opp_entropy',\n",
    "                                            'pace','opp_pace','pace_entropy','opp_pace_entropy'])\n",
    "    history = history.merge(stf_schedule[['match_id','team_id','score','opp_score']].rename(columns={'team_id':'id'}),\n",
    "                        how='left', on=['match_id','id'])\n",
    "\n",
    "    history['score_diff'] = history['score'].copy()-history['opp_score'].copy()\n",
    "    history['ratings_diff'] = history['rating'].copy()-history['opp_rating'].copy()\n",
    "    history['ratings_v2'] = history['rating'].copy()-history['neg_rating'].copy()\n",
    "    history['opp_ratings_v2'] = history['opp_rating'].copy()-history['neg_opp_rating'].copy()\n",
    "    history['rtg_diff_v2'] = history['ratings_v2'].copy()-history['opp_ratings_v2'].copy()\n",
    "    history['score_total'] = history['score'].copy()+history['opp_score'].copy()\n",
    "    history['pace_comb'] = history['pace'].copy()+history['opp_pace'].copy()\n",
    "\n",
    "\n",
    "    teams = load_dict(os.path.join(DROPBOX_PATH, 'IDs/teams'))\n",
    "    competitions = load_dict(os.path.join(DROPBOX_PATH, 'IDs/competitions'))\n",
    "\n",
    "\n",
    "    def get_current_ratings():\n",
    "        \n",
    "        current_rtgs = []\n",
    "        for protag_id,protag_index, in team_map.items():\n",
    "            protag_index = team_map[protag_id]\n",
    "            ## grab ratings going into games\n",
    "            protag_rating = ratings_vec[protag_index] \n",
    "            protag_neg_rating = neg_ratings_vec[protag_index]\n",
    "            protag_entr = ratings_entr[protag_index]\n",
    "            protag_pace = pace_vec[protag_index]\n",
    "            protag_pentr = pace_entr[protag_index]\n",
    "            \n",
    "            current_rtgs.append([protag_id, protag_rating, protag_neg_rating, protag_entr, protag_pace, protag_pentr])\n",
    "        \n",
    "        return pd.DataFrame(current_rtgs, columns=['team_id','rating','neg_rating','entropy','pace','pace_entropy'])\n",
    "    # \n",
    "    to_save = history.copy().dropna(subset=['match_id']).reset_index(drop=True)\n",
    "    to_save = to_save.drop(columns=['score','opp_score','score_diff','score_total'])\n",
    "    to_save = to_save.rename(columns={'id':'team_id','opp_id':'opp_team_id'})\n",
    "    to_save_opp = to_save.copy().reset_index(drop=True)\n",
    "    to_save_opp.columns=['match_id','opp_team_id','team_id','opp_rating','rating','neg_opp_rating','neg_rating','opp_entropy','entropy','opp_pace','pace','opp_pace_entropy','pace_entropy','ratings_diff','opp_ratings_v2','ratings_v2','rtg_diff_v2','pace_comb']\n",
    "    to_save_opp = to_save_opp[list(to_save)]\n",
    "    to_save_opp['ratings_diff'] = to_save_opp['ratings_diff'].copy()*-1\n",
    "    to_save_opp['rtg_diff_v2'] = to_save_opp['rtg_diff_v2'].copy()*-1\n",
    "    to_save = pd.concat([to_save, to_save_opp], axis=0).reset_index(drop=True)\n",
    "    to_save.sort_values(by=['match_id'])\n",
    "    to_save = to_save.drop(columns=['opp_rating','rating','neg_opp_rating','neg_rating','ratings_diff'])\n",
    "    current_ratings = get_current_ratings()\n",
    "    opp_current_ratings = current_ratings.copy()\n",
    "    opp_current_ratings.columns=['opp_team_id','opp_rating','neg_opp_rating','opp_entropy','opp_pace','opp_pace_entropy']\n",
    "    upc = stf_schedule.copy().loc[stf_schedule['is_upcoming']==1].reset_index(drop=True)[['match_id','team_id','opp_team_id']]\n",
    "    upc = upc.merge(current_ratings.copy(), how='left', on=['team_id'])\n",
    "    upc = upc.merge(opp_current_ratings.copy(), how='left', on='opp_team_id')\n",
    "    # upc['ratings_diff'] = upc['rating'].copy()-upc['opp_rating'].copy()\n",
    "    upc['ratings_v2'] = upc['rating'].copy()-history['neg_rating'].copy()\n",
    "    upc['opp_ratings_v2'] = upc['opp_rating'].copy()-history['neg_opp_rating'].copy()\n",
    "    upc['rtg_diff_v2'] = upc['ratings_v2'].copy()-history['opp_ratings_v2'].copy()\n",
    "    upc = upc.drop(columns=['opp_rating','rating','neg_opp_rating','neg_rating'])\n",
    "    upc['pace_comb'] = upc['pace'].copy()+upc['opp_pace'].copy()\n",
    "    to_save = pd.concat([to_save, upc], axis=0).reset_index(drop=True)\n",
    "\n",
    "    to_save.to_csv(os.path.join(DROPBOX_PATH, 'team_ratings/grade_network.csv'), index=False)\n",
    "    return\n",
    "\n",
    "def gg3():\n",
    "\n",
    "    # schedule, stf_schedule = load_schedules()\n",
    "    # footy = load_footy()\n",
    "    # teams_footy2id = load_dict(os.path.join(DROPBOX_PATH, 'IDs/footy/teams_footy2id'))\n",
    "    # footy = prep_footy_data(footy.copy(), teams_footy2id)\n",
    "    # df, missing = merge_game_ids(stf_schedule.copy(), footy.copy().rename(columns={'datetime_UTC':'footy_datetime_UTC'}), schedule)\n",
    "\n",
    "    # df = processing_steps(df, schedule.copy())\n",
    "    \n",
    "    ## gg 3: prev matchup\n",
    "    print(\"Creating team score/opp score game grades...\")\n",
    "    ### data prep ###\n",
    "    teams = load_dict(os.path.join(DROPBOX_PATH, 'IDs/teams'))\n",
    "    competitions = load_dict(os.path.join(DROPBOX_PATH, 'IDs/competitions'))\n",
    "    \n",
    "    schedule, stf_schedule = load_schedules()\n",
    "    teams_id2footy = load_dict(os.path.join(DROPBOX_PATH, 'IDs/footy/teams_id2footy'))\n",
    "    teams_footy2id = load_dict(os.path.join(DROPBOX_PATH, 'IDs/footy/teams_footy2id'))\n",
    "    footy = load_footy()\n",
    "    footy = prep_footy_data(footy.copy(), teams_footy2id)\n",
    "\n",
    "    df, missing = merge_game_ids(stf_schedule.copy(), footy.copy().rename(columns={'datetime_UTC':'footy_datetime_UTC'}), schedule.copy())\n",
    "\n",
    "    df = add_sb_metrics(df, stf_schedule)\n",
    "    df = df.rename(columns={'id':'footy_match_id'})\n",
    "\n",
    "    df = processing_steps(df, schedule.copy())\n",
    "    sb_data = df.copy().loc[df['match_id'].notnull()].sort_values(by='datetime_UTC').reset_index(drop=True)\n",
    "    non_sb_data = df.copy().loc[df['match_id'].isnull()].sort_values(by='datetime_UTC').reset_index(drop=True)\n",
    "    \n",
    "    def prep_sb_data(data):\n",
    "        stat_decays = {\n",
    "            'sb_team_goals':0.05,\n",
    "            'sb_opp_goals':0.05,\n",
    "            'sb_team_SOTs':0.06,\n",
    "            'sb_opp_SOTs':0.06,\n",
    "            'sb_team_obv':0.04,\n",
    "            'sb_opp_obv':0.04\n",
    "\n",
    "        }\n",
    "        \n",
    "        data = data.copy().dropna(subset=['sb_opp_shots','sb_team_xxG'])\n",
    "        data['games_in_season'] = data.groupby(['team_id','season'])['match_id'].transform('count')\n",
    "        data['szn_already_played'] = data.groupby(['team_id','season'])['match_id'].transform('cumcount')\n",
    "        data['games_remaining'] = data['games_in_season'].copy()-data['szn_already_played'].copy()\n",
    "        data = data.reset_index(drop=True)\n",
    "        \n",
    "        data = data.sort_values(by=['datetime_UTC']).reset_index(drop=True)\n",
    "        for stat, decay in stat_decays.items():\n",
    "            alpha=decay\n",
    "            data = data.set_index('datetime_UTC')\n",
    "            data['left_target'] = data.groupby(['team_id'])[stat].transform(lambda x: x.shift().ewm(alpha=alpha).mean())\n",
    "            # data['left_target'] = data.groupby(['team_id'])[stat].transform(lambda x: x.shift().ewm(halflife=f'{halflife}D', times=pd.DatetimeIndex(x.index)).mean())\n",
    "            # data['left_target'] = data.groupby(['team_id'])[stat].transform(lambda x: x.shift().rolling(40).mean())\n",
    "            data = data.reset_index()\n",
    "            data = data.sort_values(by=['datetime_UTC'], ascending=False)\n",
    "            data = data.set_index('datetime_UTC')\n",
    "    #         data['right_league_HFA'] = data.groupby(['competition_id','is_home'])[stat].transform(lambda x: x.shift().ewm(alpha=alpha**2).mean())\n",
    "            ## not sure why this gives worse results\n",
    "            #data[f'adj_{stat}'] = data[stat].copy()-(data['right_league_HFA'].copy()-data['right_league_HFA'].mean())\n",
    "            data['right_target'] = data.groupby(['team_id'])[stat].transform(lambda x: x.shift().ewm(alpha=alpha).mean())\n",
    "            #data['right_adj_target'] = data.groupby(['team_id'])[f'adj_{stat}'].transform(lambda x: x.shift().ewm(alpha=alpha).mean())\n",
    "\n",
    "            data = data.reset_index()\n",
    "            data = data.sort_values(by=['datetime_UTC']).reset_index(drop=True)\n",
    "            data[f'{stat}_tgt'] = data[['left_target','right_target']].mean(axis=1)\n",
    "            data[f'{stat}_fwd_tgt'] = data['right_target'].copy()\n",
    "        \n",
    "        return data\n",
    "\n",
    "    sb_data = prep_sb_data(sb_data.copy())\n",
    "\n",
    "    def prep_non_sb_data(data):\n",
    "        stat_decays = {\n",
    "            'sb_team_goals':0.05,\n",
    "            'sb_opp_goals':0.05,\n",
    "            'sb_team_SOTs':0.06,\n",
    "            'sb_opp_SOTs':0.06\n",
    "    #         'sb_team_obv':0.04,\n",
    "    #         'sb_opp_obv':0.04\n",
    "\n",
    "        }\n",
    "        \n",
    "        data = data.copy().dropna(subset=['team_goals','team_shotsOnTarget'])\n",
    "        data['games_in_season'] = data.groupby(['team_id','season'])['footy_match_id'].transform('count')\n",
    "        data['szn_already_played'] = data.groupby(['team_id','season'])['footy_match_id'].transform('cumcount')\n",
    "        data['games_remaining'] = data['games_in_season'].copy()-data['szn_already_played'].copy()\n",
    "        \n",
    "        data = data.sort_values(by=['datetime_UTC']).reset_index(drop=True)\n",
    "        for stat, decay in stat_decays.items():\n",
    "            alpha = decay\n",
    "    #         data = data.set_index('datetime_UTC')\n",
    "            data['left_target'] = data.groupby(['team_id'])[stat].transform(lambda x: x.shift().ewm(alpha=alpha).mean())\n",
    "            # data['left_target'] = data.groupby(['team_id'])[stat].transform(lambda x: x.shift().ewm(halflife=f'{halflife}D', times=pd.DatetimeIndex(x.index)).mean())\n",
    "            # data['left_target'] = data.groupby(['team_id'])[stat].transform(lambda x: x.shift().rolling(40).mean())\n",
    "    #         data = data.reset_index()\n",
    "            data = data.sort_values(by=['datetime_UTC'], ascending=False).reset_index(drop=True)\n",
    "    #         data = data.set_index('datetime_UTC')\n",
    "    #         data['right_league_HFA'] = data.groupby(['competition_id','is_home'])[stat].transform(lambda x: x.shift().ewm(alpha=alpha**2).mean())\n",
    "            ## not sure why this gives worse results\n",
    "            #data[f'adj_{stat}'] = data[stat].copy()-(data['right_league_HFA'].copy()-data['right_league_HFA'].mean())\n",
    "            data['right_target'] = data.groupby(['team_id'])[stat].transform(lambda x: x.shift().ewm(alpha=alpha).mean())\n",
    "            #data['right_adj_target'] = data.groupby(['team_id'])[f'adj_{stat}'].transform(lambda x: x.shift().ewm(alpha=alpha).mean())\n",
    "\n",
    "            data = data.sort_values(by=['datetime_UTC']).reset_index(drop=True)\n",
    "            data[f'{stat}_tgt'] = data[['left_target','right_target']].mean(axis=1)\n",
    "            data[f'{stat}_fwd_tgt'] = data['right_target'].copy()\n",
    "        \n",
    "        # weird bug fix\n",
    "        data['szn_already_played'] = data['games_in_season'].copy()-data['games_remaining'].copy()\n",
    "        return data\n",
    "\n",
    "    non_sb_data = prep_non_sb_data(non_sb_data.copy())\n",
    "\n",
    "\n",
    "    stats = [\n",
    "        'score_diff', 'xG_diff', 'shot_diff', 'sq_diff', 'sot_diff', 'score_total', 'xG_total', 'shot_total', 'sq_total', 'sot_total', 'sit_boost', 'pace', 'obv_pace', 'xxG_diff', 'pct_xxG_diff', 'obv_diff', 'pct_obv_diff', 'cgoal_skew', 'cgoal_kurt', 'cgoal_sum', 'cgoal_std', 'cconcede_skew', 'cconcede_kurt', 'cconcede_sum', 'cconcede_std', 'xxG_conversion', 'xG_conversion', 'opp_xG_conversion', 'opp_xxG_conversion', 'win_prob', 'draw_prob', 'opp_dt_eff', 'opp_mt_eff', 'opp_at_eff', 'opp_d_qual', 'opp_mid_qual', 'opp_atck_qual', 'team_dt_eff', 'team_mt_eff', 'team_at_eff', 'team_d_qual', 'team_mid_qual', 'team_atck_qual', 'opp_gk_pass_loc', 'opp_def_pass_loc', 'opp_mid_pass_loc', 'opp_atck_pass_loc', 'opp_gk_pass_angle', 'opp_def_pass_angle', 'opp_mid_pass_angle', 'opp_atck_pass_angle', 'opp_gk_pass_count', 'opp_def_pass_count', 'opp_mid_pass_count', 'opp_atck_pass_count', 'opp_gk_pass_dist', 'opp_def_pass_dist', 'opp_mid_pass_dist', 'opp_atck_pass_dist', 'opp_gk_pass_obv_vol', 'opp_def_pass_obv_vol', 'opp_mid_pass_obv_vol', 'opp_atck_pass_obv_vol', 'opp_gk_pass_obv_eff', 'opp_def_pass_obv_eff', 'opp_mid_pass_obv_eff', 'opp_atck_pass_obv_eff', 'team_gk_pass_loc', 'team_def_pass_loc', 'team_mid_pass_loc', 'team_atck_pass_loc', 'team_gk_pass_angle', 'team_def_pass_angle', 'team_mid_pass_angle', 'team_atck_pass_angle', 'team_gk_pass_count', 'team_def_pass_count', 'team_mid_pass_count', 'team_atck_pass_count', 'team_gk_pass_dist', 'team_def_pass_dist', 'team_mid_pass_dist', 'team_atck_pass_dist', 'team_gk_pass_obv_vol', 'team_def_pass_obv_vol', 'team_mid_pass_obv_vol', 'team_atck_pass_obv_vol', 'team_gk_pass_obv_eff', 'team_def_pass_obv_eff', 'team_mid_pass_obv_eff', 'team_atck_pass_obv_eff', 'opp_gk_carry_angle', 'opp_def_carry_angle', 'opp_mid_carry_angle', 'opp_atck_carry_angle', 'opp_gk_carry_count', 'opp_def_carry_count', 'opp_mid_carry_count', 'opp_atck_carry_count', 'opp_gk_carry_dist', 'opp_def_carry_dist', 'opp_mid_carry_dist', 'opp_atck_carry_dist', 'opp_gk_carry_vol', 'opp_def_carry_vol', 'opp_mid_carry_vol', 'opp_atck_carry_vol', 'opp_gk_carry_eff', 'opp_def_carry_eff', 'opp_mid_carry_eff', 'opp_atck_carry_eff', 'team_gk_carry_angle', 'team_def_carry_angle', 'team_mid_carry_angle', 'team_atck_carry_angle', 'team_gk_carry_dist', 'team_def_carry_dist', 'team_mid_carry_dist', 'team_atck_carry_dist', 'team_gk_carry_count', 'team_def_carry_count', 'team_mid_carry_count', 'team_atck_carry_count', 'team_gk_carry_vol', 'team_def_carry_vol', 'team_mid_carry_vol', 'team_atck_carry_vol', 'team_gk_carry_eff', 'team_def_carry_eff', 'team_mid_carry_eff', 'team_atck_carry_eff', 'opp_gk_dfn_pos', 'opp_def_dfn_pos', 'opp_mid_dfn_pos', 'opp_atck_dfn_pos', 'opp_gk_dfn_obv_vol', 'opp_def_dfn_obv_vol', 'opp_mid_dfn_obv_vol', 'opp_atck_dfn_obv_vol', 'opp_gk_dfn_obv_eff', 'opp_def_dfn_obv_eff', 'opp_mid_dfn_obv_eff', 'opp_atck_dfn_obv_eff', 'opp_gk_dfn_xxG_vol', 'opp_def_dfn_xxG_vol', 'opp_mid_dfn_xxG_vol', 'opp_atck_dfn_xxG_vol', 'team_gk_dfn_pos', 'team_def_dfn_pos', 'team_mid_dfn_pos', 'team_atck_dfn_pos', 'team_gk_dfn_obv_vol', 'team_def_dfn_obv_vol', 'team_mid_dfn_obv_vol', 'team_atck_dfn_obv_vol', 'team_gk_dfn_obv_eff', 'team_def_dfn_obv_eff', 'team_mid_dfn_obv_eff', 'team_atck_dfn_obv_eff', 'team_gk_dfn_xxG_vol', 'team_def_dfn_xxG_vol', 'team_mid_dfn_xxG_vol', 'team_atck_dfn_xxG_vol', 'team_weighted_fouls', 'opp_weighted_fouls', 'opp_corners', 'team_corners', 'team_crosses', 'opp_crosses', 'team_cross_pct', 'opp_cross_pct', 'team_pens', 'opp_pens', 'carry_lengths', 'gk_dist', 'fields_gained', 'fields_gained_comp', 'med_def_action', 'threat_pp', 'dthreat_pp', 'off_embed_0', 'off_embed_1', 'off_embed_2', 'off_embed_3', 'off_embed_4', 'off_embed_5', 'def_embed_0', 'def_embed_1', 'def_embed_2', 'def_embed_3', 'def_embed_4', 'def_embed_5', 'opp_poss', 'opp_ppp', 'opp_spp', 'team_poss', 'team_ppp', 'team_spp', 'team_poss_start', 'opp_poss_start', 'team_poss_len', 'opp_poss_len', 'team_poss_width', 'opp_poss_width', 'team_poss_time_sum', 'opp_poss_time_sum', 'team_poss_time_median', 'opp_poss_time_median', 'oxG_f3', 'txG_f3', 'pct_lead', 'pct_tied', 'pct_trail', 'man_adv', 'team_dx_sec', 'team_xxG_sec', 'opp_dx_sec', 'opp_xxG_sec', 'opp_d3_passes', 'opp_d3_comp%', 'opp_d3_fcomp%', 'opp_m3_passes', 'opp_m3_comp%', 'opp_m3_fcomp%', 'opp_a3_passes', 'opp_a3_comp%', 'opp_a3_fcomp%', 'team_d3_passes', 'team_d3_comp%', 'team_d3_fcomp%', 'team_m3_passes', 'team_m3_comp%', 'team_m3_fcomp%', 'team_a3_passes', 'team_a3_comp%', 'team_a3_fcomp%', 'opp_SOT%', 'team_save%', 'team_SOT%', 'opp_save%', 'team_XGOT/SOT', 'opp_XGOT/SOT', 'opp_end_d3', 'opp_end_m3', 'opp_end_a3', 'team_end_d3', 'team_end_m3', 'team_end_a3', 'team_switches', 'opp_switches', 'opp_d3_press', 'opp_m3_press', 'opp_a3_press', 'team_d3_press', 'team_m3_press', 'team_a3_press', 'opp_wide_poss', 'team_wide_poss',\n",
    "    ]\n",
    "    stats+=[\n",
    "    'team_goals', 'opp_goals', 'attacks_recorded', 'team_yellow_cards', 'opp_yellow_cards', 'team_red_cards', 'opp_red_cards', 'team_shotsOnTarget', 'opp_shotsOnTarget', 'team_shotsOffTarget', 'opp_shotsOffTarget', 'team_shots', 'opp_shots', 'team_fouls', 'opp_fouls', 'team_possession', 'opp_possession', 'team_offsides', 'opp_offsides', 'team_dangerous_attacks', 'opp_dangerous_attacks', 'team_attacks', 'opp_attacks', 'team_xg', 'opp_xg', 'total_xg', 'team_penalties_won', 'opp_penalties_won', 'team_penalty_goals', 'opp_penalty_goals', 'team_penalty_missed', 'opp_penalty_missed', 'team_throwins', 'opp_throwins', 'team_freekicks', 'opp_freekicks', 'team_goalkicks', 'opp_goalkicks'\n",
    "    ]\n",
    "    stats+=[\n",
    "        'goals_total', 'shots_total', 'shotsOnTarget_total', 'attacks_total', 'dangerous_attacks_total', 'xg_total'\n",
    "    ]\n",
    "\n",
    "    stats+=[\n",
    "        'avg_potential','goals_diff', 'shots_diff', 'shotsOnTarget_diff', 'possession_diff', 'attacks_diff', 'dangerous_attacks_diff', 'xg_diff','no_home_away','is_home'\n",
    "    ]\n",
    "\n",
    "    # leaky for what happened in past\n",
    "    ## but fixed this notebook\n",
    "    stats+=[\n",
    "        'odds_ft_1', 'odds_ft_x'\n",
    "    ]\n",
    "\n",
    "    # found to not be useful # these were useful for pace\n",
    "    # 171           team_pens    0.000000\n",
    "    # 172            opp_pens    0.000000\n",
    "    # 283  team_penalties_won    0.000000\n",
    "    for stat in ['opp_penalty_goals','team_penalty_goals','team_penalties_won','opp_penalties_won','opp_penalty_missed','attacks_recorded','goals_total']:\n",
    "        stats.remove(stat)\n",
    "        \n",
    "    total_non_sb = len(non_sb_data)\n",
    "    # drop cols where 93% nulls\n",
    "    threshold = int(0.93*total_non_sb)\n",
    "    footy_stats = stats.copy()\n",
    "    to_drop = list(non_sb_data[stats].isnull().sum()[non_sb_data[stats].isnull().sum()>threshold].index)\n",
    "    for td in to_drop:\n",
    "        footy_stats.remove(td)\n",
    "    non_sb_data = non_sb_data.drop(columns=to_drop)\n",
    "    non_sb_data = non_sb_data.drop(columns=['team_id','opp_team_id','match_id','competition_id','is_upcoming']+['left_target','right_target'])\n",
    "\n",
    "    def create_game_grades_non_sb(data, thresh=5):\n",
    "        \n",
    "        ## training data is a subset of all data, prefer mid season, because then the targets are stable\n",
    "        train = data.copy().loc[data['games_remaining']>=thresh] ## will train right first\n",
    "        train = train.copy().loc[train['szn_already_played']>=(thresh-3)].reset_index(drop=True) ## smaller here because it doesn't matter for one of the models\n",
    "        \n",
    "        print(f'{(len(train)/len(data))*100:.2f}% of data used for training')\n",
    "        \n",
    "        match_ids_used = list(train.copy().footy_match_id)\n",
    "        remain = data.copy().loc[(data['games_remaining']<thresh)|(data['szn_already_played']<(thresh-3))].reset_index(drop=True)\n",
    "        print(f'{(len(remain)/len(data))*100:.2f}% of data will be predicted OOS (bc didnt meet thresholds)')\n",
    "        \n",
    "        X = train.copy().sample(frac=1, random_state=17)\n",
    "        \n",
    "        non_sb_stats = ['sb_team_goals','sb_opp_goals','sb_team_SOTs','sb_opp_SOTs']        \n",
    "        feat_importances = []\n",
    "        feat_importances_fwd = []\n",
    "        scores = {}\n",
    "        scores_fwd = {}\n",
    "        models = {}\n",
    "        models_fwd = {}\n",
    "        \n",
    "        ## use kfold on historical data to train models\n",
    "        hist_game_grades = None\n",
    "        for stat in tqdm(non_sb_stats):\n",
    "            fwd_tgt = stat+'_fwd_tgt'\n",
    "            tgt = stat + '_tgt'\n",
    "            stat_models = []\n",
    "            stat_fwd_models = []\n",
    "            stat_display = footy_stats.copy()\n",
    "            stat_scores = []\n",
    "            stat_fwd_scores = []\n",
    "            kf = KFold(n_splits=5, shuffle=True, random_state=17)\n",
    "            match_ids = []\n",
    "            team_ids = []\n",
    "            fwd_preds = []\n",
    "            preds = []\n",
    "            for train_index, test_index in kf.split(X):\n",
    "                X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "                y_train, y_test = X_train[tgt], X_test[tgt]\n",
    "                y_train_fwd, y_test_fwd = X_train[fwd_tgt], X_test[fwd_tgt]\n",
    "                X_train = X_train.copy()[footy_stats]\n",
    "                match_ids.extend(X_test.footy_match_id)\n",
    "                team_ids.extend(X_test.footy_team_id)\n",
    "                X_test = X_test.copy()[footy_stats]\n",
    "                fwd_model = catboost.CatBoostRegressor(verbose=False)\n",
    "                fwd_model.fit(X_train, y_train_fwd)\n",
    "                feat_importances_fwd.append(fwd_model.feature_importances_)\n",
    "                stat_fwd_models.append(fwd_model)\n",
    "                fwd_pred = fwd_model.predict(X_test)\n",
    "                fwd_score = mean_squared_error(y_test_fwd, fwd_pred)\n",
    "                fwd_preds.extend(fwd_pred)\n",
    "                stat_fwd_scores.append(fwd_score)\n",
    "                X_train = X_train.drop(columns=['odds_ft_x','odds_ft_1','avg_potential'])\n",
    "                model = catboost.CatBoostRegressor(verbose=False)\n",
    "                model.fit(X_train, y_train)\n",
    "                feat_importances.append(model.feature_importances_)\n",
    "                stat_models.append(model)\n",
    "                pred = model.predict(X_test.drop(columns=['odds_ft_x','odds_ft_1','avg_potential']))\n",
    "                score = mean_squared_error(y_test, pred)\n",
    "                preds.extend(pred)\n",
    "                stat_scores.append(score)\n",
    "\n",
    "            scores[stat] = np.mean(stat_scores)\n",
    "            scores_fwd[stat] = np.mean(stat_fwd_scores)\n",
    "            models[stat] = stat_models\n",
    "            models_fwd[stat] = stat_fwd_models\n",
    "            stat_df = pd.DataFrame({\n",
    "                'footy_match_id':match_ids,\n",
    "                'footy_team_id':team_ids,\n",
    "                f'{stat}_gg':preds,\n",
    "                f'{stat}_fwd_gg':fwd_preds\n",
    "            })\n",
    "            \n",
    "            ## record historical predictions\n",
    "            if hist_game_grades is None:\n",
    "                hist_game_grades = stat_df.copy()\n",
    "            else:\n",
    "                hist_game_grades = hist_game_grades.merge(stat_df, how='left', on=['footy_match_id','footy_team_id'])\n",
    "        \n",
    "        feat_importances_fwd = np.mean(np.array(feat_importances_fwd), axis=0)\n",
    "        feat_importances_fwd = pd.DataFrame({\n",
    "            'stats':stat_display,\n",
    "            'importance': feat_importances_fwd\n",
    "        }).sort_values(by='importance',ascending=False)\n",
    "\n",
    "        feat_importances = np.mean(np.array(feat_importances), axis=0)\n",
    "        stat_display.remove('odds_ft_x')\n",
    "        stat_display.remove('odds_ft_1')\n",
    "        stat_display.remove('avg_potential')\n",
    "        feat_importances = pd.DataFrame({\n",
    "            'stats':stat_display,\n",
    "            'importance': feat_importances\n",
    "        }).sort_values(by='importance',ascending=False)\n",
    "        \n",
    "        ## predict for non train games\n",
    "        remaining_gg = pd.DataFrame({\n",
    "            'footy_match_id':remain.footy_match_id,\n",
    "            'footy_team_id':remain.footy_team_id\n",
    "        })\n",
    "        \n",
    "        for stat in tqdm(non_sb_stats):\n",
    "            fwd_stat_models = models_fwd[stat]\n",
    "            stat_models = models[stat]\n",
    "            fwd_cols = footy_stats.copy()\n",
    "            reg_cols = fwd_cols.copy()\n",
    "            reg_cols.remove('odds_ft_x')\n",
    "            reg_cols.remove('odds_ft_1')\n",
    "            reg_cols.remove('avg_potential')\n",
    "            sm_preds =[]\n",
    "            fsm_preds = []\n",
    "            for sm in stat_models:\n",
    "                sm_pred = sm.predict(remain[reg_cols])\n",
    "                sm_preds.append(sm_pred)\n",
    "            for fsm in fwd_stat_models:\n",
    "                fsm_pred = fsm.predict(remain[fwd_cols])\n",
    "                fsm_preds.append(fsm_pred)\n",
    "            sm_preds = np.mean(np.array(sm_preds), axis=0)\n",
    "            fsm_preds = np.mean(np.array(fsm_preds), axis=0)\n",
    "            remaining_gg[f'{stat}_gg'] = sm_preds\n",
    "            remaining_gg[f'{stat}_fwd_gg'] = fsm_preds\n",
    "            \n",
    "        ## commented out is just a check\n",
    "    #     print(len(hist_game_grades))\n",
    "    #     print(hist_game_grades[[f'{stat}_gg',f'{stat}_fwd_gg']].isnull().sum())\n",
    "        hist_game_grades = pd.concat([hist_game_grades, remaining_gg], axis=0).reset_index(drop=True)\n",
    "    #     print(len(hist_game_grades))\n",
    "    #     print(hist_game_grades[[f'{stat}_gg',f'{stat}_fwd_gg']].isnull().sum())\n",
    "        \n",
    "        for stat_name, stat_models in models_fwd.items():\n",
    "            for i, sm in enumerate(stat_models):\n",
    "                model_path = os.path.join(DROPBOX_PATH, f'models/game_grades/fwd_non_{stat_name}_{i}')\n",
    "                sm.save_model(model_path)\n",
    "        for stat_name, _stat_models in models.items():\n",
    "            for i, sm in enumerate(_stat_models):\n",
    "                model_path = os.path.join(DROPBOX_PATH, f'models/game_grades/non_{stat_name}_{i}')\n",
    "                sm.save_model(model_path)\n",
    "            \n",
    "            \n",
    "        \n",
    "        return hist_game_grades\n",
    "\n",
    "    non_sb_grades = create_game_grades_non_sb(non_sb_data.copy())\n",
    "\n",
    "\n",
    "\n",
    "    def create_game_grades_sb(data, thresh=5):\n",
    "        \n",
    "        ## training data is a subset of all data, prefer mid season, because then the targets are stable\n",
    "        train = data.copy().loc[data['games_remaining']>=thresh] ## will train right first\n",
    "        train = train.copy().loc[train['szn_already_played']>=(thresh-3)].reset_index(drop=True) ## smaller here because it doesn't matter for one of the models\n",
    "        \n",
    "        print(f'{(len(train)/len(data))*100:.2f}% of data used for training')\n",
    "        \n",
    "        \n",
    "        remain = data.copy().loc[(data['games_remaining']<thresh)|(data['szn_already_played']<(thresh-3))].reset_index(drop=True)\n",
    "        print(f'{(len(remain)/len(data))*100:.2f}% of data will be predicted OOS')\n",
    "        \n",
    "        X = train.copy().sample(frac=1, random_state=17)\n",
    "        \n",
    "        sb_stats = ['sb_team_goals','sb_opp_goals','sb_team_SOTs','sb_opp_SOTs','sb_team_obv','sb_opp_obv']        \n",
    "        feat_importances = []\n",
    "        feat_importances_fwd = []\n",
    "        scores = {}\n",
    "        scores_fwd = {}\n",
    "        models = {}\n",
    "        models_fwd = {}\n",
    "        \n",
    "        fwd_cols = stats.copy()\n",
    "        reg_cols = fwd_cols.copy()\n",
    "        reg_cols.remove('odds_ft_x')\n",
    "        reg_cols.remove('odds_ft_1')\n",
    "        reg_cols.remove('avg_potential')\n",
    "\n",
    "        ## use kfold on historical data to train models\n",
    "        hist_game_grades = None\n",
    "        for stat in tqdm(sb_stats):\n",
    "            fwd_tgt = stat+'_fwd_tgt'\n",
    "            tgt = stat + '_tgt'\n",
    "            stat_models = []\n",
    "            stat_fwd_models = []\n",
    "            stat_scores = []\n",
    "            stat_fwd_scores = []\n",
    "            kf = KFold(n_splits=5, shuffle=True, random_state=17)\n",
    "            match_ids = []\n",
    "            team_ids = []\n",
    "            fwd_preds = []\n",
    "            preds = []\n",
    "            for train_index, test_index in kf.split(X):\n",
    "                X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "                y_train, y_test = X_train[tgt], X_test[tgt]\n",
    "                y_train_fwd, y_test_fwd = X_train[fwd_tgt], X_test[fwd_tgt]\n",
    "                X_train = X_train.copy()[fwd_cols]\n",
    "                match_ids.extend(X_test.match_id)\n",
    "                team_ids.extend(X_test.team_id)\n",
    "                X_test = X_test.copy()[fwd_cols]\n",
    "                fwd_model = catboost.CatBoostRegressor(verbose=False)\n",
    "                fwd_model.fit(X_train, y_train_fwd)\n",
    "                feat_importances_fwd.append(fwd_model.feature_importances_)\n",
    "                stat_fwd_models.append(fwd_model)\n",
    "                fwd_pred = fwd_model.predict(X_test)\n",
    "                fwd_score = mean_squared_error(y_test_fwd, fwd_pred)\n",
    "                fwd_preds.extend(fwd_pred)\n",
    "                stat_fwd_scores.append(fwd_score)\n",
    "                X_train = X_train[reg_cols]\n",
    "                model = catboost.CatBoostRegressor(verbose=False)\n",
    "                model.fit(X_train, y_train)\n",
    "                feat_importances.append(model.feature_importances_)\n",
    "                stat_models.append(model)\n",
    "                pred = model.predict(X_test[reg_cols])\n",
    "                score = mean_squared_error(y_test, pred)\n",
    "                preds.extend(pred)\n",
    "                stat_scores.append(score)\n",
    "\n",
    "            scores[stat] = np.mean(stat_scores)\n",
    "            scores_fwd[stat] = np.mean(stat_fwd_scores)\n",
    "            models[stat] = stat_models\n",
    "            models_fwd[stat] = stat_fwd_models\n",
    "            stat_df = pd.DataFrame({\n",
    "                'match_id':match_ids,\n",
    "                'team_id':team_ids,\n",
    "                f'{stat}_gg':preds,\n",
    "                f'{stat}_fwd_gg':fwd_preds\n",
    "            })\n",
    "            \n",
    "            ## record historical predictions\n",
    "            if hist_game_grades is None:\n",
    "                hist_game_grades = stat_df.copy()\n",
    "            else:\n",
    "                hist_game_grades = hist_game_grades.merge(stat_df, how='left', on=['match_id','team_id'])\n",
    "                \n",
    "        print(scores_fwd)\n",
    "        print(scores)\n",
    "        \n",
    "        feat_importances_fwd = np.mean(np.array(feat_importances_fwd), axis=0)\n",
    "        feat_importances_fwd = pd.DataFrame({\n",
    "            'stats':fwd_cols,\n",
    "            'importance': feat_importances_fwd\n",
    "        }).sort_values(by='importance',ascending=False)\n",
    "\n",
    "        feat_importances = np.mean(np.array(feat_importances), axis=0)\n",
    "        feat_importances = pd.DataFrame({\n",
    "            'stats':reg_cols,\n",
    "            'importance': feat_importances\n",
    "        }).sort_values(by='importance',ascending=False)\n",
    "        \n",
    "        ## predict for non train games\n",
    "        remaining_gg = pd.DataFrame({\n",
    "            'match_id':remain.match_id,\n",
    "            'team_id':remain.team_id,\n",
    "            'opp_team_id':remain.opp_team_id\n",
    "        })\n",
    "        \n",
    "        for stat in tqdm(sb_stats):\n",
    "            fwd_stat_models = models_fwd[stat]\n",
    "            stat_models = models[stat]\n",
    "\n",
    "            sm_preds =[]\n",
    "            fsm_preds = []\n",
    "            for sm in stat_models:\n",
    "                sm_pred = sm.predict(remain[reg_cols])\n",
    "                sm_preds.append(sm_pred)\n",
    "            for fsm in fwd_stat_models:\n",
    "                fsm_pred = fsm.predict(remain[fwd_cols])\n",
    "                fsm_preds.append(fsm_pred)\n",
    "            sm_preds = np.mean(np.array(sm_preds), axis=0)\n",
    "            fsm_preds = np.mean(np.array(fsm_preds), axis=0)\n",
    "            remaining_gg[f'{stat}_gg'] = sm_preds\n",
    "            remaining_gg[f'{stat}_fwd_gg'] = fsm_preds\n",
    "            \n",
    "        for stat_name, stat_models in models_fwd.items():\n",
    "            for i, sm in enumerate(stat_models):\n",
    "                model_path = os.path.join(DROPBOX_PATH, f'models/game_grades/fwd_{stat_name}_{i}')\n",
    "                sm.save_model(model_path)\n",
    "        for stat_name, _stat_models in models.items():\n",
    "            for i, sm in enumerate(_stat_models):\n",
    "                model_path = os.path.join(DROPBOX_PATH, f'models/game_grades/{stat_name}_{i}')\n",
    "                sm.save_model(model_path)\n",
    "            \n",
    "        ## commented out is just a check\n",
    "        print(len(hist_game_grades))\n",
    "        print(hist_game_grades[[f'{stat}_gg',f'{stat}_fwd_gg']].isnull().sum())\n",
    "        hist_game_grades = pd.concat([hist_game_grades, remaining_gg], axis=0).reset_index(drop=True)\n",
    "        print(len(hist_game_grades))\n",
    "        print(hist_game_grades[[f'{stat}_gg',f'{stat}_fwd_gg']].isnull().sum())\n",
    "        \n",
    "        return hist_game_grades, feat_importances_fwd, feat_importances\n",
    "\n",
    "\n",
    "    sb_game_grades, fi_fwd, fi_ = create_game_grades_sb(sb_data.copy())\n",
    "\n",
    "    \n",
    "\n",
    "    master = df.copy()[['datetime_UTC','competition_id','season','footy_match_id','footy_comp_name','footy_team_id','footy_opp_id','match_date_UTC','match_id','team_id','opp_team_id']]\n",
    "\n",
    "\n",
    "    non_sb_grades.columns=['footy_match_id','footy_team_id']+['non_'+stat for stat in list(non_sb_grades)[2:]]\n",
    "\n",
    "    if 'opp_team_id' in list(non_sb_grades):\n",
    "        non_sb_grades = non_sb_grades.drop(columns=['opp_team_id'])\n",
    "    if 'opp_team_id' in list(sb_game_grades):\n",
    "        sb_game_grades = sb_game_grades.drop(columns=['opp_team_id'])\n",
    "    master = master.merge(non_sb_grades, how='left', on=['footy_match_id','footy_team_id'])\n",
    "    master = master.merge(sb_game_grades, how='left', on=['match_id','team_id'])\n",
    "    master.drop_duplicates(subset=['footy_match_id','footy_team_id'])\n",
    "\n",
    "    \n",
    "    for col_name in ['non_sb_team_goals_gg', 'non_sb_team_goals_fwd_gg', 'non_sb_opp_goals_gg', 'non_sb_opp_goals_fwd_gg', 'non_sb_team_SOTs_gg', 'non_sb_team_SOTs_fwd_gg', 'non_sb_opp_SOTs_gg', 'non_sb_opp_SOTs_fwd_gg']:\n",
    "        sb_col_name = copy(col_name).replace('non_', '')\n",
    "        master[sb_col_name] = master[sb_col_name].fillna(master[col_name])\n",
    "        master = master.drop(columns=[col_name])\n",
    "\n",
    "    master.to_csv(os.path.join(DROPBOX_PATH, 'Statsbomb/game_grades/game_grades.csv'),index=False)\n",
    "\n",
    "    gg = master.copy()\n",
    "    del master\n",
    "    import gc\n",
    "    gc.collect()\n",
    "\n",
    "    for gg_stat in ['sb_team_goals','sb_opp_goals','sb_team_SOTs','sb_opp_SOTs','sb_team_obv','sb_opp_obv']:\n",
    "        stat_name = gg_stat.replace('sb_','')\n",
    "        gg[stat_name] = 0.7*gg[gg_stat+'_fwd_gg'].copy() + 0.3*gg[gg_stat+'_gg'].copy()\n",
    "        gg = gg.drop(columns=[gg_stat+'_fwd_gg',gg_stat+'_gg'])\n",
    "    ## concat upcoming to help upcoming model\n",
    "    gg = pd.concat([gg, stf_schedule.loc[stf_schedule['is_upcoming']==1][['datetime_UTC','competition_id','match_date_UTC','match_id','team_id','opp_team_id']]\n",
    "    ], axis=0).reset_index(drop=True)\n",
    "\n",
    "    \n",
    "    gg[['prev_tgoals','prev_ogoals','prev_tSOTs','prev_oSOTs','prev_tobv','prev_oobv']] = gg.groupby(['team_id','opp_team_id'])[['team_goals','opp_goals','team_SOTs','opp_SOTs','team_obv','opp_obv']].transform(lambda x: x.shift().ewm(alpha=0.3).mean())\n",
    "    model_ready = gg[['match_id','team_id']+['prev_tgoals','prev_ogoals','prev_tSOTs','prev_oSOTs','prev_tobv','prev_oobv']].dropna(subset=['match_id']).reset_index(drop=True)\n",
    "    model_ready['match_id'] = model_ready['match_id'].astype(int)\n",
    "    model_ready['team_id'] = model_ready['team_id'].astype(int)\n",
    "\n",
    "    model_ready.to_csv(os.path.join(DROPBOX_PATH, 'model_ready/prev_matchup_data.csv'), index=False)\n",
    "    return\n",
    "\n",
    "def game_grades():\n",
    "    jobs = []\n",
    "    \n",
    "    p1 = Process(target=gg1)\n",
    "    jobs.append(p1)\n",
    "    p1.start()\n",
    "    p2 = Process(target=gg2)\n",
    "    jobs.append(p2)\n",
    "    p2.start()\n",
    "    p3 = Process(target=gg3)\n",
    "    jobs.append(p3)\n",
    "    p3.start()\n",
    "    # p4 = Process(target=update_game_state_vecs)\n",
    "    # jobs.append(p4)\n",
    "    # p4.start()\n",
    "    # p5 = Process(target=download_odds)\n",
    "    # jobs.append(p5)\n",
    "    # p5.start()\n",
    "\n",
    "    # checks to see if they are finished\n",
    "    for job in jobs:\n",
    "        job.join()\n",
    "        time.sleep(1)\n",
    "    return\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    game_grades()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
